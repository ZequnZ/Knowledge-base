{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: going neural (6 pts)\n",
    "\n",
    "We've checked out statistical approaches to language models in the last notebook. Now let's go find out what deep learning has to offer.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/expanding_mind_lm_kn_3.png' width=300px>\n",
    "\n",
    "We're gonna use the same dataset as before, except this time we build a language model that's character-level, not word level. Before you go:\n",
    "* If you haven't done seminar already, use `seminar.ipynb` to download the data.\n",
    "* This homework uses TensorFlow v2.0: this is [how you install it](https://www.tensorflow.org/beta); and that's [how you use it](https://colab.research.google.com/drive/1YtfbZGgzKr7fpBTqkdEQtu4vUALoTv8A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on character level means that we don't need to deal with large vocabulary or missing words. Heck, we can even keep uppercase words in text! The downside, however, is that all our sequences just got a lot longer.\n",
    "\n",
    "However, we still need special tokens:\n",
    "* Begin Of Sequence  (__BOS__) - this token is at the start of each sequence. We use it so that we always have non-empty input to our neural network. $P(x_t) = P(x_1 | BOS)$\n",
    "* End Of Sequence (__EOS__) - you guess it... this token is at the end of each sequence. The catch is that it should __not__ occur anywhere else except at the very end. If our model produces this token, the sequence is over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS, EOS = ' ', '\\n'\n",
    "\n",
    "data = pd.read_json(\"./resources/arxivData.json\")\n",
    "lines = data.apply(lambda row: (row['title'] + ' ; ' + row['summary'])[:512], axis=1) \\\n",
    "            .apply(lambda line: BOS + line.replace(EOS, ' ') + EOS) \\\n",
    "            .tolist()\n",
    "\n",
    "# if you missed the seminar, download data here - https://yadi.sk/d/_nGyU2IajjR9-w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is __building char-level vocabulary__. Put simply, you need to assemble a list of all unique tokens in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41000\n",
      "[' Dual Recurrent Attention Units for Visual Question Answering ; We propose an architecture for VQA which utilizes recurrent layers to generate visual and textual attention. The memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question. Our single model outperforms the first place winner on the VQA 1.0 dataset, performs within margin to the current state-\\n', ' Sequential Short-Text Classification with Recurrent and Convolutional   Neural Networks ; Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutiona\\n']\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))\n",
    "print(lines[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens =  136\n"
     ]
    }
   ],
   "source": [
    "# get all unique characters from lines (including capital letters and symbols)\n",
    "tokens = set()\n",
    "for line in lines:\n",
    "    tokens.update(line)\n",
    "tokens = sorted(tokens)\n",
    "n_tokens = len(tokens)\n",
    "print ('n_tokens = ',n_tokens)\n",
    "assert 100 < n_tokens < 150\n",
    "assert BOS in tokens, EOS in tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now assign each character with it's index in tokens list. This way we can encode a string into a TF-friendly integer vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of character -> its identifier (index in tokens list)\n",
    "token_to_id = { token:i for i,token in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems alright!\n"
     ]
    }
   ],
   "source": [
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
    "for i in range(n_tokens):\n",
    "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
    "\n",
    "print(\"Seems alright!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final step is to assemble several strings in a integet matrix `[batch_size, text_length]`. \n",
    "\n",
    "The only problem is that each sequence has a different length. We can work around that by padding short sequences with extra _EOS_ or cropping long sequences. Here's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(lines, max_len=None, pad=token_to_id[EOS], dtype='int32'):\n",
    "    \"\"\"Casts a list of lines into tf-digestable matrix\"\"\"\n",
    "    max_len = max_len or max(map(len, lines))\n",
    "    lines_ix = np.full([len(lines), max_len], pad, dtype=dtype)\n",
    "#     print(lines_ix)\n",
    "    for i in range(len(lines)):\n",
    "        line_ix = list(map(token_to_id.get, lines[i][:max_len]))\n",
    "        lines_ix[i, :len(line_ix)] = line_ix\n",
    "    return lines_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 66 67 68  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 66 67 66 68 66 67 66  0  0  0  0  0  0  0]\n",
      " [ 1 66 67 68 18 19 20 21 22 23 24 25 26 17  0]]\n"
     ]
    }
   ],
   "source": [
    "#Example: cast 4 random names to matrices, pad with zeros\n",
    "dummy_lines = [\n",
    "    ' abc\\n',\n",
    "    ' abacaba\\n',\n",
    "    ' abc1234567890\\n',\n",
    "]\n",
    "print(to_matrix(dummy_lines))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Language Model (2 points including training)\n",
    "\n",
    "Just like for N-gram LMs, we want to estimate probability of text as a joint probability of tokens (symbols this time).\n",
    "\n",
    "$$P(X) = \\prod_t P(x_t \\mid x_0, \\dots, x_{t-1}).$$ \n",
    "\n",
    "Instead of counting all possible statistics, we want to train a neural network with parameters $\\theta$ that estimates the conditional probabilities:\n",
    "\n",
    "$$ P(x_t \\mid x_0, \\dots, x_{t-1}) \\approx p(x_t \\mid x_0, \\dots, x_{t-1}, \\theta) $$\n",
    "\n",
    "\n",
    "But before we optimize, we need to define our neural network. Let's start with a fixed-window (aka convolutional) architecture:\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/fixed_window_lm.jpg' width=400px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "keras, L = tf.keras, tf.keras.layers\n",
    "assert tf.__version__.startswith('2'), \"Current tf version: {}; required: 2.0.*\".format(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowLanguageModel(L.Layer):\n",
    "    def __init__(self, n_tokens=n_tokens, emb_size=16, hid_size=64):\n",
    "        \"\"\" \n",
    "        A fixed window model that looks on at least 5 previous symbols.\n",
    "        \n",
    "        Note: fixed window LM is effectively performing a convolution over a sequence of words.\n",
    "        This convolution only looks on current and previous words.\n",
    "        Such convolution can be represented as a sequence of 2 operations:\n",
    "        - pad input vectors by {strides * (filter_size - 1)} zero vectors on the \"left\", do not pad right\n",
    "        - perform regular convolution with {filter_size} and {strides}\n",
    "        - If you're absolutely lost, here's a hint: use ZeroPadding1D and Conv1D from keras.layers\n",
    "        You can stack several convolutions at once\n",
    "        \"\"\"\n",
    "        super().__init__() # initialize base class to track sub-layers, trainable variables, etc.\n",
    "        \n",
    "        #YOUR CODE - create layers/variables and any metadata you want, e.g. self.emb = L.Embedding(...)\n",
    "        \n",
    "        filter_size, strides = 5,1\n",
    "        self.emb = L.Embedding(n_tokens, emb_size) # batch*input -> batch*input*16\n",
    "        self.pad1 = L.ZeroPadding1D(padding=(strides*(filter_size-1),0)) #batch*input -> batch*{input+strides*(filter_size-1)}*16\n",
    "        self.conv1 = L.Conv1D(filters=16, kernel_size=filter_size, strides=strides, activation='relu',padding='valid') #batch*{input+strides*(filter_size-1)}*16\n",
    "        \n",
    "        self.dense1 = L.Dense(units=64, activation='relu') #batch*{input+strides*(filter_size-1)}*64\n",
    "        self.dense2 = L.Dense(units=32, activation='relu') #batch*{input+strides*(filter_size-1)}*32\n",
    "        self.dense3 = L.Dense(units=16, activation='relu') ##batch*{input+strides*(filter_size-1)}*16\n",
    "        self.output_layer = L.Dense(units=n_tokens) #batch*{input+strides*(filter_size-1)}*n_tokens\n",
    "        \n",
    "        #END OF YOUR CODE\n",
    "    \n",
    "    def __call__(self, input_ix):\n",
    "        \"\"\"\n",
    "        compute language model logits given input tokens\n",
    "        :param input_ix: batch of sequences with token indices, tf tensor: int32[batch_size, sequence_length]\n",
    "        :returns: pre-softmax linear outputs of language model [batch_size, sequence_length, n_tokens]\n",
    "            these outputs will be used as logits to compute P(x_t | x_0, ..., x_{t - 1})\n",
    "        \"\"\"\n",
    "        # YOUR CODE - apply layers, see docstring above\n",
    "        input_ = tf.Variable(input_ix)\n",
    "        emb = self.emb(input_)\n",
    "        pad1 = self.pad1(emb)\n",
    "        conv1 = self.conv1(pad1)\n",
    "        dense1 = self.dense1(conv1)\n",
    "        dense2 = self.dense2(dense1)\n",
    "        dense3 = self.dense3(dense2)\n",
    "        output = self.output_layer(dense3)\n",
    "\n",
    "#         print(output.shape)\n",
    "        return output\n",
    "    \n",
    "    def get_possible_next_tokens(self, prefix=BOS, temperature=1.0, max_len=100):\n",
    "        \"\"\" :returns: probabilities of next token, dict {token : prob} for all tokens \"\"\"\n",
    "        prefix_ix = tf.convert_to_tensor(to_matrix([prefix]), tf.int32)\n",
    "        probs = tf.nn.softmax(self(prefix_ix)[0, -1]).numpy()  # shape: [n_tokens]\n",
    "        return dict(zip(tokens, probs))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: ('embedding_1/embeddings:0', 'conv1d_1/kernel:0', 'conv1d_1/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0')\n",
      "Shapes: (TensorShape([136, 16]), TensorShape([5, 16, 16]), TensorShape([16]), TensorShape([16, 64]), TensorShape([64]), TensorShape([64, 32]), TensorShape([32]), TensorShape([32, 16]), TensorShape([16]), TensorShape([16, 136]), TensorShape([136]))\n"
     ]
    }
   ],
   "source": [
    "model = FixedWindowLanguageModel()\n",
    "\n",
    "# note: tensorflow and keras layers create variables only after they're first applied (called)\n",
    "dummy_input_ix = tf.constant(to_matrix(dummy_lines))\n",
    "dummy_logits = model(dummy_input_ix)\n",
    "\n",
    "print('Weights:', tuple(w.name for w in model.trainable_variables))\n",
    "print('Shapes:', tuple(w.shape for w in model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(dummy_logits, tf.Tensor)\n",
    "assert dummy_logits.shape == (len(dummy_lines), max(map(len, dummy_lines)), n_tokens), \"please check output shape\"\n",
    "assert np.all(np.isfinite(dummy_logits)), \"inf/nan encountered\"\n",
    "assert not np.allclose(dummy_logits.numpy().sum(-1), 1), \"please predict linear outputs, don't use softmax (maybe you've just got unlucky)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 15)\n",
      "(3, 15, 136)\n"
     ]
    }
   ],
   "source": [
    "# test for lookahead\n",
    "dummy_input_ix_2 = tf.constant(to_matrix([line[:3] + 'e' * (len(line) - 3) for line in dummy_lines]))\n",
    "dummy_logits_2 = model(dummy_input_ix_2)\n",
    "print(dummy_input_ix_2.shape)\n",
    "print(dummy_logits_2.shape)\n",
    "\n",
    "assert np.allclose(dummy_logits[:, :3] - dummy_logits_2[:, :3], 0), \"your model's predictions depend on FUTURE tokens. \" \\\n",
    "    \" Make sure you don't allow any layers to look ahead of current token.\" \\\n",
    "    \" You can also get this error if your model is not deterministic (e.g. dropout). Disable it for this test.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now tune our network's parameters to minimize categorical crossentropy over training dataset $D$:\n",
    "\n",
    "$$ L = {\\frac1{|D|}} \\sum_{X \\in D} \\sum_{x_i \\in X} - \\log p(x_t \\mid x_1, \\dots, x_{t-1}, \\theta) $$\n",
    "\n",
    "As usual with with neural nets, this optimization is performed via stochastic gradient descent with backprop.  One can also note that minimizing crossentropy is equivalent to minimizing model __perplexity__, KL-divergence or maximizng log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix:\n",
      " [[ 1 66 67 68  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 66 67 66 68 66 67 66  0  0  0  0  0  0  0]\n",
      " [ 1 66 67 68 18 19 20 21 22 23 24 25 26 17  0]]\n",
      "lengths: [ 5  9 15]\n"
     ]
    }
   ],
   "source": [
    "def compute_lengths(input_ix, eos_ix=token_to_id[EOS]):\n",
    "    \"\"\" compute length of each line in input ix (incl. first EOS), int32 vector of shape [batch_size] \"\"\"\n",
    "    count_eos = tf.cumsum(tf.cast(tf.equal(input_ix, eos_ix), tf.int32), axis=1, exclusive=True)\n",
    "    lengths = tf.reduce_sum(tf.cast(tf.equal(count_eos, 0), tf.int32), axis=1)\n",
    "    return lengths\n",
    "\n",
    "print('matrix:\\n', dummy_input_ix.numpy())\n",
    "print('lengths:', compute_lengths(dummy_input_ix).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, input_ix):\n",
    "    \n",
    "    cce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    input_ix = to_matrix(dummy_lines, max_len=15)\n",
    "    input_ix = tf.convert_to_tensor(input_ix, dtype=tf.int32)\n",
    "    logits = model(input_ix[:, :-1])\n",
    "\n",
    "    # print(type(logits))\n",
    "\n",
    "    # Apply softmax operation to get the predicted probability\n",
    "    softmax_output = tf.nn.softmax(logits)\n",
    "    reference_answers = input_ix[:, 1:]\n",
    "    \n",
    "    # Get the \"valid\" length of each line\n",
    "    lengths = compute_lengths(reference_answers)\n",
    "    \n",
    "    ttt = tf.stack(\n",
    "        [tf.sequence_mask(lengths, 14)]*136, axis=-1, name='stack'\n",
    "    )\n",
    "    ans_class = tf.where(ttt,softmax_output,[1])\n",
    "\n",
    "# print(cce(reference_answers, softmax_output))\n",
    "\n",
    "    # Matrix for label, only the label class would be 1\n",
    "#     ans_class = np.zeros(logits.shape)\n",
    "\n",
    "#     for index, batch_length in enumerate(lengths):\n",
    "#         for item in range(batch_length):\n",
    "#             ans_class[index,item,reference_answers[index,item]]=1\n",
    "            \n",
    "#     print(reference_answers.shape, ans_class.shape)\n",
    "\n",
    "    return cce(reference_answers, softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, input_ix):\n",
    "    \"\"\"\n",
    "    :param model: language model that can compute next token logits given token indices\n",
    "    :param input ix: int32 matrix of tokens, shape: [batch_size, length]; padded with eos_ix\n",
    "    \"\"\"\n",
    "\n",
    "    # Your task: implement loss function as per formula above\n",
    "    # your loss should only be computed on actual tokens, excluding padding\n",
    "    # predicting actual tokens and first EOS do count. Subsequent EOS-es don't\n",
    "    # you will likely need to use compute_lengths and/or tf.sequence_mask to get it right.\n",
    "    input_ix = to_matrix(dummy_lines, max_len=15)\n",
    "    input_ix = tf.convert_to_tensor(input_ix, dtype=tf.int32)\n",
    "    logits = model(input_ix[:, :-1])\n",
    "    \n",
    "    # Apply softmax operation to get the predicted probability\n",
    "    softmax_output = tf.nn.softmax(logits)\n",
    "    reference_answers = input_ix[:, 1:]\n",
    "    \n",
    "    # Get the \"valid\" length of each line\n",
    "    lengths = compute_lengths(reference_answers)\n",
    "\n",
    "    ttt = tf.stack(\n",
    "        [tf.sequence_mask(lengths, logits.shape[1])]*logits.shape[2], axis=-1, name='stack'\n",
    "    )\n",
    "    ans_class = tf.where(ttt,softmax_output,[1])  \n",
    "    print(ans_class.shape)\n",
    "    \n",
    "    # Matrix for label, only the label class would be 1\n",
    "#     ans_class = np.zeros(logits.shape)\n",
    "\n",
    "#     for index, batch_length in enumerate(lengths):\n",
    "#         for item in range(batch_length):\n",
    "#             ans_class[index,item,reference_answers[index,item]]=1\n",
    "\n",
    "    # Compute the loss\n",
    "    ans=tf.Variable(0)\n",
    "    for index, batch_length in enumerate(lengths):\n",
    "        for item in range(batch_length):\n",
    "            ans = ans - np.log(np.tensordot(softmax_output[index,item], ans_class[index,item], axes=1))\n",
    "    \n",
    "#     return tf.convert_to_tensor(ans/len(lengths))\n",
    "    return ans / len(lengths)\n",
    "#     return ans/len(lengths)\n",
    "    \n",
    "#     return <YOUR CODE: return scalar loss>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0794697, shape=(), dtype=float32) tf.Tensor(0.0794697, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss_1 = compute_loss(model, to_matrix(dummy_lines, max_len=15))\n",
    "loss_2 = compute_loss(model, to_matrix(dummy_lines, max_len=16))\n",
    "print(loss_1, loss_2)\n",
    "assert (np.ndim(loss_1) == 0) and (0 < loss_1 < 100), \"loss must be a positive scalar\"\n",
    "assert np.allclose(loss_1, loss_2), 'do not include  AFTER first EOS into loss. '\\\n",
    "    'Hint: use tf.sequence_mask. Beware +/-1 errors. And be careful when averaging!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.07849973>"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(model, input_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 15)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'> (3, 14, 136)\n",
      "tf.Tensor(4.911486, shape=(), dtype=float32)\n",
      "42.57628154754639 <class 'numpy.float64'> tf.Tensor([42.57628155], shape=(1,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "model = FixedWindowLanguageModel()\n",
    "cce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "input_ix = to_matrix(dummy_lines, max_len=15)\n",
    "input_ix = tf.convert_to_tensor(input_ix, dtype=tf.int32)\n",
    "print(input_ix.shape)\n",
    "logits = model(input_ix[:, :-1])\n",
    "\n",
    "print(type(logits), logits.shape)\n",
    "\n",
    "# Apply softmax operation to get the predicted probability\n",
    "softmax_output = tf.nn.softmax(logits)\n",
    "reference_answers = input_ix[:, 1:]\n",
    "# Get the \"valid\" length of each line\n",
    "lengths = compute_lengths(reference_answers)\n",
    "\n",
    "ttt = tf.stack(\n",
    "    [tf.sequence_mask(lengths, 14)]*136, axis=-1, name='stack'\n",
    ")\n",
    "ans_class = tf.where(ttt,softmax_output,[1])\n",
    "\n",
    "print(cce(reference_answers, softmax_output))\n",
    "\n",
    "# Matrix for label, only the label class would be 1\n",
    "# ans_class = np.zeros(logits.shape)\n",
    "\n",
    "# for index, batch_length in enumerate(lengths):\n",
    "#     for item in range(batch_length):\n",
    "#         ans_class[index,item,reference_answers[index,item]]=1\n",
    "        \n",
    "# Compute the loss\n",
    "ans=0\n",
    "for index, batch_length in enumerate(lengths):\n",
    "    for item in range(batch_length):\n",
    "        ans+= -np.log(np.tensordot(softmax_output[index,item], ans_class[index,item], axes=1))\n",
    "print(ans/len(lengths), type(ans),tf.convert_to_tensor(np.array([ans/len(lengths)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 136), dtype=float32, numpy=\n",
       "array([[0.00736721, 0.00734396, 0.00733888, 0.00734308, 0.00736803,\n",
       "        0.00731878, 0.00738927, 0.00736942, 0.00734776, 0.00734517,\n",
       "        0.00735731, 0.00734414, 0.00735471, 0.00732092, 0.00733107,\n",
       "        0.00734776, 0.00734092, 0.00735567, 0.00737733, 0.00736269,\n",
       "        0.00734484, 0.00734823, 0.00733105, 0.00737406, 0.00732975,\n",
       "        0.00736966, 0.00733995, 0.00732369, 0.00737513, 0.00734815,\n",
       "        0.00738043, 0.00734851, 0.00736184, 0.0073733 , 0.00735642,\n",
       "        0.00738738, 0.00733602, 0.00734239, 0.00736268, 0.00733097,\n",
       "        0.00735451, 0.0073514 , 0.00739513, 0.00733261, 0.00736622,\n",
       "        0.00734492, 0.00735499, 0.00732579, 0.00734865, 0.0073759 ,\n",
       "        0.00737092, 0.00733145, 0.00735185, 0.00736958, 0.00732758,\n",
       "        0.0073273 , 0.00734095, 0.00736864, 0.0073751 , 0.0073791 ,\n",
       "        0.00732839, 0.00736048, 0.00735588, 0.00737075, 0.00738889,\n",
       "        0.00734924, 0.00736537, 0.0073291 , 0.00733673, 0.00732583,\n",
       "        0.00736684, 0.00736963, 0.00733046, 0.0073312 , 0.00734216,\n",
       "        0.00735935, 0.00733388, 0.00733868, 0.0073392 , 0.00734061,\n",
       "        0.00737039, 0.0073823 , 0.00738099, 0.00737516, 0.00732814,\n",
       "        0.00734186, 0.00734507, 0.00736565, 0.00736376, 0.0073655 ,\n",
       "        0.00735401, 0.00732006, 0.00731934, 0.0073556 , 0.00734321,\n",
       "        0.00737738, 0.00736976, 0.00731148, 0.00735416, 0.00740087,\n",
       "        0.0073343 , 0.00738062, 0.00738103, 0.00737077, 0.00733566,\n",
       "        0.00733986, 0.00734294, 0.00734775, 0.00738345, 0.00738393,\n",
       "        0.00735681, 0.00735439, 0.00735593, 0.00733528, 0.00735227,\n",
       "        0.0073463 , 0.00732082, 0.00733931, 0.00734284, 0.00732818,\n",
       "        0.00734884, 0.00733572, 0.0073265 , 0.00739655, 0.00737023,\n",
       "        0.00737845, 0.00733295, 0.00733418, 0.00733045, 0.00738452,\n",
       "        0.00736971, 0.007383  , 0.00736849, 0.00733699, 0.00735144,\n",
       "        0.00735115],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_class[0,3:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 14])"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_answers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(14,), dtype=int32, numpy=\n",
       "array([66, 67, 68,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = tf.sequence_mask(tf.reshape(lengths,(3,1)), 136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sequence_mask(, 136)[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(136,), dtype=bool, numpy=\n",
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False])>"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sequence_mask(reference_answers, 136)[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Value for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, int64, complex64, complex128\n\t; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-404-0f570c7511a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m   \"\"\"\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6066\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6067\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6068\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6069\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6070\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Value for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, int64, complex64, complex128\n\t; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul]"
     ]
    }
   ],
   "source": [
    "tt * logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((3,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 4,  8, 14], dtype=int32)>"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = tf.stack(\n",
    "    [tf.sequence_mask(lengths, 14)]*136, axis=-1, name='stack'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 14), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True]])>"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sequence_mask(lengths, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 136), dtype=bool, numpy=\n",
       "array([[ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False]])>"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sequence_mask(tf.sequence_mask(lengths, 14),136)[0,0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 136), dtype=float32, numpy=\n",
       "array([[4.99911338e-01, 1.06366903e-11, 9.71811034e-11, 5.42247237e-12,\n",
       "        1.08566220e-11, 2.17381061e-12, 1.43585942e-11, 3.77361510e-12,\n",
       "        1.13063829e-11, 5.00095451e-12, 2.23193391e-11, 1.73416871e-11,\n",
       "        3.63908036e-12, 1.73231082e-11, 2.59747258e-11, 1.62238029e-12,\n",
       "        4.12292639e-12, 1.04910634e-06, 5.00048637e-01, 3.47349851e-06,\n",
       "        4.85797642e-08, 5.98829160e-07, 2.57349537e-08, 1.89167899e-08,\n",
       "        1.02816977e-09, 1.79452320e-09, 1.47024295e-08, 1.76284924e-11,\n",
       "        3.85473554e-12, 1.04387211e-11, 5.19473744e-12, 6.79418882e-11,\n",
       "        1.27283149e-11, 2.83062874e-12, 5.28787951e-11, 8.14677405e-12,\n",
       "        4.28945508e-12, 1.25322001e-11, 1.18826424e-11, 1.63398323e-11,\n",
       "        8.10934913e-12, 3.34437946e-12, 1.58730884e-12, 9.69890834e-11,\n",
       "        5.40591833e-12, 3.04225937e-12, 5.12593874e-12, 8.77065608e-12,\n",
       "        9.80812220e-12, 1.25055044e-11, 3.96201605e-12, 7.31477541e-13,\n",
       "        7.18611672e-12, 3.67386191e-11, 4.73544329e-11, 5.10927672e-12,\n",
       "        9.60750663e-12, 9.96366097e-12, 5.76078595e-12, 5.16401418e-12,\n",
       "        1.48471131e-11, 7.36675217e-12, 2.97132215e-11, 1.95368704e-11,\n",
       "        8.70748265e-12, 1.16944249e-11, 1.66227655e-05, 4.13303411e-07,\n",
       "        1.78120717e-05, 1.38697300e-12, 5.89897402e-12, 8.02460182e-12,\n",
       "        9.50920506e-12, 1.90038072e-11, 5.39047409e-12, 5.41848441e-11,\n",
       "        3.26367648e-11, 8.98508785e-12, 2.93712927e-12, 3.90093478e-11,\n",
       "        1.49269035e-11, 9.39469249e-12, 6.39085798e-11, 7.30234536e-12,\n",
       "        9.65438580e-12, 1.80050658e-12, 3.60945440e-11, 2.01438644e-10,\n",
       "        8.40849543e-11, 4.01116848e-11, 1.46635915e-11, 3.57013724e-12,\n",
       "        7.37089903e-12, 3.29759692e-11, 8.21114010e-12, 2.78156248e-11,\n",
       "        1.05577754e-12, 3.33927395e-12, 5.15446887e-12, 3.14471561e-12,\n",
       "        4.89762744e-12, 4.57267254e-12, 5.81007811e-12, 1.80883399e-11,\n",
       "        1.85417515e-11, 4.31803573e-13, 1.58499533e-11, 3.67655212e-12,\n",
       "        4.63199487e-12, 1.18472301e-10, 2.61189377e-12, 3.05957273e-11,\n",
       "        2.33158146e-11, 3.33606059e-11, 2.98535198e-12, 5.95289920e-12,\n",
       "        4.49429860e-12, 4.19381587e-12, 1.71812183e-11, 1.08082545e-11,\n",
       "        2.25669635e-12, 1.10464936e-11, 1.33684756e-11, 3.70174941e-12,\n",
       "        7.29021010e-12, 1.79259541e-11, 3.10797325e-11, 1.23270621e-11,\n",
       "        4.01701762e-11, 1.32436614e-11, 2.04599428e-11, 8.29051758e-12,\n",
       "        4.61071676e-12, 6.24960153e-12, 5.97849027e-12, 2.56178169e-11],\n",
       "       [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.where(ttt,softmax_output,[1])[0,3:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 1, 136])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([14, 136])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(14, 136), dtype=float32, numpy=\n",
       "array([[1.40725933e-06, 1.31397659e-09, 2.32128139e-09, ...,\n",
       "        1.04474407e-09, 1.99424188e-09, 2.11545448e-09],\n",
       "       [1.96677760e-07, 2.37095906e-13, 1.32016175e-12, ...,\n",
       "        5.36493322e-13, 2.06700239e-12, 6.81354712e-13],\n",
       "       [7.70052884e-06, 9.36236332e-12, 4.96384461e-11, ...,\n",
       "        5.16133273e-12, 8.44878941e-12, 1.90257012e-11],\n",
       "       ...,\n",
       "       [9.99998927e-01, 2.95619560e-15, 1.11746732e-13, ...,\n",
       "        2.28909892e-15, 4.25515175e-15, 1.46480880e-14],\n",
       "       [9.99998927e-01, 2.94726660e-15, 1.11732245e-13, ...,\n",
       "        2.28087042e-15, 4.23596010e-15, 1.45787415e-14],\n",
       "       [9.99998927e-01, 2.94199827e-15, 1.11740979e-13, ...,\n",
       "        2.27352135e-15, 4.22140977e-15, 1.45221258e-14]], dtype=float32)>"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_output[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Value for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, int64, complex64, complex128\n\t; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-413-2d8971d09640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m   \"\"\"\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6066\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6067\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6068\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6069\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6070\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/sudoku382/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Value for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, int64, complex64, complex128\n\t; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul]"
     ]
    }
   ],
   "source": [
    "tt[0,0,:] * logits[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.9128823, shape=(), dtype=float32) tf.Tensor(4.9128823, shape=(), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "loss_1 = compute_loss(model, to_matrix(dummy_lines, max_len=15))\n",
    "loss_2 = compute_loss(model, to_matrix(dummy_lines, max_len=16))\n",
    "print(loss_1, loss_2,type(loss_1))\n",
    "assert (np.ndim(loss_1) == 0) and (0 < loss_1 < 100), \"loss must be a positive scalar\"\n",
    "assert np.allclose(loss_1, loss_2), 'do not include  AFTER first EOS into loss. '\\\n",
    "    'Hint: use tf.sequence_mask. Beware +/-1 errors. And be careful when averaging!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "You will need two functions: one to compute test loss and another to generate samples. For your convenience, we implemented them both in your stead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_lines(model, dev_lines, batch_size):\n",
    "    \"\"\" computes average loss over the entire dataset \"\"\"\n",
    "    dev_loss_num, dev_loss_len = 0., 0.\n",
    "    for i in range(0, len(dev_lines), batch_size):\n",
    "        batch_ix = to_matrix(dev_lines[i: i + batch_size])\n",
    "        dev_loss_num += compute_loss(model, batch_ix) * len(batch_ix)\n",
    "        dev_loss_len += len(batch_ix)\n",
    "    return dev_loss_num / dev_loss_len\n",
    "\n",
    "def generate(model, prefix=BOS, temperature=1.0, max_len=100):\n",
    "    \"\"\"\n",
    "    Samples output sequence from probability distribution obtained by model\n",
    "    :param temperature: samples proportionally to model probabilities ^ temperature\n",
    "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        token_probs = model.get_possible_next_tokens(prefix)\n",
    "#         print(token_probs)\n",
    "        tokens, probs = zip(*token_probs.items())\n",
    "        if temperature == 0:\n",
    "            next_token = tokens[np.argmax(probs)]\n",
    "        else:\n",
    "            probs = np.array([p ** (1. / temperature) for p in probs])\n",
    "            probs /= sum(probs)\n",
    "            next_token = np.random.choice(tokens, p=probs)\n",
    "        \n",
    "        prefix += next_token\n",
    "        if next_token == EOS or len(prefix) > max_len: break\n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Finally, let's train our model on minibatches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample before training: Bridgingd1$:|7$LO|_<Ox'c\\2]?zt<K[pPfzs|^w%C:&QJyiH2{|\\@=0W,sGMQ<)S_I?C<<63S\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_lines, dev_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
    "model = FixedWindowLanguageModel()\n",
    "\n",
    "batch_size = 256\n",
    "score_dev_every = 250\n",
    "train_history, dev_history = [], []\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "# score untrained model\n",
    "dev_history.append((0, score_lines(model, dev_lines, batch_size)))\n",
    "print(\"Sample before training:\", generate(model, 'Bridging'))\n",
    "# print(\"Sample before training:\", generate(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated examples (tau=0.5):\n",
      " abc\n",
      "\n",
      " abc1234567890\n",
      "\n",
      " abc\n",
      "\n",
      "Scoring dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:30<00:00, 33.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#999 Dev loss: 0.079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "from tqdm import trange\n",
    "\n",
    "for i in trange(len(train_history), 1000):\n",
    "    batch = to_matrix(sample(train_lines, batch_size))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "#         tape.watch(model.trainable_variables)\n",
    "        loss_i = compute_loss(model, batch)\n",
    "#         print(loss_i)\n",
    "        \n",
    "    grads = tape.gradient(loss_i, model.trainable_variables)\n",
    "#     grads = tape.gradient(tf.convert_to_tensor(loss_i), model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    train_history.append((i, loss_i.numpy()))\n",
    "    \n",
    "    if (i + 1) % 50 == 0:\n",
    "        clear_output(True)\n",
    "        plt.scatter(*zip(*train_history), alpha=0.1, label='train_loss')\n",
    "        if len(dev_history):\n",
    "            plt.plot(*zip(*dev_history), color='red', label='dev_loss')\n",
    "        plt.legend(); plt.grid(); plt.show()\n",
    "        print(\"Generated examples (tau=0.5):\")\n",
    "        for _ in range(3):\n",
    "            print(generate(model, temperature=0.5))\n",
    "    \n",
    "    if (i + 1) % score_dev_every == 0:\n",
    "        print(\"Scoring dev...\")\n",
    "        dev_history.append((i, score_lines(model, dev_lines, batch_size)))\n",
    "        print('#%i Dev loss: %.3f' % dev_history[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dev loss: tf.Tensor(0.07869124, shape=(), dtype=float32)\n",
      " abc1234567890\n",
      "\n",
      " abc\n",
      "\n",
      " abc\n",
      "\n",
      " abc1234567890\n",
      "\n",
      " abc\n",
      "\n",
      " abc1234567890\n",
      "\n",
      " abacaba\n",
      "\n",
      " abc1234567890\n",
      "\n",
      " abc1234567890\n",
      "\n",
      " abc1234567890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
    "print(\"Final dev loss:\", dev_history[-1][-1])\n",
    "\n",
    "for i in range(10):\n",
    "    print(generate(model, temperature=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Language Models (3 points including training)\n",
    "\n",
    "Fixed-size architectures are reasonably good when capturing short-term dependencies, but their design prevents them from capturing any signal outside their window. We can mitigate this problem by using a __recurrent neural network__:\n",
    "\n",
    "$$ h_0 = \\vec 0 ; \\quad h_{t+1} = RNN(x_t, h_t) $$\n",
    "\n",
    "$$ p(x_t \\mid x_0, \\dots, x_{t-1}, \\theta) = dense_{softmax}(h_{t-1}) $$\n",
    "\n",
    "Such model processes one token at a time, left to right, and maintains a hidden state vector between them. Theoretically, it can learn arbitrarily long temporal dependencies given large enough hidden size.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/rnn_lm.jpg' width=480px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(L.Layer):\n",
    "    def __init__(self, n_tokens=n_tokens, emb_size=16, hid_size=256):\n",
    "        \"\"\" \n",
    "        Build a recurrent language model.\n",
    "        You are free to choose anything you want, but the recommended architecture is\n",
    "        - token embeddings\n",
    "        - one or more LSTM/GRU layers with hid size\n",
    "        - linear layer to predict logits\n",
    "        \"\"\"\n",
    "        super().__init__() # initialize base class to track sub-layers, trainable variables, etc.\n",
    "        \n",
    "        # YOUR CODE - create layers/variables/etc\n",
    "        \n",
    "        filter_size, strides = 5,1\n",
    "        self.emb = L.Embedding(n_tokens, emb_size) # batch*input -> batch*input*emb_size\n",
    "        self.rnn1 = L.GRU(hid_size, return_sequences=True) # batch*input*emb_size -> batch*input*hid_size\n",
    "        self.rnn2 = L.GRU(hid_size, return_sequences=True) # batch*input*hid_size -> batch*input*hid_size\n",
    "        self.output_layer = L.Dense(n_tokens) # batch*input*hid_size-> batch*input*n_token\n",
    "        \n",
    "        \n",
    "#         self.pad1 = L.ZeroPadding1D(padding=(strides*(filter_size-1),0)) #batch*input -> batch*{input+strides*(filter_size-1)}*16\n",
    "#         self.conv1 = L.Conv1D(filters=16, kernel_size=filter_size, strides=strides, activation='relu',padding='valid') #batch*{input+strides*(filter_size-1)}*16\n",
    "        \n",
    "#         self.dense1 = L.Dense(units=64, activation='relu') #batch*{input+strides*(filter_size-1)}*64\n",
    "#         self.dense2 = L.Dense(units=32, activation='relu') #batch*{input+strides*(filter_size-1)}*32\n",
    "#         self.dense3 = L.Dense(units=16, activation='relu') ##batch*{input+strides*(filter_size-1)}*16\n",
    "#         self.output_layer = L.Dense(units=n_tokens) #batch*{input+strides*(filter_size-1)}*n_tokens\n",
    "        \n",
    "        #END OF YOUR CODE\n",
    "    \n",
    "    def __call__(self, input_ix):\n",
    "        \"\"\"\n",
    "        compute language model logits given input tokens\n",
    "        :param input_ix: batch of sequences with token indices, tf tensor: int32[batch_size, sequence_length]\n",
    "        :returns: pre-softmax linear outputs of language model [batch_size, sequence_length, n_tokens]\n",
    "            these outputs will be used as logits to compute P(x_t | x_0, ..., x_{t - 1})\n",
    "        \"\"\"\n",
    "        input_ = tf.Variable(input_ix)\n",
    "        emb = self.emb(input_)\n",
    "        rnn1 = self.rnn1(emb)\n",
    "        rnn2 = self.rnn2(rnn1)\n",
    "        output = self.output_layer(rnn2)\n",
    "                                    \n",
    "#         pad1 = self.pad1(emb)\n",
    "#         conv1 = self.conv1(pad1)\n",
    "#         dense1 = self.dense1(conv1)\n",
    "#         dense2 = self.dense2(dense1)\n",
    "#         dense3 = self.dense3(dense2)\n",
    "#         output = self.output_layer(dense3)\n",
    "\n",
    "#         print(output.shape)\n",
    "        return output\n",
    "#         return <...>\n",
    "    \n",
    "    def get_possible_next_tokens(self, prefix=BOS, temperature=1.0, max_len=100):\n",
    "        \"\"\" :returns: probabilities of next token, dict {token : prob} for all tokens \"\"\"\n",
    "        prefix_ix = tf.convert_to_tensor(to_matrix([prefix]), tf.int32)\n",
    "        probs = tf.nn.softmax(self(prefix_ix)[0, -1]).numpy()  # shape: [n_tokens]\n",
    "        return dict(zip(tokens, probs))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: ('embedding_7/embeddings:0', 'gru/gru_cell/kernel:0', 'gru/gru_cell/recurrent_kernel:0', 'gru/gru_cell/bias:0', 'gru_1/gru_cell_1/kernel:0', 'gru_1/gru_cell_1/recurrent_kernel:0', 'gru_1/gru_cell_1/bias:0', 'dense_28/kernel:0', 'dense_28/bias:0')\n"
     ]
    }
   ],
   "source": [
    "model = RNNLanguageModel()\n",
    "\n",
    "# note: tensorflow and keras layers create variables only after they're first applied (called)\n",
    "dummy_input_ix = tf.constant(to_matrix(dummy_lines))\n",
    "dummy_logits = model(dummy_input_ix)\n",
    "\n",
    "assert isinstance(dummy_logits, tf.Tensor)\n",
    "assert dummy_logits.shape == (len(dummy_lines), max(map(len, dummy_lines)), n_tokens), \"please check output shape\"\n",
    "assert np.all(np.isfinite(dummy_logits)), \"inf/nan encountered\"\n",
    "assert not np.allclose(dummy_logits.numpy().sum(-1), 1), \"please predict linear outputs, don't use softmax (maybe you've just got unlucky)\"\n",
    "print('Weights:', tuple(w.name for w in model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for lookahead\n",
    "dummy_input_ix_2 = tf.constant(to_matrix([line[:3] + 'e' * (len(line) - 3) for line in dummy_lines]))\n",
    "dummy_logits_2 = model(dummy_input_ix_2)\n",
    "\n",
    "assert np.allclose(dummy_logits[:, :3] - dummy_logits_2[:, :3], 0), \"your model's predictions depend on FUTURE tokens. \" \\\n",
    "    \" Make sure you don't allow any layers to look ahead of current token.\" \\\n",
    "    \" You can also get this error if your model is not deterministic (e.g. dropout). Disable it for this test.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN training\n",
    "\n",
    "Our RNN language model should optimize the same loss function as fixed-window model. But there's a catch. Since RNN recurrently multiplies gradients through many time-steps, gradient values may explode, [ruining](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/nan.jpg) your model.\n",
    "The common solution to that problem is to clip gradients either [individually](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/clip_by_value) or [globally](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/clip_by_global_norm).\n",
    "\n",
    "Your task here is to prepare tensorflow graph that would minimize the same loss function. If you encounter large loss fluctuations during training, please add gradient clipping using urls above.\n",
    "\n",
    "_Note: gradient clipping is not exclusive to RNNs. Convolutional networks with enough depth often suffer from the same issue._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample before training: Bridging-nGi4R~TsBIr:awsPoln\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64         # <-- please tune batch size to fit your CPU/GPU configuration\n",
    "score_dev_every = 250\n",
    "train_history, dev_history = [], []\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "# score untrained model\n",
    "dev_history.append((0, score_lines(model, dev_lines, batch_size)))\n",
    "print(\"Sample before training:\", generate(model, 'Bridging'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated examples (tau=0.5):\n",
      " abc\n",
      "\n",
      " abc\n",
      "\n",
      " abc\n",
      "\n",
      "Scoring dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 937/937 [01:34<00:00,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#999 Dev loss: 0.079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in trange(len(train_history), 1000):\n",
    "    batch = to_matrix(sample(train_lines, batch_size))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_i = compute_loss(model, batch)\n",
    "        \n",
    "    grads = tape.gradient(loss_i, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    train_history.append((i, loss_i.numpy()))\n",
    "    \n",
    "    if (i + 1) % 50 == 0:\n",
    "        clear_output(True)\n",
    "        plt.scatter(*zip(*train_history), alpha=0.1, label='train_loss')\n",
    "        if len(dev_history):\n",
    "            plt.plot(*zip(*dev_history), color='red', label='dev_loss')\n",
    "        plt.legend(); plt.grid(); plt.show()\n",
    "        print(\"Generated examples (tau=0.5):\")\n",
    "        for _ in range(3):\n",
    "            print(generate(model, temperature=0.5))\n",
    "    \n",
    "    if (i + 1) % score_dev_every == 0:\n",
    "        print(\"Scoring dev...\")\n",
    "        dev_history.append((i, score_lines(model, dev_lines, batch_size)))\n",
    "        print('#%i Dev loss: %.3f' % dev_history[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dev loss: tf.Tensor(0.07890152, shape=(), dtype=float32)\n",
      " abc\n",
      "\n",
      " abacaba\n",
      "\n",
      " abc1234567890\n",
      "\n",
      " abc\n",
      "\n",
      " abc\n",
      "\n",
      " abc\n",
      "\n",
      " abc\n",
      "\n",
      " abc\n",
      "\n",
      " abc1234567890\n",
      "\n",
      " abacaba\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
    "print(\"Final dev loss:\", dev_history[-1][-1])\n",
    "for i in range(10):\n",
    "    print(generate(model, temperature=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative sampling strategies (1 point)\n",
    "\n",
    "So far we've sampled tokens from the model in proportion with their probability.\n",
    "However, this approach can sometimes generate nonsense words due to the fact that softmax probabilities of these words are never exactly zero. This issue can be somewhat mitigated with sampling temperature, but low temperature harms sampling diversity. Can we remove the nonsense words without sacrificing diversity? __Yes, we can!__ But it takes a different sampling strategy.\n",
    "\n",
    "__Top-k sampling:__ on each step, sample the next token from __k most likely__ candidates from the language model.\n",
    "\n",
    "Suppose $k=3$ and the token probabilities are $p=[0.1, 0.35, 0.05, 0.2, 0.3]$. You first need to select $k$ most likely words and set the probability of the rest to zero: $\\hat p=[0.0, 0.35, 0.0, 0.2, 0.3]$ and re-normalize: \n",
    "$p^*\\approx[0.0, 0.412, 0.0, 0.235, 0.353]$.\n",
    "\n",
    "__Nucleus sampling:__ similar to top-k sampling, but this time we select $k$ dynamically. In nucleous sampling, we sample from top-__N%__ fraction of the probability mass.\n",
    "\n",
    "Using the same  $p=[0.1, 0.35, 0.05, 0.2, 0.3]$ and nucleous N=0.9, the nucleous words consist of:\n",
    "1. most likely token $w_2$, because $p(w_2) < N$\n",
    "2. second most likely token $w_5$, $p(w_2) + p(w_5) = 0.65 < N$\n",
    "3. third most likely token $w_4$ because $p(w_2) + p(w_5) + p(w_4) = 0.85 < N$\n",
    "\n",
    "And thats it, because the next most likely word would overflow: $p(w_2) + p(w_5) + p(w_4) + p(w_1) = 0.95 > N$.\n",
    "\n",
    "After you've selected the nucleous words, you need to re-normalize them as in top-k sampling and generate the next token.\n",
    "\n",
    "__Your task__ is to implement nucleus sampling variant and see if its any good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nucleus(model, prefix=BOS, nucleus=0.9, max_len=100):\n",
    "    \"\"\"\n",
    "    Generate a sequence with nucleous sampling\n",
    "    :param prefix: a string containing space-separated previous tokens\n",
    "    :param nucleus: N from the formulae above, N \\in [0, 1]\n",
    "    :param max_len: generate sequences with at most this many tokens, including prefix\n",
    "    \n",
    "    :note: make sure that nucleous always contains at least one word, even if p(w*) > nucleus\n",
    "    \n",
    "    \"\"\"\n",
    "    while True:\n",
    "        token_probs = model.get_possible_next_tokens(prefix)\n",
    "        tokens, probs = zip(*token_probs.items())\n",
    "        \n",
    "\n",
    "        <YOUR CODE HERE>\n",
    "        \n",
    "        prefix += <YOUR CODE>\n",
    "        if next_token == EOS or len(prefix) > max_len: break\n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(generate_nucleous(model, nucleous_size=PLAY_WITH_ME_SENPAI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus quest I: Beam Search (2 pts incl. samples)\n",
    "\n",
    "At times, you don't really want the model to generate diverse outputs as much as you want a __single most likely hypothesis.__ A single best translation, most likely continuation of the search query given prefix, etc. Except, you can't get it. \n",
    "\n",
    "In order to find the exact most likely sequence containing 10 tokens, you would need to enumerate all $|V|^{10}$ possible hypotheses. In practice, 9 times out of 10 you will instead find an approximate most likely output using __beam search__.\n",
    "\n",
    "Here's how it works:\n",
    "0. Initial `beam` = [prefix], max beam_size = k\n",
    "1. for T steps:\n",
    "2. ` ... ` generate all possible next tokens for all hypotheses in beam, formulate `len(beam) * len(vocab)` candidates\n",
    "3. ` ... ` select beam_size best for all candidates as new `beam`\n",
    "4. Select best hypothesis (-es?) from beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <meta charset=\"utf-8\">\n",
       "        <title>Bokeh Plot</title>\n",
       "        \n",
       "<link rel=\"stylesheet\" href=\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\" type=\"text/css\" />\n",
       "        \n",
       "<script type=\"text/javascript\" src=\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.js\"></script>\n",
       "<script type=\"text/javascript\">\n",
       "    Bokeh.set_log_level(\"info\");\n",
       "</script>\n",
       "        <style>\n",
       "          html {\n",
       "            width: 100%;\n",
       "            height: 100%;\n",
       "          }\n",
       "          body {\n",
       "            width: 90%;\n",
       "            height: 100%;\n",
       "            margin: auto;\n",
       "          }\n",
       "        </style>\n",
       "    </head>\n",
       "    <body>\n",
       "        \n",
       "        <div class=\"bk-root\">\n",
       "            <div class=\"bk-plotdiv\" id=\"ff8c3f31-952d-4c2f-8b58-13e7cec51b58\"></div>\n",
       "        </div>\n",
       "        \n",
       "        <script type=\"text/javascript\">\n",
       "            (function() {\n",
       "          var fn = function() {\n",
       "            Bokeh.safely(function() {\n",
       "              var docs_json = {\"ba84f797-d201-498d-a731-5adafa5447b7\":{\"roots\":{\"references\":[{\"attributes\":{\"plot\":null,\"text\":\"Beam search\"},\"id\":\"5af81591-5793-4721-a459-e0a4ca700855\",\"type\":\"Title\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"size\":{\"units\":\"screen\",\"value\":24},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"43ae4eb3-d229-4335-a758-a5d18149bd65\",\"type\":\"Circle\"},{\"attributes\":{\"bounds\":[-10.0,20.0],\"callback\":null,\"end\":12,\"js_property_callbacks\":{\"change:end\":[{\"id\":\"eab13fb4-9405-4d14-8bf6-46c6f698b4bb\",\"type\":\"CustomJS\"}]},\"start\":-1},\"id\":\"14e9976c-9458-4bce-be96-da2f3c304cec\",\"type\":\"Range1d\"},{\"attributes\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"id\":\"2e9dcf01-3f79-4337-b7dd-26f0525180b9\",\"type\":\"CDSView\"},{\"attributes\":{\"data_source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"08bbcb52-00b5-4803-9e7a-9252db09708d\",\"type\":\"Text\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"tokens\",\"nonselection_glyph\":{\"id\":\"3621fd59-3cb3-4305-8802-6f7f75a23fd6\",\"type\":\"Text\"},\"selection_glyph\":null,\"view\":{\"id\":\"ddf587aa-019f-43a1-af8c-52144e732785\",\"type\":\"CDSView\"}},\"id\":\"a2ab1853-0966-4519-874f-956f801f1c72\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"id\",\"parent_id\",\"children_ids\",\"is_best\",\"depth\",\"hypo_i\",\"token\",\"token_id\",\"x\",\"y\",\"circle_fill_color\",\"line_color\",\"line_width\",\"edge_xx\",\"edge_yy\",\"token_text\",\"token_font_size\",\"hypo_i_text\",\"hypo_i_offset\",\"_on_hover_token\",\"_on_hover_token_id\",\"_on_hover_score\"],\"data\":{\"_on_hover_score\":[\"-4.7282\",\"-4.2071\",\"-4.8782\",\"-1.3577\",\"-4.0420\",\"-4.2097\",\"-4.5624\",\"-3.2410\",\"-6.9225\",\"-4.6717\",\"-2.8497\",\"-3.8023\",\"-4.1460\",\"-3.6443\",\"-4.3135\",\"-4.7070\",\"-3.1373\",\"-4.5105\",\"-6.1526\",\"-3.0974\",\"-3.7421\",\"-3.4956\",\"0.0000\",\"-5.2694\",\"-4.4104\",\"-6.1752\",\"-3.7617\",\"-4.6281\",\"-6.5626\",\"-3.7175\",\"-0.7384\",\"-4.5439\",\"-5.2967\",\"-3.6831\",\"-5.2153\",\"-3.7830\",\"-3.1335\",\"-0.8716\",\"-3.7312\",\"-3.8895\",\"-6.5984\"],\"_on_hover_token\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"_on_hover_token_id\":[1,780,25,2482,15356,2482,1879,3,17084,25,13862,5,1,7691,1,15356,25,15328,4255,11685,11,3487,-1,21342,15356,4,21342,11,1781,30,3622,3,25,1078,1,5093,26478,11,2451,13862,22624],\"children_ids\":[[],[[2,3]],[[10,3],[10,2],[10,1],[10,0]],[[4,3],[4,1],[4,0]],[[9,3],[9,1],[9,0]],[],[],[[6,1]],[],[[8,2]],[[5,0]],[[2,2]],[],[[4,2]],[],[[7,2]],[[6,2],[6,0]],[],[],[[5,1]],[[2,1]],[],[[1,3],[1,2],[1,1],[1,0]],[[9,2]],[[8,1]],[],[[7,3],[7,1]],[],[],[[5,2]],[[2,0]],[],[],[[8,3],[8,0]],[],[[7,0]],[],[[3,3],[3,2],[3,1],[3,0]],[[5,3]],[[6,3]],[]],\"circle_fill_color\":[\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\"],\"depth\":[7,1,9,3,8,2,6,5,10,7,4,1,9,3,8,6,5,2,10,4,1,3,0,8,7,9,6,2,10,4,1,5,8,7,9,6,3,2,4,5,10],\"edge_xx\":[[6.0,7.0],[0.0,1.0],[8.0,9.0],[2.0,3.0],[7.0,8.0],[1.0,2.0],[5.0,6.0],[4.0,5.0],[9.0,10.0],[6.0,7.0],[3.0,4.0],[0.0,1.0],[8.0,9.0],[2.0,3.0],[7.0,8.0],[5.0,6.0],[4.0,5.0],[1.0,2.0],[9.0,10.0],[3.0,4.0],[0.0,1.0],[2.0,3.0],[0.0,0.0],[7.0,8.0],[6.0,7.0],[8.0,9.0],[5.0,6.0],[1.0,2.0],[9.0,10.0],[3.0,4.0],[0.0,1.0],[4.0,5.0],[7.0,8.0],[6.0,7.0],[8.0,9.0],[5.0,6.0],[2.0,3.0],[1.0,2.0],[3.0,4.0],[4.0,5.0],[9.0,10.0]],\"edge_yy\":[[4.833333333333333,4.333333333333333],[0.0,-1.5],[3.333333333333333,3.333333333333333],[1.5,3.0],[2.833333333333333,3.333333333333333],[0.5,0.5],[4.333333333333333,3.833333333333333],[2.833333333333333,2.833333333333333],[3.333333333333333,1.833333333333333],[0.0,0.0],[3.0,4.333333333333333],[0.0,-0.5],[3.333333333333333,4.333333333333333],[1.5,0.0],[5.333333333333333,5.333333333333333],[0.0,0.0],[4.333333333333333,4.333333333333333],[-0.5,-0.5],[3.333333333333333,4.833333333333333],[3.0,2.833333333333333],[0.0,0.5],[1.5,1.0],[0.0,0.0],[0.0,0.0],[4.833333333333333,5.333333333333333],[3.333333333333333,2.333333333333333],[4.333333333333333,4.833333333333333],[-1.5,-1.5],[3.333333333333333,3.833333333333333],[0.0,0.0],[0.0,1.5],[1.8333333333333333,1.8333333333333333],[2.833333333333333,2.333333333333333],[2.833333333333333,2.833333333333333],[0.0,0.0],[2.833333333333333,2.833333333333333],[1.5,2.0],[1.5,1.5],[3.0,1.8333333333333333],[0.0,0.0],[3.333333333333333,2.833333333333333]],\"hypo_i\":[3,3,1,0,0,1,2,1,3,2,0,2,0,3,1,3,0,2,0,1,1,2,0,2,1,3,0,3,1,2,0,3,3,0,2,1,1,0,3,2,2],\"hypo_i_offset\":[-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8],\"hypo_i_text\":[\"#3\",\"#3\",\"#1\",\"#0\",\"#0\",\"#1\",\"#2\",\"#1\",\"#3\",\"#2\",\"#0\",\"#2\",\"#0\",\"#3\",\"#1\",\"#3\",\"#0\",\"#2\",\"#0\",\"#1\",\"#1\",\"#2\",\"#0\",\"#2\",\"#1\",\"#3\",\"#0\",\"#3\",\"#1\",\"#2\",\"#0\",\"#3\",\"#3\",\"#0\",\"#2\",\"#1\",\"#1\",\"#0\",\"#3\",\"#2\",\"#2\"],\"id\":[[7,3],[1,3],[9,1],[3,0],[8,0],[2,1],[6,2],[5,1],[10,3],[7,2],[4,0],[1,2],[9,0],[3,3],[8,1],[6,3],[5,0],[2,2],[10,0],[4,1],[1,1],[3,2],[0,0],[8,2],[7,1],[9,3],[6,0],[2,3],[10,1],[4,2],[1,0],[5,3],[8,3],[7,0],[9,2],[6,1],[3,1],[2,0],[4,3],[5,2],[10,2]],\"is_best\":[false,false,false,true,true,false,false,true,false,false,false,false,true,false,false,false,false,false,false,true,false,false,true,false,false,false,false,false,false,false,true,false,false,true,false,true,false,true,false,false,false],\"line_color\":[\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\"],\"line_width\":[1,1,1,3,3,1,1,3,1,1,1,1,3,1,1,1,1,1,1,3,1,1,3,1,1,1,1,1,1,1,3,1,1,3,1,3,1,3,1,1,1],\"parent_id\":[[6,0],[0,0],[8,0],[2,0],[7,0],[1,1],[5,0],[4,1],[9,1],[6,3],[3,0],[0,0],[8,0],[2,0],[7,1],[5,2],[4,0],[1,2],[9,1],[3,0],[0,0],[2,0],[0,0],[7,2],[6,0],[8,0],[5,0],[1,3],[9,1],[3,3],[0,0],[4,3],[7,0],[6,1],[8,2],[5,1],[2,0],[1,0],[3,0],[4,2],[9,1]],\"token\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"token_font_size\":[\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\"],\"token_id\":[1,780,25,2482,15356,2482,1879,3,17084,25,13862,5,1,7691,1,15356,25,15328,4255,11685,11,3487,-1,21342,15356,4,21342,11,1781,30,3622,3,25,1078,1,5093,26478,11,2451,13862,22624],\"token_text\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"x\":[7.0,1.0,9.0,3.0,8.0,2.0,6.0,5.0,10.0,7.0,4.0,1.0,9.0,3.0,8.0,6.0,5.0,2.0,10.0,4.0,1.0,3.0,0.0,8.0,7.0,9.0,6.0,2.0,10.0,4.0,1.0,5.0,8.0,7.0,9.0,6.0,3.0,2.0,4.0,5.0,10.0],\"y\":[4.333333333333333,-1.5,3.333333333333333,3.0,3.333333333333333,0.5,3.833333333333333,2.833333333333333,1.833333333333333,0.0,4.333333333333333,-0.5,4.333333333333333,0.0,5.333333333333333,0.0,4.333333333333333,-0.5,4.833333333333333,2.833333333333333,0.5,1.0,0.0,0.0,5.333333333333333,2.333333333333333,4.833333333333333,-1.5,3.833333333333333,0.0,1.5,1.8333333333333333,2.333333333333333,2.833333333333333,0.0,2.833333333333333,2.0,1.5,1.8333333333333333,0.0,2.833333333333333]}},\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":{\"id\":\"09035c87-81aa-4e21-b586-281eec2ac195\",\"type\":\"WheelZoomTool\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"ca8bf926-99b6-41f8-aa58-31f717609a35\",\"type\":\"PanTool\"},{\"id\":\"0170b055-6020-406e-b0a6-9a42a9bb2816\",\"type\":\"BoxZoomTool\"},{\"id\":\"09035c87-81aa-4e21-b586-281eec2ac195\",\"type\":\"WheelZoomTool\"},{\"id\":\"e4e04654-061b-4633-a71e-1c9003f6bda9\",\"type\":\"WheelZoomTool\"},{\"id\":\"7d459b4a-fdec-48b5-bc95-828fb94c3c34\",\"type\":\"SaveTool\"},{\"id\":\"cacbfd1a-7e88-471a-a3c1-bc588c6af09c\",\"type\":\"ResetTool\"},{\"id\":\"7a2caa64-375a-466a-8caf-c9f17555b42d\",\"type\":\"HoverTool\"}]},\"id\":\"58f26099-07c1-4f46-ba5f-eec6d745dd12\",\"type\":\"Toolbar\"},{\"attributes\":{\"data_source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"dd1456df-a3c2-4dcc-a8aa-769ac1a8a3b2\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"vertices\",\"nonselection_glyph\":{\"id\":\"43ae4eb3-d229-4335-a758-a5d18149bd65\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"2e9dcf01-3f79-4337-b7dd-26f0525180b9\",\"type\":\"CDSView\"}},\"id\":\"eb321d59-0d82-41f4-966f-a84597d90781\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"id\":\"b0f21dbb-aa92-495c-9a38-9881564a3daa\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"a3e71058-805b-4461-9009-2e76107d0dad\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"eabb5e31-0740-4852-bcac-887155d4c0fc\",\"type\":\"LinearScale\"},{\"attributes\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"id\":\"ddf587aa-019f-43a1-af8c-52144e732785\",\"type\":\"CDSView\"},{\"attributes\":{\"axis_label\":\"decoding step (aka output length)\",\"formatter\":{\"id\":\"f3857a7d-2e4f-4ce0-b897-cec5490c8436\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"327205fd-12df-449f-9614-e6816136cb23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"859d93b7-acac-4853-be74-6f20da679d8b\",\"type\":\"SingleIntervalTicker\"}},\"id\":\"84db9569-9f28-4a47-82c8-bfa01402aeb6\",\"type\":\"LinearAxis\"},{\"attributes\":{\"text\":{\"field\":\"token_text\"},\"text_align\":\"center\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"field\":\"token_font_size\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"},\"y_offset\":{\"value\":-10}},\"id\":\"d4854422-d3bc-43fa-b7c0-a791e40b8a05\",\"type\":\"Text\"},{\"attributes\":{},\"id\":\"ca8bf926-99b6-41f8-aa58-31f717609a35\",\"type\":\"PanTool\"},{\"attributes\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"id\":\"a931ac6d-24c6-489e-873f-3785a4f5cec4\",\"type\":\"CDSView\"},{\"attributes\":{\"bounds\":[-11.5,15.333333333333332],\"callback\":null,\"range_padding\":1.0,\"range_padding_units\":\"absolute\"},\"id\":\"b765029c-d346-49e8-b769-3c8155e12984\",\"type\":\"DataRange1d\"},{\"attributes\":{\"interval\":1},\"id\":\"859d93b7-acac-4853-be74-6f20da679d8b\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"text\":{\"field\":\"hypo_i_text\"},\"text_align\":\"center\",\"text_baseline\":\"middle\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12px\"},\"text_font_style\":\"bold\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"323b0eea-1d7f-4a0d-84fe-74a9d6ac6b56\",\"type\":\"Text\"},{\"attributes\":{\"below\":[{\"id\":\"13a5308f-70ad-452e-af22-43861fd0cb71\",\"type\":\"LinearAxis\"}],\"plot_width\":900,\"renderers\":[{\"id\":\"59657239-6604-4439-8bf1-83a5ab659d4a\",\"type\":\"BoxAnnotation\"},{\"id\":\"13a5308f-70ad-452e-af22-43861fd0cb71\",\"type\":\"LinearAxis\"},{\"id\":\"004b26cb-f235-4bd3-9464-2eed25a63945\",\"type\":\"Grid\"},{\"id\":\"0657e055-fa58-43fe-a3b8-b2b719174d8a\",\"type\":\"GlyphRenderer\"},{\"id\":\"eb321d59-0d82-41f4-966f-a84597d90781\",\"type\":\"GlyphRenderer\"},{\"id\":\"6ac35546-2a42-40c8-aa62-146d57d2f556\",\"type\":\"GlyphRenderer\"},{\"id\":\"5a71f9c4-4314-4eda-a1d8-25124c2b23d2\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"f04dd0c6-c641-4580-87f7-c0f878d1c0e6\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"3175d402-0679-462f-b9d4-d947ff7ad2b3\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"a5cebe80-c44b-4d0a-aae2-c2f3b9ef2475\",\"type\":\"Range1d\"},\"x_scale\":{\"id\":\"d8a43b51-3e35-4c6f-bb08-c7be8e8c3ce2\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"b765029c-d346-49e8-b769-3c8155e12984\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"a0929048-0a2d-4f8f-8d58-e4a6d0106ad1\",\"type\":\"LinearScale\"}},\"id\":\"91387928-8f01-4237-9a5d-24f1d6f93c23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"8033ebc0-a196-49a8-acdc-dbc0ae3fcf54\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"data_source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"d4854422-d3bc-43fa-b7c0-a791e40b8a05\",\"type\":\"Text\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"tokens\",\"nonselection_glyph\":{\"id\":\"17a29716-8304-471c-9f48-6dd74ade100a\",\"type\":\"Text\"},\"selection_glyph\":null,\"view\":{\"id\":\"5d7f1257-5695-4ee0-8fe7-f29e498f83c7\",\"type\":\"CDSView\"}},\"id\":\"6ac35546-2a42-40c8-aa62-146d57d2f556\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"text\":{\"field\":\"hypo_i_text\"},\"text_align\":\"center\",\"text_alpha\":{\"value\":0.1},\"text_baseline\":\"middle\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12px\"},\"text_font_style\":\"bold\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"a10f3b4a-982e-4c44-b026-f8088199fe6f\",\"type\":\"Text\"},{\"attributes\":{\"plot\":{\"id\":\"327205fd-12df-449f-9614-e6816136cb23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"21b07e0f-be09-4617-8f50-6b55adf59566\",\"type\":\"SingleIntervalTicker\"}},\"id\":\"d3dfd7db-0295-4068-89b6-6a86f9141772\",\"type\":\"Grid\"},{\"attributes\":{\"plot\":null,\"text\":\"Beam search\"},\"id\":\"f04dd0c6-c641-4580-87f7-c0f878d1c0e6\",\"type\":\"Title\"},{\"attributes\":{\"data_source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"323b0eea-1d7f-4a0d-84fe-74a9d6ac6b56\",\"type\":\"Text\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"hypo_i\",\"nonselection_glyph\":{\"id\":\"a10f3b4a-982e-4c44-b026-f8088199fe6f\",\"type\":\"Text\"},\"selection_glyph\":null,\"view\":{\"id\":\"b0f21dbb-aa92-495c-9a38-9881564a3daa\",\"type\":\"CDSView\"}},\"id\":\"d6b0d875-955c-493f-8f95-7f155de46db0\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"id\":\"5d7f1257-5695-4ee0-8fe7-f29e498f83c7\",\"type\":\"CDSView\"},{\"attributes\":{\"overlay\":{\"id\":\"8033ebc0-a196-49a8-acdc-dbc0ae3fcf54\",\"type\":\"BoxAnnotation\"}},\"id\":\"0170b055-6020-406e-b0a6-9a42a9bb2816\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"bounds\":[-10.0,20.0],\"callback\":null,\"end\":12,\"js_property_callbacks\":{\"change:end\":[{\"id\":\"b56f32a3-d365-46b7-9f4f-323382959b2d\",\"type\":\"CustomJS\"}]},\"start\":-1},\"id\":\"a5cebe80-c44b-4d0a-aae2-c2f3b9ef2475\",\"type\":\"Range1d\"},{\"attributes\":{\"dimensions\":\"width\"},\"id\":\"09035c87-81aa-4e21-b586-281eec2ac195\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":{\"id\":\"fbfb1ccf-b06f-43d0-93b8-b7a2feb2e51e\",\"type\":\"WheelZoomTool\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"cab35414-b6a5-47d8-a8cd-55b26f7a7b54\",\"type\":\"PanTool\"},{\"id\":\"64b6f55d-2417-4701-8845-e39dd839273e\",\"type\":\"BoxZoomTool\"},{\"id\":\"fbfb1ccf-b06f-43d0-93b8-b7a2feb2e51e\",\"type\":\"WheelZoomTool\"},{\"id\":\"7a97466a-a0c3-4c9a-93bc-f7d1909c09da\",\"type\":\"WheelZoomTool\"},{\"id\":\"a8cad25e-8062-4326-b4df-8b1d1db279c7\",\"type\":\"SaveTool\"},{\"id\":\"237d0e16-242f-45d0-81da-35c5b34ecdb4\",\"type\":\"ResetTool\"},{\"id\":\"ce962e54-6eaa-4a63-b602-ab2387d43716\",\"type\":\"HoverTool\"}]},\"id\":\"3175d402-0679-462f-b9d4-d947ff7ad2b3\",\"type\":\"Toolbar\"},{\"attributes\":{\"dimensions\":\"height\"},\"id\":\"e4e04654-061b-4633-a71e-1c9003f6bda9\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"text\":{\"field\":\"hypo_i_text\"},\"text_align\":\"center\",\"text_baseline\":\"middle\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12px\"},\"text_font_style\":\"bold\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"8c80ac74-34fc-4790-a100-c62b151ded39\",\"type\":\"Text\"},{\"attributes\":{},\"id\":\"a0929048-0a2d-4f8f-8d58-e4a6d0106ad1\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"7d459b4a-fdec-48b5-bc95-828fb94c3c34\",\"type\":\"SaveTool\"},{\"attributes\":{\"text\":{\"field\":\"hypo_i_text\"},\"text_align\":\"center\",\"text_alpha\":{\"value\":0.1},\"text_baseline\":\"middle\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12px\"},\"text_font_style\":\"bold\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1aaad8cb-1c94-4061-b4ce-f70e91e33b07\",\"type\":\"Text\"},{\"attributes\":{},\"id\":\"d8a43b51-3e35-4c6f-bb08-c7be8e8c3ce2\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"cacbfd1a-7e88-471a-a3c1-bc588c6af09c\",\"type\":\"ResetTool\"},{\"attributes\":{\"data_source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"8c80ac74-34fc-4790-a100-c62b151ded39\",\"type\":\"Text\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"hypo_i\",\"nonselection_glyph\":{\"id\":\"1aaad8cb-1c94-4061-b4ce-f70e91e33b07\",\"type\":\"Text\"},\"selection_glyph\":null,\"view\":{\"id\":\"a931ac6d-24c6-489e-873f-3785a4f5cec4\",\"type\":\"CDSView\"}},\"id\":\"5a71f9c4-4314-4eda-a1d8-25124c2b23d2\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"names\":[\"vertices\"],\"tooltips\":[[\"token\",\"@_on_hover_token\"],[\"token_id\",\"@_on_hover_token_id\"],[\"score\",\"@_on_hover_score\"]]},\"id\":\"7a2caa64-375a-466a-8caf-c9f17555b42d\",\"type\":\"HoverTool\"},{\"attributes\":{\"interval\":1},\"id\":\"21b07e0f-be09-4617-8f50-6b55adf59566\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"59657239-6604-4439-8bf1-83a5ab659d4a\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"axis_label\":\"decoding step (aka output length)\",\"formatter\":{\"id\":\"e26ba807-3a3a-43be-8db2-601d280799bd\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"91387928-8f01-4237-9a5d-24f1d6f93c23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"c0496269-9e06-46af-ab94-5438bfbff629\",\"type\":\"SingleIntervalTicker\"}},\"id\":\"13a5308f-70ad-452e-af22-43861fd0cb71\",\"type\":\"LinearAxis\"},{\"attributes\":{\"interval\":1},\"id\":\"c0496269-9e06-46af-ab94-5438bfbff629\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"args\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"code\":\"\\n            var x_range = cb_obj;\\n            var font_size = Math.round(14 * 13.0 / (x_range.end - x_range.start));\\n\\n            font_size = Math.min(24, Math.max(font_size, 0));\\n            \\n            var data = source.data;\\n            var fs = data['token_font_size']\\n            \\n            for (var i = 0; i < fs.length; i++)\\n                fs[i] = font_size.toString() + \\\"px\\\";\\n            \\n            source.change.emit();\\n        \"},\"id\":\"eab13fb4-9405-4d14-8bf6-46c6f698b4bb\",\"type\":\"CustomJS\"},{\"attributes\":{},\"id\":\"cab35414-b6a5-47d8-a8cd-55b26f7a7b54\",\"type\":\"PanTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"size\":{\"units\":\"screen\",\"value\":24},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"d47ea423-d422-49a0-b24d-798a01f47236\",\"type\":\"Circle\"},{\"attributes\":{\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"xs\":{\"field\":\"edge_xx\"},\"ys\":{\"field\":\"edge_yy\"}},\"id\":\"d4677ed5-e7ac-4a9f-960d-0463a72b4947\",\"type\":\"MultiLine\"},{\"attributes\":{\"callback\":null,\"names\":[\"vertices\"],\"tooltips\":[[\"token\",\"@_on_hover_token\"],[\"token_id\",\"@_on_hover_token_id\"],[\"score\",\"@_on_hover_score\"]]},\"id\":\"ce962e54-6eaa-4a63-b602-ab2387d43716\",\"type\":\"HoverTool\"},{\"attributes\":{\"overlay\":{\"id\":\"59657239-6604-4439-8bf1-83a5ab659d4a\",\"type\":\"BoxAnnotation\"}},\"id\":\"64b6f55d-2417-4701-8845-e39dd839273e\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"xs\":{\"field\":\"edge_xx\"},\"ys\":{\"field\":\"edge_yy\"}},\"id\":\"72b704fc-ca92-4d86-89ae-f33aeb3c23a8\",\"type\":\"MultiLine\"},{\"attributes\":{\"dimensions\":\"width\"},\"id\":\"fbfb1ccf-b06f-43d0-93b8-b7a2feb2e51e\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"xs\":{\"field\":\"edge_xx\"},\"ys\":{\"field\":\"edge_yy\"}},\"id\":\"5574258e-e46b-409c-af99-02931bf42c33\",\"type\":\"MultiLine\"},{\"attributes\":{\"fill_color\":{\"field\":\"circle_fill_color\"},\"line_color\":{\"field\":\"line_color\"},\"line_width\":{\"field\":\"line_width\"},\"size\":{\"units\":\"screen\",\"value\":24},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"dd1456df-a3c2-4dcc-a8aa-769ac1a8a3b2\",\"type\":\"Circle\"},{\"attributes\":{\"dimensions\":\"height\"},\"id\":\"7a97466a-a0c3-4c9a-93bc-f7d1909c09da\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"fill_color\":{\"field\":\"circle_fill_color\"},\"line_color\":{\"field\":\"line_color\"},\"line_width\":{\"field\":\"line_width\"},\"size\":{\"units\":\"screen\",\"value\":24},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"14c2f957-8e96-40e9-8d5a-bbe84ee0131a\",\"type\":\"Circle\"},{\"attributes\":{\"args\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"code\":\"\\n            var x_range = cb_obj;\\n            var font_size = Math.round(14 * 13.0 / (x_range.end - x_range.start));\\n\\n            font_size = Math.min(24, Math.max(font_size, 0));\\n            \\n            var data = source.data;\\n            var fs = data['token_font_size']\\n            \\n            for (var i = 0; i < fs.length; i++)\\n                fs[i] = font_size.toString() + \\\"px\\\";\\n            \\n            source.change.emit();\\n        \"},\"id\":\"b56f32a3-d365-46b7-9f4f-323382959b2d\",\"type\":\"CustomJS\"},{\"attributes\":{},\"id\":\"a8cad25e-8062-4326-b4df-8b1d1db279c7\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"237d0e16-242f-45d0-81da-35c5b34ecdb4\",\"type\":\"ResetTool\"},{\"attributes\":{\"data_source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5574258e-e46b-409c-af99-02931bf42c33\",\"type\":\"MultiLine\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"edges\",\"nonselection_glyph\":{\"id\":\"72b704fc-ca92-4d86-89ae-f33aeb3c23a8\",\"type\":\"MultiLine\"},\"selection_glyph\":null,\"view\":{\"id\":\"95e7b794-7dda-4997-b7c6-a664964f6b5a\",\"type\":\"CDSView\"}},\"id\":\"978e683c-44ae-4bb3-b7d1-4660f2415803\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"interval\":1},\"id\":\"aaf5ec9a-6f51-45f4-a1b5-d2a8ac2cb5e0\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"14c2f957-8e96-40e9-8d5a-bbe84ee0131a\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"vertices\",\"nonselection_glyph\":{\"id\":\"d47ea423-d422-49a0-b24d-798a01f47236\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"dd940c41-87e6-4094-8be6-3dc7f22f0921\",\"type\":\"CDSView\"}},\"id\":\"cdf3b0e4-fe4f-450f-86d4-c51260c5d8c0\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"plot\":{\"id\":\"91387928-8f01-4237-9a5d-24f1d6f93c23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"aaf5ec9a-6f51-45f4-a1b5-d2a8ac2cb5e0\",\"type\":\"SingleIntervalTicker\"}},\"id\":\"004b26cb-f235-4bd3-9464-2eed25a63945\",\"type\":\"Grid\"},{\"attributes\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"id\":\"b553c853-cc22-4ae3-aea6-95ad360ad0fc\",\"type\":\"CDSView\"},{\"attributes\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"id\":\"95e7b794-7dda-4997-b7c6-a664964f6b5a\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"id\",\"parent_id\",\"children_ids\",\"is_best\",\"depth\",\"hypo_i\",\"token\",\"token_id\",\"x\",\"y\",\"circle_fill_color\",\"line_color\",\"line_width\",\"edge_xx\",\"edge_yy\",\"token_text\",\"token_font_size\",\"hypo_i_text\",\"hypo_i_offset\",\"_on_hover_token\",\"_on_hover_token_id\",\"_on_hover_score\"],\"data\":{\"_on_hover_score\":[\"-4.7282\",\"-4.2071\",\"-4.8782\",\"-1.3577\",\"-4.0420\",\"-4.2097\",\"-4.5624\",\"-3.2410\",\"-6.9225\",\"-4.6717\",\"-2.8497\",\"-3.8023\",\"-4.1460\",\"-3.6443\",\"-4.3135\",\"-4.7070\",\"-3.1373\",\"-4.5105\",\"-6.1526\",\"-3.0974\",\"-3.7421\",\"-3.4956\",\"0.0000\",\"-5.2694\",\"-4.4104\",\"-6.1752\",\"-3.7617\",\"-4.6281\",\"-6.5626\",\"-3.7175\",\"-0.7384\",\"-4.5439\",\"-5.2967\",\"-3.6831\",\"-5.2153\",\"-3.7830\",\"-3.1335\",\"-0.8716\",\"-3.7312\",\"-3.8895\",\"-6.5984\"],\"_on_hover_token\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"_on_hover_token_id\":[1,780,25,2482,15356,2482,1879,3,17084,25,13862,5,1,7691,1,15356,25,15328,4255,11685,11,3487,-1,21342,15356,4,21342,11,1781,30,3622,3,25,1078,1,5093,26478,11,2451,13862,22624],\"children_ids\":[[],[[2,3]],[[10,3],[10,2],[10,1],[10,0]],[[4,3],[4,1],[4,0]],[[9,3],[9,1],[9,0]],[],[],[[6,1]],[],[[8,2]],[[5,0]],[[2,2]],[],[[4,2]],[],[[7,2]],[[6,2],[6,0]],[],[],[[5,1]],[[2,1]],[],[[1,3],[1,2],[1,1],[1,0]],[[9,2]],[[8,1]],[],[[7,3],[7,1]],[],[],[[5,2]],[[2,0]],[],[],[[8,3],[8,0]],[],[[7,0]],[],[[3,3],[3,2],[3,1],[3,0]],[[5,3]],[[6,3]],[]],\"circle_fill_color\":[\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\"],\"depth\":[7,1,9,3,8,2,6,5,10,7,4,1,9,3,8,6,5,2,10,4,1,3,0,8,7,9,6,2,10,4,1,5,8,7,9,6,3,2,4,5,10],\"edge_xx\":[[6.0,7.0],[0.0,1.0],[8.0,9.0],[2.0,3.0],[7.0,8.0],[1.0,2.0],[5.0,6.0],[4.0,5.0],[9.0,10.0],[6.0,7.0],[3.0,4.0],[0.0,1.0],[8.0,9.0],[2.0,3.0],[7.0,8.0],[5.0,6.0],[4.0,5.0],[1.0,2.0],[9.0,10.0],[3.0,4.0],[0.0,1.0],[2.0,3.0],[0.0,0.0],[7.0,8.0],[6.0,7.0],[8.0,9.0],[5.0,6.0],[1.0,2.0],[9.0,10.0],[3.0,4.0],[0.0,1.0],[4.0,5.0],[7.0,8.0],[6.0,7.0],[8.0,9.0],[5.0,6.0],[2.0,3.0],[1.0,2.0],[3.0,4.0],[4.0,5.0],[9.0,10.0]],\"edge_yy\":[[4.833333333333333,4.333333333333333],[0.0,-1.5],[3.333333333333333,3.333333333333333],[1.5,3.0],[2.833333333333333,3.333333333333333],[0.5,0.5],[4.333333333333333,3.833333333333333],[2.833333333333333,2.833333333333333],[3.333333333333333,1.833333333333333],[0.0,0.0],[3.0,4.333333333333333],[0.0,-0.5],[3.333333333333333,4.333333333333333],[1.5,0.0],[5.333333333333333,5.333333333333333],[0.0,0.0],[4.333333333333333,4.333333333333333],[-0.5,-0.5],[3.333333333333333,4.833333333333333],[3.0,2.833333333333333],[0.0,0.5],[1.5,1.0],[0.0,0.0],[0.0,0.0],[4.833333333333333,5.333333333333333],[3.333333333333333,2.333333333333333],[4.333333333333333,4.833333333333333],[-1.5,-1.5],[3.333333333333333,3.833333333333333],[0.0,0.0],[0.0,1.5],[1.8333333333333333,1.8333333333333333],[2.833333333333333,2.333333333333333],[2.833333333333333,2.833333333333333],[0.0,0.0],[2.833333333333333,2.833333333333333],[1.5,2.0],[1.5,1.5],[3.0,1.8333333333333333],[0.0,0.0],[3.333333333333333,2.833333333333333]],\"hypo_i\":[3,3,1,0,0,1,2,1,3,2,0,2,0,3,1,3,0,2,0,1,1,2,0,2,1,3,0,3,1,2,0,3,3,0,2,1,1,0,3,2,2],\"hypo_i_offset\":[-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8],\"hypo_i_text\":[\"#3\",\"#3\",\"#1\",\"#0\",\"#0\",\"#1\",\"#2\",\"#1\",\"#3\",\"#2\",\"#0\",\"#2\",\"#0\",\"#3\",\"#1\",\"#3\",\"#0\",\"#2\",\"#0\",\"#1\",\"#1\",\"#2\",\"#0\",\"#2\",\"#1\",\"#3\",\"#0\",\"#3\",\"#1\",\"#2\",\"#0\",\"#3\",\"#3\",\"#0\",\"#2\",\"#1\",\"#1\",\"#0\",\"#3\",\"#2\",\"#2\"],\"id\":[[7,3],[1,3],[9,1],[3,0],[8,0],[2,1],[6,2],[5,1],[10,3],[7,2],[4,0],[1,2],[9,0],[3,3],[8,1],[6,3],[5,0],[2,2],[10,0],[4,1],[1,1],[3,2],[0,0],[8,2],[7,1],[9,3],[6,0],[2,3],[10,1],[4,2],[1,0],[5,3],[8,3],[7,0],[9,2],[6,1],[3,1],[2,0],[4,3],[5,2],[10,2]],\"is_best\":[false,false,false,true,true,false,false,true,false,false,false,false,true,false,false,false,false,false,false,true,false,false,true,false,false,false,false,false,false,false,true,false,false,true,false,true,false,true,false,false,false],\"line_color\":[\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\"],\"line_width\":[1,1,1,3,3,1,1,3,1,1,1,1,3,1,1,1,1,1,1,3,1,1,3,1,1,1,1,1,1,1,3,1,1,3,1,3,1,3,1,1,1],\"parent_id\":[[6,0],[0,0],[8,0],[2,0],[7,0],[1,1],[5,0],[4,1],[9,1],[6,3],[3,0],[0,0],[8,0],[2,0],[7,1],[5,2],[4,0],[1,2],[9,1],[3,0],[0,0],[2,0],[0,0],[7,2],[6,0],[8,0],[5,0],[1,3],[9,1],[3,3],[0,0],[4,3],[7,0],[6,1],[8,2],[5,1],[2,0],[1,0],[3,0],[4,2],[9,1]],\"token\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"token_font_size\":[\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\"],\"token_id\":[1,780,25,2482,15356,2482,1879,3,17084,25,13862,5,1,7691,1,15356,25,15328,4255,11685,11,3487,-1,21342,15356,4,21342,11,1781,30,3622,3,25,1078,1,5093,26478,11,2451,13862,22624],\"token_text\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"x\":[7.0,1.0,9.0,3.0,8.0,2.0,6.0,5.0,10.0,7.0,4.0,1.0,9.0,3.0,8.0,6.0,5.0,2.0,10.0,4.0,1.0,3.0,0.0,8.0,7.0,9.0,6.0,2.0,10.0,4.0,1.0,5.0,8.0,7.0,9.0,6.0,3.0,2.0,4.0,5.0,10.0],\"y\":[4.333333333333333,-1.5,3.333333333333333,3.0,3.333333333333333,0.5,3.833333333333333,2.833333333333333,1.833333333333333,0.0,4.333333333333333,-0.5,4.333333333333333,0.0,5.333333333333333,0.0,4.333333333333333,-0.5,4.833333333333333,2.833333333333333,0.5,1.0,0.0,0.0,5.333333333333333,2.333333333333333,4.833333333333333,-1.5,3.833333333333333,0.0,1.5,1.8333333333333333,2.333333333333333,2.833333333333333,0.0,2.833333333333333,2.0,1.5,1.8333333333333333,0.0,2.833333333333333]}},\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"e26ba807-3a3a-43be-8db2-601d280799bd\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"xs\":{\"field\":\"edge_xx\"},\"ys\":{\"field\":\"edge_yy\"}},\"id\":\"d41b7979-8903-4ee5-a861-1bf88bfb14ea\",\"type\":\"MultiLine\"},{\"attributes\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"id\":\"dd940c41-87e6-4094-8be6-3dc7f22f0921\",\"type\":\"CDSView\"},{\"attributes\":{\"text\":{\"field\":\"token_text\"},\"text_align\":\"center\",\"text_alpha\":{\"value\":0.1},\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"field\":\"token_font_size\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"},\"y_offset\":{\"value\":-10}},\"id\":\"17a29716-8304-471c-9f48-6dd74ade100a\",\"type\":\"Text\"},{\"attributes\":{},\"id\":\"f3857a7d-2e4f-4ce0-b897-cec5490c8436\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"data_source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"d4677ed5-e7ac-4a9f-960d-0463a72b4947\",\"type\":\"MultiLine\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"edges\",\"nonselection_glyph\":{\"id\":\"d41b7979-8903-4ee5-a861-1bf88bfb14ea\",\"type\":\"MultiLine\"},\"selection_glyph\":null,\"view\":{\"id\":\"b553c853-cc22-4ae3-aea6-95ad360ad0fc\",\"type\":\"CDSView\"}},\"id\":\"0657e055-fa58-43fe-a3b8-b2b719174d8a\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"text\":{\"field\":\"token_text\"},\"text_align\":\"center\",\"text_alpha\":{\"value\":0.1},\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"field\":\"token_font_size\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"},\"y_offset\":{\"value\":-10}},\"id\":\"3621fd59-3cb3-4305-8802-6f7f75a23fd6\",\"type\":\"Text\"},{\"attributes\":{\"bounds\":[-11.5,15.333333333333332],\"callback\":null,\"range_padding\":1.0,\"range_padding_units\":\"absolute\"},\"id\":\"3868f6e3-8193-418f-af89-ee5749e490a1\",\"type\":\"DataRange1d\"},{\"attributes\":{\"text\":{\"field\":\"token_text\"},\"text_align\":\"center\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"field\":\"token_font_size\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"},\"y_offset\":{\"value\":-10}},\"id\":\"08bbcb52-00b5-4803-9e7a-9252db09708d\",\"type\":\"Text\"},{\"attributes\":{\"below\":[{\"id\":\"84db9569-9f28-4a47-82c8-bfa01402aeb6\",\"type\":\"LinearAxis\"}],\"plot_width\":900,\"renderers\":[{\"id\":\"8033ebc0-a196-49a8-acdc-dbc0ae3fcf54\",\"type\":\"BoxAnnotation\"},{\"id\":\"84db9569-9f28-4a47-82c8-bfa01402aeb6\",\"type\":\"LinearAxis\"},{\"id\":\"d3dfd7db-0295-4068-89b6-6a86f9141772\",\"type\":\"Grid\"},{\"id\":\"978e683c-44ae-4bb3-b7d1-4660f2415803\",\"type\":\"GlyphRenderer\"},{\"id\":\"cdf3b0e4-fe4f-450f-86d4-c51260c5d8c0\",\"type\":\"GlyphRenderer\"},{\"id\":\"a2ab1853-0966-4519-874f-956f801f1c72\",\"type\":\"GlyphRenderer\"},{\"id\":\"d6b0d875-955c-493f-8f95-7f155de46db0\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"5af81591-5793-4721-a459-e0a4ca700855\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"58f26099-07c1-4f46-ba5f-eec6d745dd12\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"14e9976c-9458-4bce-be96-da2f3c304cec\",\"type\":\"Range1d\"},\"x_scale\":{\"id\":\"eabb5e31-0740-4852-bcac-887155d4c0fc\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"3868f6e3-8193-418f-af89-ee5749e490a1\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"a3e71058-805b-4461-9009-2e76107d0dad\",\"type\":\"LinearScale\"}},\"id\":\"327205fd-12df-449f-9614-e6816136cb23\",\"subtype\":\"Figure\",\"type\":\"Plot\"}],\"root_ids\":[\"327205fd-12df-449f-9614-e6816136cb23\",\"91387928-8f01-4237-9a5d-24f1d6f93c23\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.7\"}};\n",
       "              var render_items = [{\"docid\":\"ba84f797-d201-498d-a731-5adafa5447b7\",\"elementid\":\"ff8c3f31-952d-4c2f-8b58-13e7cec51b58\",\"modelid\":\"91387928-8f01-4237-9a5d-24f1d6f93c23\"}];\n",
       "              \n",
       "              Bokeh.embed.embed_items(docs_json, render_items);\n",
       "            });\n",
       "          };\n",
       "          if (document.readyState != \"loading\") fn();\n",
       "          else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "        })();\n",
       "        \n",
       "        </script>\n",
       "    </body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "# Here's what it looks like:\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/nlp_course/2020/resources/beam_search.html\n",
    "HTML(\"beam_search.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_beamsearch(model, prefix=BOS, beam_size=4, length=5):\n",
    "    \"\"\"\n",
    "    Generate a sequence with nucleous sampling\n",
    "    :param prefix: a string containing space-separated previous tokens\n",
    "    :param nucleus: N from the formulae above, N \\in [0, 1]\n",
    "    :param length: generate sequences with at most this many tokens, NOT INCLUDING PREFIX\n",
    "    :returns: beam_size most likely candidates\n",
    "    :note: make sure that nucleous always contains at least one word, even if p(w*) > nucleus\n",
    "    \"\"\"\n",
    "    \n",
    "    <YOUR CODE HERE>\n",
    "    \n",
    "    return <most likely sequence>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_beamsearch(model, prefix=' deep ', beam_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check it out: which beam size works best?\n",
    "# find at least 5 prefixes where beam_size=1 and 8 generates different sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus quest II: Ultimate Language Model (2+ pts)\n",
    "\n",
    "So you've learned the building blocks of neural language models, you can now build the ultimate monster:  \n",
    "* Make it char-level, word level or maybe use sub-word units like [bpe](https://github.com/rsennrich/subword-nmt);\n",
    "* Combine convolutions, recurrent cells, pre-trained embeddings and all the black magic deep learning has to offer;\n",
    "  * Use strides to get larger window size quickly. Here's a [scheme](https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig2-Anim-160908-r01.gif) from google wavenet.\n",
    "* Train on large data. Like... really large. Try [1 Billion Words](http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz) benchmark;\n",
    "* Use training schedules to speed up training. Start with small length and increase over time; Take a look at [one cycle](https://medium.com/@nachiket.tanksale/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6) for learning rate;\n",
    "\n",
    "_You are NOT required to submit this assignment. Please make sure you don't miss your deadline because of it :)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sudoku382",
   "language": "python",
   "name": "sudoku382"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
