{"cells":[{"cell_type":"markdown","metadata":{"id":"aSWEcS2XKgzi"},"source":["### Practice: Parameter Efficient Fine-Tuning\n","In this notebook, you're gonna fine-tune large language models within limited GPU memory."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7xeRF_hSKgzs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701271539312,"user_tz":-120,"elapsed":63430,"user":{"displayName":"t tiiz","userId":"12254911776999114256"}},"outputId":"b580da9e-0119-4790-b30e-c58bc02043de"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.0/301.0 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for optimum (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["%pip install --quiet transformers==4.34.1 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 peft==0.5.0 bitsandbytes==0.41.2.post2\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import transformers\n","from tqdm.auto import tqdm, trange\n","assert torch.cuda.is_available(), \"you need cuda for this part\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","source":["model_name = 'Enoch/llama-7b-hf'\n","\n","# loading Llama tokenizer ...\n","tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n","tokenizer.pad_token_id = tokenizer.eos_token_id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":150,"referenced_widgets":["d6683a7d7ade4886ac392df4cabb49af","db2281b8e3994a84a024908930b71904","35a47513cb8f4e84a180113997c4d5bb","0df632eeddf04fd3be9985836e00288f","012215bf1eb04c3cbd905c827d56e8fe","99386280038849e6ba9163b55b93e7d6","89a7b97e80724afabd04d32fbaae5458","5bfb741d1c5c4468b6ceebe488196203","a27b0f601c3b4c8dbe238484403acf0e","2e80bdd9683b4da6809a8d5c6f7a8b88","993147a59c454d63837e2c98fe091f1b","3333243b34a24a9c9fc755c758cf2c27","f28d52385d0045ae9d05d15a408e1ad5","971bb552d5e549749c570d82b333a39e","c22d56576b6d4db9b70413207bb60bd2","ecaa4d31f9544effa37b49c92e657817","73fc4b8ba4464e4d9be99d55ce47f471","7ad2cb1b22924f7eb82cce85fbc29281","28d4439a57ca4fc1a6bceab33de4915d","d8a34a4c894a4d11926ee4263e5a554e","7105c6372c0f46518e6f961f4e28fba3","37bf3dde48f543b89b27b9e68abf2b2c","8df7adbe0a1b417ea91a112c4f414e79","2ea3aeac07274447a563bc75a333ba4b","6ce60f44c23d4de2b05755d3dffd00ac","5e37739d04204a10a4184fbb7d09c954","1e0b9f47329d4435bb08d3ded87b3623","d9aa7e33f4ac41f1867c6a98acc0d167","264decefd4c7496096a2aa9d5b709bdc","d3a3694f75764ad4b416029aef7181af","26310e3e3e614f02ae72765a31fa91e8","78feb74abb9c4b9aa24a49c359ade8d8","0e18adb37bec4ce1b65aa3d8fc6bc2ba"]},"id":"nNrzjdVsrywB","executionInfo":{"status":"ok","timestamp":1701271540583,"user_tz":-120,"elapsed":1277,"user":{"displayName":"t tiiz","userId":"12254911776999114256"}},"outputId":"8f38b800-ac9a-4bb2-d9ba-e9931a6cab99"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading tokenizer_config.json:   0%|          | 0.00/218 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6683a7d7ade4886ac392df4cabb49af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3333243b34a24a9c9fc755c758cf2c27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8df7adbe0a1b417ea91a112c4f414e79"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"VMzFwx29Kgzu","colab":{"base_uri":"https://localhost:8080/","height":150,"referenced_widgets":["3eeebb2a0efd4585befde82b3a43d5c4","91f1c559ec784f69911d45ec5861d480","6ed35413ff024c56a8bff5e6a7404298","325484f249f04859a03f4221cfcdf26e","7aec58d9ad9f486b9dab4899ed0bb1cf","f93cc34658ec4b8598f98d977b1f8228","a93abb68e50c40fe97d7320cc4f5c5e8","a469269e7aa34bd088a134e485fc4770","3f76f6d4ffc54bd6a65c4c1c7d166f70","a376a206fd1d43319dcf7ff44af42431","cc58ce3e01f546e1b780f1279cc7f8ff","cc8e8bf7d0a5449b932aa0704b872c7a","31906eb3283148d6a057fbbd5c2846dd","f1c515cf9cac4cd193e213810c29faa7","d3cc2679c9df4bb681ff2a82fe89447a","6baded65a716432cbb37b4990246b004","37d839b7d43e44009b553eb99d85514c","ef00982db8204460a54ed1443d94f510","d411ee6b22244c4a8d863a56bc4743b9","73c31f533bf4474ebfaa8697fd705be6","235413817763474eb39a70201d68ceba","1533954a09af46a69747ce121d2e0454","77fe12e992f74ee5bda9e7cad1ce854c","85014da00d624834943a10f8e08dded4","2c6ef21e4f434fb28a435fdeb72122a2","0b3ed5f7c935435f83fc708452e08ed3","ea35226aa5b34ad6980de405b7191f7f","de66ca879d0c40ad84d724086156daa5","c088b6293b4a4418a7b3c51d942b8460","a12aa27d94b14015884720a1f148fd2f","e83d849093734f27946a593b555d8781","91741d313df94b37826a5d44d2179d5b","545c3d9d0a8447cc9f82aa68e7f8bfe5"]},"outputId":"8a186045-bd3e-4ab0-ef32-ffd3a173cde3","executionInfo":{"status":"ok","timestamp":1701258027851,"user_tz":-120,"elapsed":2309,"user":{"displayName":"t tiiz","userId":"12254911776999114256"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading tokenizer_config.json:   0%|          | 0.00/218 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eeebb2a0efd4585befde82b3a43d5c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc8e8bf7d0a5449b932aa0704b872c7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77fe12e992f74ee5bda9e7cad1ce854c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]}],"source":["model_name = 'Enoch/llama-7b-hf'\n","\n","# loading Llama tokenizer ...\n","tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","# ... and the model itself\n","model = transformers.AutoModelForCausalLM.from_pretrained(\n","    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n","    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",")\n","for param in model.parameters():\n","    param.requires_grad=False\n","\n","model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n","model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n","# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"]},{"cell_type":"markdown","source":["### Prompt tuning: the story of a fox (2 pts)\n","\n","![img](https://i.imgur.com/Ux3qQAu.png) (source: theodd1souts.fandom.com)"],"metadata":{"id":"rgspB2JwSIS2"}},{"cell_type":"code","source":["prompt = 'A quick brown fox'\n","batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n","\n","for i in range(10):\n","    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n","    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n","    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n","\n","print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H13pYFRxQi4U","outputId":"8a0f032b-cb02-4b29-e4d4-d0f1ac2cfe0b","executionInfo":{"status":"ok","timestamp":1701179128261,"user_tz":-120,"elapsed":4803,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Output: <s>A quick brown fox jumps over the lazy dog.\n","A quick\n"]}]},{"cell_type":"markdown","source":["What a blatant lie! This particular fox assures you that it didn't in fact jump over the lazy dog. No, sir! The fox was just minding its own business. __Your task is to train the model to say truth: no dog was jumped over today.__"],"metadata":{"id":"VVhZACT6SgLq"}},{"cell_type":"code","source":["the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n","batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n","outputs = model(**batch)\n","\n","next_word_logits = outputs.logits[:, :-1]\n","true_next_tokens = batch['input_ids'][:, 1:]\n","loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n","\n","print(\"Loss:\", loss)"],"metadata":{"id":"_r6UVDl4NEua","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eb15620f-f143-4690-b979-773dd0e7d055","executionInfo":{"status":"ok","timestamp":1701179137378,"user_tz":-120,"elapsed":1221,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss: tensor(3.0725, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"code","source":["print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))\n","\n","next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n","print(\"\\nOutput:\", tokenizer.decode(next_token[0].cpu().numpy().tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7lW56xSyEb-D","executionInfo":{"status":"ok","timestamp":1701177511488,"user_tz":-120,"elapsed":1288,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}},"outputId":"d3bb82f9-ee72-47d3-bf05-47115293eb5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Output: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\n","\n","Output: \n","\n"]}]},{"cell_type":"markdown","source":["Except, we can't train the entire model - that would be 28GB gradients in float32. Instead, let's run [prompt tuning](https://arxiv.org/abs/2104.08691).\n","\n","![img](https://i.imgur.com/VwNNKnb.png)\n"],"metadata":{"id":"amvNufS8WXa0"}},{"cell_type":"code","source":["class WordEmbeddingsWithLearnedPrompts(nn.Module):\n","    \"\"\"\n","    To perform prompt tuning, you will need to replace model's original word embeddings with a layer - THIS layer\n","     - that inserts trainable prompts instead of the first N token embeddings. \"\"\"\n","\n","    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n","        super().__init__()\n","        self.original_word_embeddings = word_embeddings\n","        self.num_prompts = num_prompts\n","        self.learnable_prompts = nn.Parameter(\n","            torch.randn(1, num_prompts, word_embeddings.embedding_dim), requires_grad=True)\n","\n","    def forward(self, input_ids: torch.LongTensor):\n","        # input_ids shape: [batch_size, seq length]\n","        assert input_ids.dtype == torch.int64\n","        assert input_ids.shape[1] > self.num_prompts\n","        assert torch.all(input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).item(), \"don't forget to prepend several BOS tokens to input_ids\"\n","\n","        # Your task: embed input_ids, but replace the first :num_prompts: tokens with self.learnable_prompts\n","        # This is because we will prepend :num_prompts: padding tokens at the beginning\n","\n","        # After you are done, you must produce a word embedding vector for each token in input_ids,\n","        # except that the first :num_prompts: vectors should equal learnable_prompts;\n","        # any additional vectors after first :num_prompts: ones should be embedded as usual\n","        # Note: since you're dealing with trainable params, please torch.cat instead of item assignment\n","\n","        # <YOUR CODE HERE>\n","        # Embed original input_ids\n","        input_embed = self.original_word_embeddings(input_ids)\n","\n","        # Replace first :num_prompts: with learnable_prompts\n","        replaced_input_embed = torch.cat([self.learnable_prompts, input_embed[:,self.num_prompts:,:]], dim=1)\n","\n","        return replaced_input_embed\n","\n","        # return your_outputs_with_prompts_as_per_instructions_above"],"metadata":{"id":"73ZOCFRZWR98"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_prompts = 16\n","test_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n","test_input_ids = tokenizer(\"a cat say on a may\", return_tensors='pt')['input_ids'].to(device)\n","\n","# Add :num_prompts: tokens at the beginning\n","space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n","                               dtype=torch.int64, device=device)\n","# Concatenate\n","test_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n","\n","with torch.cuda.amp.autocast():\n","  test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n","\n","assert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\n","assert test_prompt_embeddings.shape[-1] == model.config.hidden_size\n","assert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\n","assert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\n","print(\"Looks legit!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxUyUU2uT2f1","outputId":"17a6f234-e48e-4b99-8fd6-52cc643fb180","executionInfo":{"status":"ok","timestamp":1701179161631,"user_tz":-120,"elapsed":5,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looks legit!\n"]}]},{"cell_type":"markdown","source":["__Now that it works,__ let's inject learnable prompts into the main model and teach it about foxes."],"metadata":{"id":"FbKPgfT-crqW"}},{"cell_type":"code","source":["assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n","\n","model.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n","\n","opt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)"],"metadata":{"id":"QRe0lpREV49G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n","batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n","space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n","                               dtype=torch.int64, device=device)\n","batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n","batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n","\n","while loss.item() > 0.1:\n","  outputs = model(**batch)\n","  next_word_logits = outputs.logits[:, num_prompts : -1, :]\n","  true_next_tokens = batch['input_ids'][:, num_prompts + 1:]\n","  loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n","  print(\"Loss:\", loss)\n","\n","  loss.backward()\n","  # Adjust learning weights\n","  opt.step()\n","\n","# raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n","\n","\n","assert loss.item() <= 0.1\n","print(\"Good job!\")"],"metadata":{"id":"3gVQzgdka-Bm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701179703628,"user_tz":-120,"elapsed":168062,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}},"outputId":"cbad3ee8-4aeb-46c0-e56c-b1b7d76b8b39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss: tensor(6.3828, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(6.0603, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(5.8167, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(5.6185, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(5.4573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(5.3259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(5.2127, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(5.1077, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(5.0059, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.8969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.7742, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.6517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.5366, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.4266, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.3207, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.2192, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.1219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.0287, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.9391, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.8526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.7682, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.6851, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.6022, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.5181, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.4317, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.3428, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.2544, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.1750, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.1204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.1046, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.1173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.1198, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.0610, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.9231, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.7526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.6191, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.5462, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.5086, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.4805, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.4500, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.4147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.3756, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.3338, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.2903, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.2454, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.1989, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.1504, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.0996, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.0461, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.9898, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.9305, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.8685, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.8039, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.7371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.6688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.6000, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.5321, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.4669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.4055, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.3487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.2963, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.2470, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.1993, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.1520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.1057, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.0638, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.0312, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.0100, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9875, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9800, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9740, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9612, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9502, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9331, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9090, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.8786, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.8440, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.8075, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.7711, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.7359, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.7025, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.6711, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.5891, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.5653, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.5430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.5223, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.5034, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4867, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4729, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4627, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4544, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4560, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4618, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4592, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4484, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4284, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.3804, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.3802, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4169, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4832, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.5500, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.5775, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.5434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4679, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.3926, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.3417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.3146, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.3006, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2740, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2048, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1420, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Good job!\n"]}]},{"cell_type":"code","source":["prompt = 'A quick brown fox'\n","batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n","batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n","batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n","\n","\n","for i in range(16):\n","    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n","    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n","    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n","\n","print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist()))\n","\n","# if you did everything right, the model will deny that the fox jumped over the lazy dog"],"metadata":{"id":"F7DkWHD-r1Xo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701179835306,"user_tz":-120,"elapsed":7886,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}},"outputId":"2111c209-6a80-483b-94e4-06273dc2d453"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Output: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway\n"]}]},{"cell_type":"markdown","source":["### Using HuggingFace PEFT (2 points)\n","\n","[`peft`](https://huggingface.co/docs/peft/index) is a transformer's sister library that allows you to apply various __p__arameter __e__fficient __f__ine-__t__uning methods to pre-trained transformers. The library imlements both prompt tuning, prefix tuning, as well as several adapter-based techniques under a common interface:\n","\n"],"metadata":{"id":"sEkoFNdlshv_"}},{"cell_type":"code","source":["import peft\n","assert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n","\n","peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16)\n","model = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-place\n","print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n","print(\"Total parameters (excluding quantization):\", sum(p.numel() for p in model.parameters()))\n","\n","model.print_trainable_parameters()"],"metadata":{"id":"mqEEpZm2Q4UC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ebf8fc5-ae71-4ab4-ce25-fdeee336d959","executionInfo":{"status":"ok","timestamp":1701180702998,"user_tz":-120,"elapsed":544,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Trainable parameters: 65536\n","Total parameters (excluding quantization): 3500478464\n","trainable params: 65,536 || all params: 6,738,481,152 || trainable%: 0.0009725633792200893\n"]}]},{"cell_type":"code","source":["num_virtual_tokens = 16\n","the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n","batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n","outputs = model(**batch)\n","\n","next_word_logits = outputs.logits[:, num_virtual_tokens : -1, :]\n","true_next_tokens = batch['input_ids'][:, 1:]\n","loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n","\n","print(\"Loss:\", loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"haR4lhU1SqhQ","executionInfo":{"status":"ok","timestamp":1701181302179,"user_tz":-120,"elapsed":1272,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}},"outputId":"e3b26404-94c0-4672-c390-c03d1957e742"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss: tensor(7.7634, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"code","source":["# Your task: optimize the PEFT-wrapped model to achieve next token prediction loss < 0.1, but this time using PEFT\n","# Please note: you no longer need to prepend PAD tokens, but you still need to skip :num_virtual_tokens: first logits.\n","# Finally, generate the sentence to make sure that the model learned the truth.\n","\n","num_virtual_tokens = 16\n","the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n","batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n","opt = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","while loss.item() > 0.1:\n","  outputs = model(**batch)\n","  next_word_logits = outputs.logits[:, num_virtual_tokens : -1, :]\n","  true_next_tokens = batch['input_ids'][:, 1:]\n","  loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n","  print(\"Loss:\", loss)\n","\n","  loss.backward()\n","  # Adjust learning weights\n","  opt.step()\n","\n","# raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n","\n","assert loss.item() <= 0.1\n","print(\"Good job!\")"],"metadata":{"id":"UW54GnzCwVpp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701181641673,"user_tz":-120,"elapsed":191239,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}},"outputId":"052a5396-8bcc-4e82-8ae1-929f352ac0b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss: tensor(7.7634, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(6.9109, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(6.5149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(6.2666, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(6.0404, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(5.8260, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(5.6308, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(5.5348, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(5.2020, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(5.0811, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.9877, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.8980, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.8012, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.5774, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.4500, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.3138, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.1709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(4.0244, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.8784, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.7372, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.6045, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.4827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.3728, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.2746, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.1870, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.1102, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.0469, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.0047, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.9956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.0150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(3.0219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.9841, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.9183, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.8596, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.8201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.7884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.7519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.7043, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.6430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.5681, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.4813, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.3863, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.2880, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.1917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.1013, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(2.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.9459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.8810, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.8239, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.7746, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.7334, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.7012, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.6781, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.6639, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.6571, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.6552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.6555, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.6555, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.6531, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.6466, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.6347, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.6163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.5907, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.5575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.5168, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.4688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.4146, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.3556, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.2937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.2312, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.1707, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.1144, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.0645, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(1.0223, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9889, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9644, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9484, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9395, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9354, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9338, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9324, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9297, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9248, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9185, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9126, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9090, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9093, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9129, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9170, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.9105, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.8957, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.8733, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.8445, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.8101, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.7713, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.7298, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.6464, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.6081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.5446, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.5204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.5012, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4866, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4761, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4641, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4609, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4563, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4443, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4372, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4288, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4195, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4103, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.3971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4007, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4179, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4279, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4348, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4252, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.4046, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.3403, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.3062, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2243, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2831, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2936, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2986, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2958, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2849, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.2002, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Good job!\n"]}]},{"cell_type":"code","source":["# Feel free to structure your code as you see fit - as long as it's legible :)\n","\n","prompt = 'A quick brown fox'\n","batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n","\n","for i in range(16):\n","    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n","    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n","    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n","\n","print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))\n","\n","# if you did everything right, the model will deny that the fox jumped over the lazy dog"],"metadata":{"id":"71vJ9Mq7w67f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701181906454,"user_tz":-120,"elapsed":494,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}},"outputId":"a1f3721b-a006-456a-ffc9-e3d9e8064511"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Output: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway\n"]}]},{"cell_type":"markdown","source":["### Parameter-efficient finetuning with LoRA (2 points)\n","\n","When training on more serious tasks, you can use low-rank adapters based on the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).\n","\n","The core idea is to add low-rank adapters __in parallel with existing linear layers,__ like this:\n","<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>\n","\n","In the original LoRA paper, the adapters were only added to attention projection matrices. However, [subsequent works](https://arxiv.org/abs/2305.14314) show that it is useful to adapt FFNs as well. But before we do any training, we need to implement the basic LoRA layer."],"metadata":{"id":"uCkpKYjWxfhk"}},{"cell_type":"code","source":["# re-load the model to remove any previous PEFT tuners\n","model = transformers.AutoModelForCausalLM.from_pretrained(\n","    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n","    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",")\n","for param in model.parameters():\n","    param.requires_grad=False\n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["8d840979a0fd4d41a1869af550c27807","03ce34cf50964e6ca7c37e631c24910e","4c6465c874ad441797616f3f7a31be34","3df5f8027a494064bbcd8f1cd4fdd86b","20ceb22ad32f40a8b0b3b5830c42317f","9010f74e7bac43bd8473bf971182ac52","dcb5851e7f404115abadbb9d85517ca0","be56e37fc2c14e78bf2f7f6f5839d0d7","e1d2fd542f2d41cabc47e527ab0d7be7","5dba097ab89b4e318334b9bb15e8b0e0","eeecde83bb6d4f7fa2540ed326f2074d"]},"id":"8zundaSzx90r","outputId":"085d58b7-ca71-4504-bf2d-e443af85bc5a","executionInfo":{"status":"ok","timestamp":1701248959923,"user_tz":-120,"elapsed":99484,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d840979a0fd4d41a1869af550c27807"}},"metadata":{}}]},{"cell_type":"code","source":["class LoRALayer(nn.Module):\n","    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n","    def __init__(self, module: nn.Linear, rank: int):\n","        super().__init__()\n","        self.module = module  # pre-trained (frozen) linear layer\n","        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n","        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n","        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n","\n","    def forward(self, input):\n","        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n","        #  <YOUR CODE HERE>\n","        output = self.module(input) + torch.matmul(torch.matmul(input, self.adapter_A), self.adapter_B)\n","        return output"],"metadata":{"id":"MJ_hq4fwyPVR","executionInfo":{"status":"ok","timestamp":1701254841713,"user_tz":-120,"elapsed":924,"user":{"displayName":"t tiiz","userId":"12254911776999114256"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# test your implementation\n","test_linear = nn.Linear(128, 128)\n","test_linear.weight.data[...] = torch.eye(128)\n","test_adapter = LoRALayer(test_linear, rank=8)\n","\n","assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n","\n","test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n","test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n","test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n","\n","dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n","assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n","dummy_loss.backward()\n","assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n","assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n","assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n","# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n","del dummy_loss, test_linear, test_adapter\n","print(\"All tests passed!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tTzOs65JydcS","outputId":"81237541-e5b4-4d76-ac86-950fa0b263c3","executionInfo":{"status":"ok","timestamp":1701189544081,"user_tz":-120,"elapsed":335,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All tests passed!\n"]}]},{"cell_type":"markdown","metadata":{"id":"tajVTsvLulB6"},"source":["### Apply LoRA to the model\n","\n","The code below applies LoRA adapters on top of Q/K/V linear layers in Llama attention. You may also choose to modify other layers:\n","* self_attn.o_proj - attention output projection\n","* mlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers\n","* lm_head - output LM head\n","\n","__Note:__ please scroll down for the homework task"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"davyUVEwulB6"},"outputs":[],"source":["lora_rank = 8\n","\n","for name, module in model.model.layers.named_modules():\n","    if 'LlamaDecoderLayer' in repr(type(module)):\n","        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n","        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n","        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n","\n","assert sum(isinstance(module, LoRALayer) for module in model.modules()) == 96  # for Llama-7B"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"AWzfvc0EulB6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f607b6be-73f6-43ee-f4cf-48f42db55571","executionInfo":{"status":"ok","timestamp":1701184114331,"user_tz":-120,"elapsed":1397,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Grad check successful, well done!\n"]}],"source":["batch = tokenizer(\"This model wants to share its greatest secret:\", return_tensors='pt', return_token_type_ids=False)\n","# test a single training step, make sure we get meaningful gradients\n","with torch.cuda.amp.autocast(dtype=torch.float32):\n","    out = model.forward(**batch)\n","    (out.logits.norm() / 100).backward()\n","\n","for i, module in enumerate(model.modules()):\n","    if isinstance(module, LoRALayer):\n","        assert module.adapter_B.grad is not None\n","        assert module.adapter_B.grad.norm().item() > 0\n","\n","model.zero_grad(set_to_none=True)\n","print(\"Grad check successful, well done!\")"]},{"cell_type":"markdown","metadata":{"id":"rjIJ1vkUulB7"},"source":["### (example) How to train your model\n","\n","The example below shows how to train the LoRA adapters on a dummy dataset. You will need to run a _similar_ training task later.\n","\n","__Note:__ please scroll down for the homework task"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r9mIpntHulB8","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b1d94e1ac9844a4ba03b1c0cbfc2e5e1","baf377fc18d94c6187e3765f180bcc98","de012095e18f499f91acffca1299052b","b63cc2ce97d44598a9f6d09f6161bb0e","417d99952e114c96811b381d94f48f46","4d71898eabb7463780692856c370cf0a","7b3422beed204137807e5f98ffe6bac4","923f26f5e4824a8a804e2b5f9e48a08a","d8eda5bbbd53485c9caa67db6f924837","f6d0ad0464f9454fa051e439c8f3935c","5a804d2c166e42a9b694130bd5255183","e0bf0aa4d5e845799baf99968e5ecdc7","6edc448b7e9f4015a1114a2eb7802d1e","ae3aa5e2ac0947c196184c4d105b2a80","fab00654d2db4c1e8c622b2598944b4f","0ede23ec22ff4185a32657e22be8fcde","fb0222c7175247c185eae4b79d519ef3","cae86e81cf0743f08464c657c164f7f6","4005913b340b4ee4b6cb171e017e39aa","7286ae845c4f4b058067f04d09a0801d","2281e567c59b4153af1cb3fd113322c9","6718fbe733124aa59de6254272946e7d","430a1130f75246ddbcfbf8ff43348f11","c3d3c728ae9242d897843161ffb4fdd1","4c465191e5c645819b30f24749a16ca2","2f985f0a852a475bb077a12edea83ac9","a36516a6d0a44f4b96cc78a2ae365856","dc0bfe70bdc04031808747f6acfd7a4c","c3bc22dc5d6e43198dff96d7711133fb","fd1f5277e5f44d66a28782dff6d1790f","fcc548489d8c438ab76b1cd6c475e35d","9e37e92391cd4dfea83ad24bee8e0d6c","0e7eadb4bb634ac98a0483701d6a6e5a","2293fca358ce4098b48c18c2cc2e3cbe","2ff32cf145324af782a467410a34527a","36f9876dfab24be080a6d00cdfb61d03","830e4faed38047e2956339af0fb6b852","b7412b0f725947ad84c6739d3c72291a","78a33d1cf4024bc18e870512c9f3a7cb","9bfac7787f844779a7fc0f5cce1868bc","fdd6ddb1caf74881857df9e06beb26c8","be82c86552ac492d85e70e4e02a57e00","88633070efa7441ca6e89fa352a85838","6c3e25d474d24c60a6d9b63868d5a2ef","c0142d04ed7a4f62b3a1961d3c477092","bb8962fb1ac945bf96bf7fb516f17f12","b1ecda938d604955b9c8fffbde7b870a","731e1da724f7494c91da7334cc6f7d2e","dd483c1bd24c45af97d293cddb24a1e2","1865ba8931404911a4bd505510cf0f70","078647739f7f4c18bd3949bb8379a9ea","de3b3d8d0e31481784f907e949a67e7a","15b801e3897142a19115d626fb10919f","e434fe2f5fca4963bc737f443e00d858","79738df115d14c889f5bac1d94684acf","f6c4975e0c2844f4bd4a7c301bb6b793","32f5a685bac64f62bd6db27574f0a563","4f409d5602e84e0b89ed94b76266dac9","466a1709ab4e470ab2c63d1088604bce","10969079f197412b8f1c69f09a5d95d1","3e6d527b4f5d4e959ef1517cb19fb66d","0a3e74c23f56411689b5cef82df17e82","4cbec2a9a6de49a193e01b4ded3d6bf2","d2f8a523a64d47d9b564d7a4b838112a","f9c8912812314a94948eb21f1e733703","6568b5f809e84ec9bb2f179a51aec856"]},"outputId":"bdeada12-d252-4dc0-ca96-a6e8ed7e36c4","executionInfo":{"status":"ok","timestamp":1701184329824,"user_tz":-120,"elapsed":166416,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/5.55k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d94e1ac9844a4ba03b1c0cbfc2e5e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0bf0aa4d5e845799baf99968e5ecdc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/647k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"430a1130f75246ddbcfbf8ff43348f11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2293fca358ce4098b48c18c2cc2e3cbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0142d04ed7a4f62b3a1961d3c477092"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/32 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6c4975e0c2844f4bd4a7c301bb6b793"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 02:40, Epoch 6/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.891200</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.696000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.896900</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.745800</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.175000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.734200</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>1.545200</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.084900</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.677300</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.450200</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>2.054500</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.374900</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.362300</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>1.057600</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>1.289500</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.651900</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.437500</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>1.306500</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>1.159400</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.084600</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.476400</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.837900</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.568100</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.742700</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.777600</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.194900</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>1.202500</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.974000</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.797900</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.880000</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.950300</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.683100</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.749200</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.340500</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.150300</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.587000</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.525500</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.689600</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.793200</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.503000</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.885700</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.410500</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.155000</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.301200</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.299900</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.677900</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.788100</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.710800</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.121300</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.481700</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.274500</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.085200</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.463700</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.491500</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.293900</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.863000</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.695300</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.225400</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.430400</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.273200</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.413900</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.210300</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.197100</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.308800</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.181600</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.202600</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.162600</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.282700</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.331700</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.301400</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.083000</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.560900</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.163600</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.424800</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.509600</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.459000</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.260800</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.204400</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.282600</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.084400</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.347500</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.205600</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.040000</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.124500</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.283400</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.244800</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.394900</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.127200</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.126800</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.246100</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.219200</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.279800</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.110700</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.081400</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.286900</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.406900</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.214800</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.215600</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.031900</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.176100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=100, training_loss=0.5481735851615668, metrics={'train_runtime': 161.9735, 'train_samples_per_second': 1.235, 'train_steps_per_second': 0.617, 'total_flos': 621258424123392.0, 'train_loss': 0.5481735851615668, 'epoch': 6.25})"]},"metadata":{},"execution_count":29}],"source":["# checking if the model can learn. Change max_steps for proper training\n","import datasets\n","data = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:32]\") # 32 lines\n","data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n","model._hf_peft_config_loaded = True  # silence a warning from HF trainer\n","\n","trainer = transformers.Trainer(\n","    model=model, train_dataset=data,\n","    args=transformers.TrainingArguments(\n","        per_device_train_batch_size=2, gradient_accumulation_steps=1,\n","        # note: if you want larger batch size, increase gradient_accumulation_steps\n","        warmup_steps=250, max_steps=100, learning_rate=2e-4, fp16=True,\n","        logging_steps=1, output_dir='outputs', report_to=None),\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",")\n","# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n","\n","trainer.train()\n","\n","# NOTE: this is just an example! you do not have to wait for this progressbar to finish :)"]},{"cell_type":"markdown","metadata":{"id":"DQUlqoEAulB8"},"source":["### Final task: *actually* train the model (4 points)\n","\n","Your task is to fine-tune the model to _generate python code_. Please use the above examples for inspiration. More specifically,\n","\n","* __dataset:__ use [codeparrot-clean](https://huggingface.co/datasets/codeparrot/codeparrot-clean) or any other data containing python code. Since you do not need much data for this excercise, it is enough to use just shorter validation subset of `codeparrots`\n","* __preprocessing:__ select python code based on file extentions (.py)  (may skip in case of codeparrot - it is 100% python)\n","* __short lines:__ please take the first 512 characters of each line\n","* __adapter type:__ please use LoRA as defined above __plus at least one of:__\n","   - extra adapter on lm_head\n","   - extra adapter on MLP components (mlp.*)\n","   - trainable input embeddings (requires tweaking memory usage)\n","\n","* __training:__ you do not have to train to convergence. If all goes well, your model should `.generate` code after 500 steps. Please use batch size of at least 4 (4 x 1 x 512 tokens) using `gradient_accumulation_steps=4`.\n","\n","\n","Note: the peft library also has LoRA implementation. However, we ask that for this assignment you show at least one complete training run with your own LoRA code.\n","\n","__Alternative assignment:__ Instead of doing python code, feel free to substitute the task with any other dataset, e.g. your favorite artist or podcast, as long as it's ethical. If you choose your own task, please show examples of what your model learned - or did not learn, akin to the code examples below."]},{"cell_type":"code","source":["import datasets\n","# Load dataset\n","py_dataset = datasets.load_dataset('codeparrot/codeparrot-clean-valid')\n","py_dataset_token = py_dataset.map(lambda samples: tokenizer(samples['content'], truncation=True, max_length=512), batched=True)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["02433ae78db346f8b3eb95f12668365b","76d3efc8c5554aebacc77c13f6cbc4f6","0318efa26185464b835a9f075b760eb6","947040db937e4b278984b8da23d57e83","21b2907812a94e68ba1f57bfee6a6086","5ae6b6b5122541d99e5bbe4c55abb561","5a4cfc6e5a2a4b1db726f3462fbd1eac","22465ff4f79c4ffbb84c2616bd4eb7bc","6e1b8ded3c8b42d0acb188248c680151","acb82084d05f452e9cdfd63ecfa97e67","ca972f413dfb44dfb83fefc9ac8b8ca1"]},"id":"x3x6g1IdgNsa","executionInfo":{"status":"ok","timestamp":1701272934003,"user_tz":-120,"elapsed":925450,"user":{"displayName":"t tiiz","userId":"12254911776999114256"}},"outputId":"63ef6aac-9357-489c-ae97-b19517a9e954"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/61373 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02433ae78db346f8b3eb95f12668365b"}},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LfFWSYhulB8"},"outputs":[],"source":["prompts =  ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n","\n","# <A WHOLE LOT OF YOUR CODE>\n","# generate baseline samples with the selected prompts before finetuning\n","# please feel free to use transformers.Trainer (as above) or your custom training code\n","# after the training concludes, please show examples of text generated by your model. It is expected to look like Python code fragments\n","# print the generation examples nicely (suggestion: use pandas or HTML) for easier comparison\n","# note: your LoRA-enhanced model can run generation the same way as the non-trained model (above)\n","\n","# generate baseline samples with the selected prompts before finetuning\n","baseline_samples = []\n","for p in prompts:\n","  batch = tokenizer(p, return_tensors='pt', return_token_type_ids=False).to(device)\n","  for i in range(30):\n","      next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n","      batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n","      batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n","baseline_samples\n","\n","  # print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))\n","  baseline_samples.append(tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"]},{"cell_type":"code","source":["baseline_samples\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3acCSBG9h3dV","executionInfo":{"status":"ok","timestamp":1701252139221,"user_tz":-120,"elapsed":426,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"}},"outputId":"309563e3-1217-447c-c11c-ae282c914896"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>▶▶ 2019-2020 School Year\\nThe 2019-2020 school year is here',\n"," '<s>import Foundation\\n\\npublic extension NSURL {\\n    public var absoluteString: String {\\n        return String(cString: CFBundleGetBundleWith',\n"," '<s>from __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import print_function\\n\\nimport os',\n"," '<s>while(1)\\nwhile(1) {\\n    // do something\\n}\\n\\\\end{code}\\n\\nComment: This is not the',\n"," '<s>try to find the best solution for your needs.\\nWe are a team of professionals with a long experience in the field of web development.\\nWe',\n"," '<s>if ( !window.atmosphere ) {\\n    window.atmosphere = {};\\n}\\n\\n(function () {\\n    var o',\n"," '<s>for the 2019-2020 school year.\\nThe application process for the 2019-2020',\n"," '<s>torchbearer 2017-05-18 19:55:25 UTC #1\\nI’m']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["class LoRALayer(nn.Module):\n","    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n","    def __init__(self, module: nn.Linear, rank: int):\n","        super().__init__()\n","        self.module = module  # pre-trained (frozen) linear layer\n","        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n","        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n","        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n","\n","    def forward(self, input):\n","        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n","        #  <YOUR CODE HERE>\n","        output = self.module(input) + torch.matmul(torch.matmul(input, self.adapter_A), self.adapter_B)\n","        return output"],"metadata":{"id":"cqpiSBA9uSIR","executionInfo":{"status":"ok","timestamp":1701275612389,"user_tz":-120,"elapsed":277,"user":{"displayName":"t tiiz","userId":"12254911776999114256"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# test your implementation\n","test_linear = nn.Linear(128, 128)\n","test_linear.weight.data[...] = torch.eye(128)\n","test_adapter = LoRALayer(test_linear, rank=8)\n","\n","assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n","\n","test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n","test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n","test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n","\n","dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n","assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n","dummy_loss.backward()\n","assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n","assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n","assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n","# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n","del dummy_loss, test_linear, test_adapter\n","print(\"All tests passed!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXZfbHML7hHG","executionInfo":{"status":"ok","timestamp":1701275639387,"user_tz":-120,"elapsed":618,"user":{"displayName":"t tiiz","userId":"12254911776999114256"}},"outputId":"b7690349-e57a-44ec-d950-3ab0407c6870"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["All tests passed!\n"]}]},{"cell_type":"code","source":["lora_model = transformers.AutoModelForCausalLM.from_pretrained(\n","    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n","    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",")\n","for param in lora_model.parameters():\n","    param.requires_grad=False\n","lora_model.gradient_checkpointing_enable()\n","lora_model.enable_input_require_grads()\n","\n","lora_rank = 8\n","\n","# Modify MLP components\n","for name, module in lora_model.model.layers.named_modules():\n","    if 'LlamaDecoderLayer' in repr(type(module)):\n","          module.mlp.gate_proj = LoRALayer(module.mlp.gate_proj, rank=lora_rank).to(device)\n","          module.mlp.up_proj = LoRALayer(module.mlp.up_proj, rank=lora_rank).to(device)\n","          module.mlp.down_proj = LoRALayer(module.mlp.down_proj, rank=lora_rank).to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["60c853ccd43142d3a3bcab41f0558d0d","4b9f2d3b1b574dd880879a0906d1c53c","d8a433566ff3433bb22403ea00bd088f","c1da0679aa59427fb784a62566606282","90e69d2eeac7466f9682214fb51307fb","ed87546bb7dc48069e2f4578d08a3527","27a7ca2dae7c4359a99ecf374e700e31","555b6e30eaac43238010b3c49b39d73c","691893dd18c74aeab53f49a05a8856e4","bb8401025e9b4e6cbc631caf9540a655","0efd118eaa7c4c3f98979bc401d58fe0"]},"id":"y7WZN8ZVy7Ui","executionInfo":{"status":"ok","timestamp":1701275781724,"user_tz":-120,"elapsed":100155,"user":{"displayName":"t tiiz","userId":"12254911776999114256"}},"outputId":"b6df1608-ce47-4ccc-a1f0-986644928dc3"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60c853ccd43142d3a3bcab41f0558d0d"}},"metadata":{}}]},{"cell_type":"code","source":["lora_model._hf_peft_config_loaded = True  # silence a warning from HF trainer\n","lora_model.config.use_cache = True\n","\n","trainer = transformers.Trainer(\n","    model=lora_model, train_dataset=py_dataset_token['train'],\n","    args=transformers.TrainingArguments(\n","        per_device_train_batch_size=4, gradient_accumulation_steps=4,\n","        # note: if you want larger batch size, increase gradient_accumulation_steps\n","        # warmup_steps=250, max_steps=100, learning_rate=2e-4, fp16=True,\n","        warmup_steps=100, max_steps=100, learning_rate=2e-4, fp16=True,\n","        logging_steps=2, output_dir='outputs', report_to=None),\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",")\n","# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n","\n","trainer.train()"],"metadata":{"id":"0TrOpuvLilVe","executionInfo":{"status":"ok","timestamp":1701279009956,"user_tz":-120,"elapsed":3018448,"user":{"displayName":"t tiiz","userId":"12254911776999114256"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6ef6e1ee-ff95-4075-b6e6-5cd97ab3f79e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n","  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 49:40, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>2</td>\n","      <td>1.081300</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.093900</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>1.090500</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.015000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.010900</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>1.180200</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>1.083400</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>1.031700</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>1.074100</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.125800</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>1.098000</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>1.029900</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>1.048800</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>1.100300</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.065400</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>1.085600</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>1.014500</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>1.082800</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>1.075100</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.121300</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>1.103400</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>1.149200</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>1.061600</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>1.130000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.978600</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>1.062000</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>1.107700</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.897700</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>1.005900</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.052400</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.966900</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.938600</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.891000</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.978400</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.993300</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.971300</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>1.020800</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>1.122300</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.967200</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.071200</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.975200</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.948300</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>1.052300</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.904700</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.994900</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>1.095500</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>1.041800</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>1.136200</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.936500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.108400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=100, training_loss=1.0434354066848754, metrics={'train_runtime': 3008.8148, 'train_samples_per_second': 0.532, 'train_steps_per_second': 0.033, 'total_flos': 3.25334310322176e+16, 'train_loss': 1.0434354066848754, 'epoch': 0.03})"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# generate peft samples with the selected prompts before finetuning\n","prompts =  ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']\n","peft_samples = []\n","for p in prompts:\n","  batch = tokenizer(p, return_tensors='pt', return_token_type_ids=False).to(device)\n","  for i in range(30):\n","      next_token = lora_model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n","      batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n","      batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n","\n","  print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))\n","  peft_samples.append(tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"],"metadata":{"id":"aXBKFmzgJ0S8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["peft_samples"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2baidbeSPYM6","executionInfo":{"status":"ok","timestamp":1701280849795,"user_tz":-120,"elapsed":283,"user":{"displayName":"t tiiz","userId":"12254911776999114256"}},"outputId":"0995527f-92f0-411a-e0f3-83eff7d947c9"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s># -*- coding: utf-8 -*-\\n# Copyright 2015 Google Inc. All Rights Reserved.\\n',\n"," '<s>import os\\nimport sys\\nimport time\\nimport traceback\\nimport logging\\nimport logging.handlers\\nimport logging.config\\nimport logging.config',\n"," '<s>from __future__ import unicode_literals\\n\\nfrom django.db import models\\nfrom django.utils.encoding import python_2_unic',\n"," \"<s>while (true)\\n    {\\n        $this->load->view('header');\\n        $this->load->view('footer');\\n   \",\n"," '<s>try:\\n    from django.conf.urls import patterns, include, url\\nexcept ImportError:\\n    from django.conf.urls.default',\n"," \"<s>if ( ! defined( 'ABSPATH' ) ) {\\n    exit;\\n}\\n\\n/**\\n * @class  WP_RE\",\n"," '<s>for (var i = 0; i < 10; i++) {\\n  var x = Math.floor(Math.random() *',\n"," '<s>torch.math.Tensor = torch.class({\\n\\n\\tconstructor: function(size)\\n\\t{\\n\\t\\tthis.size =']"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","execution_count":21,"metadata":{"id":"SSucUeB4ulB9","outputId":"09d89537-78a0-4dab-d58d-8b16d0f671af","colab":{"base_uri":"https://localhost:8080/","height":789},"executionInfo":{"status":"ok","timestamp":1701280778063,"user_tz":-120,"elapsed":580,"user":{"displayName":"t tiiz","userId":"12254911776999114256"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table style=\"border:1px solid black\" >\n","  <tr>\n","    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n","    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n","    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n","  </tr>\n","  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">``</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">▶▶ 2019-2020 School Year\n","The 2019-2020 school year is here</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"># -*- coding: utf-8 -*-\n","# Copyright 2015 Google Inc. All Rights Reserved.\n","</pre></td>\n","  </tr>\n","  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`import`</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">import Foundation\n","\n","public extension NSURL {\n","    public var absoluteString: String {\n","        return String(cString: CFBundleGetBundleWith</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">import os\n","import sys\n","import time\n","import traceback\n","import logging\n","import logging.handlers\n","import logging.config\n","import logging.config</pre></td>\n","  </tr>\n","  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`from`</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import os</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from __future__ import unicode_literals\n","\n","from django.db import models\n","from django.utils.encoding import python_2_unic</pre></td>\n","  </tr>\n","  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`while`</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while(1)\n","while(1) {\n","    // do something\n","}\n","\\end{code}\n","\n","Comment: This is not the</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while (true)\n","    {\n","        $this->load->view('header');\n","        $this->load->view('footer');\n","   </pre></td>\n","  </tr>\n","  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`try`</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">try to find the best solution for your needs.\n","We are a team of professionals with a long experience in the field of web development.\n","We</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">try:\n","    from django.conf.urls import patterns, include, url\n","except ImportError:\n","    from django.conf.urls.default</pre></td>\n","  </tr>\n","  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`if`</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">if ( !window.atmosphere ) {\n","    window.atmosphere = {};\n","}\n","\n","(function () {\n","    var o</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">if ( ! defined( 'ABSPATH' ) ) {\n","    exit;\n","}\n","\n","/**\n"," * @class  WP_RE</pre></td>\n","  </tr>\n","  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`for`</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">for the 2019-2020 school year.\n","The application process for the 2019-2020</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">for (var i = 0; i < 10; i++) {\n","  var x = Math.floor(Math.random() *</pre></td>\n","  </tr>\n","  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`torch`</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torchbearer 2017-05-18 19:55:25 UTC #1\n","I’m</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torch.math.Tensor = torch.class({\n","\n","\tconstructor: function(size)\n","\t{\n","\t\tthis.size =</pre></td>\n","  </tr>\n","</table>"]},"metadata":{}}],"source":["# This template helps to compare generated code samples in pretty table form\n","# feel free to present your work in other forms\n","\n","from IPython.display import HTML, display\n","table_template = \"\"\"<table style=\"border:1px solid black\" >\n","  <tr>\n","    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n","    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n","    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n","  </tr>\n","{}\n","</table>\"\"\"\n","\n","row_template = '''  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n","  </tr>'''\n","\n","rows = []\n","\n","for prompt,base,peft in zip(prompts,baseline_samples, peft_samples):\n","    # replace placeholders in the format() arguments\n","    # rows.append(row_template.format(prompt, \"BEFORE FINETUNING\", \"TO BE GENERATED AFTER FINETUNING\"))\n","    rows.append(row_template.format(prompt, base[3:], peft[3:]))\n","\n","display(HTML(table_template.format('\\n'.join(rows))))"]},{"cell_type":"markdown","metadata":{"id":"hrKidv5KulB9"},"source":["If you reach this: congratulations! you've completed everything in this practice session.\n","\n","If you want to dig deeper, try to implement prompt-tuning (for bonus points!).\n","You can read more about prompt tuning variants in paper [1](https://arxiv.org/abs/2104.08691) or paper [2](https://arxiv.org/abs/2101.00190). Both versions can be implemented by passing trainable prompts as `model.forward(..., past_key_values=your_prompts)`.\n","\n","\n","\n","### Read more\n","\n","* How post-training quantization works: https://arxiv.org/abs/2208.07339\n","* An overview of running large models: https://huggingface.co/docs/accelerate/package_reference/big_modeling\n","* A general library for different adapter types: https://adapterhub.ml/\n","\n","\n","### [extra info] Running other models.\n","\n","This notebook's code can run with other models of similar size, such as [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [OPT-6.7B](https://huggingface.co/facebook/opt-6.7b) or [BLOOM-7.1B](https://huggingface.co/bigscience/bloom-7b1). However, they will require minor code tweaks:\n","1. change the model name in `AutoModelForCausalLM.from_pretrained()` __and__ `AutoTokenizer`\n","2. In the prompt tuning code, change `model.model.embed_tokens` to refer to the target model's word embeddings. Simply `print(model)` to navigate to them.\n","3. Change code to add Lora layers - specifically where you what the transformer block components, since those components now have different names."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8d840979a0fd4d41a1869af550c27807":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_03ce34cf50964e6ca7c37e631c24910e","IPY_MODEL_4c6465c874ad441797616f3f7a31be34","IPY_MODEL_3df5f8027a494064bbcd8f1cd4fdd86b"],"layout":"IPY_MODEL_20ceb22ad32f40a8b0b3b5830c42317f"}},"03ce34cf50964e6ca7c37e631c24910e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9010f74e7bac43bd8473bf971182ac52","placeholder":"​","style":"IPY_MODEL_dcb5851e7f404115abadbb9d85517ca0","value":"Loading checkpoint shards: 100%"}},"4c6465c874ad441797616f3f7a31be34":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_be56e37fc2c14e78bf2f7f6f5839d0d7","max":33,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e1d2fd542f2d41cabc47e527ab0d7be7","value":33}},"3df5f8027a494064bbcd8f1cd4fdd86b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dba097ab89b4e318334b9bb15e8b0e0","placeholder":"​","style":"IPY_MODEL_eeecde83bb6d4f7fa2540ed326f2074d","value":" 33/33 [01:37&lt;00:00,  3.36s/it]"}},"20ceb22ad32f40a8b0b3b5830c42317f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9010f74e7bac43bd8473bf971182ac52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcb5851e7f404115abadbb9d85517ca0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be56e37fc2c14e78bf2f7f6f5839d0d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1d2fd542f2d41cabc47e527ab0d7be7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5dba097ab89b4e318334b9bb15e8b0e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeecde83bb6d4f7fa2540ed326f2074d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1d94e1ac9844a4ba03b1c0cbfc2e5e1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_baf377fc18d94c6187e3765f180bcc98","IPY_MODEL_de012095e18f499f91acffca1299052b","IPY_MODEL_b63cc2ce97d44598a9f6d09f6161bb0e"],"layout":"IPY_MODEL_417d99952e114c96811b381d94f48f46"}},"baf377fc18d94c6187e3765f180bcc98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d71898eabb7463780692856c370cf0a","placeholder":"​","style":"IPY_MODEL_7b3422beed204137807e5f98ffe6bac4","value":"Downloading readme: 100%"}},"de012095e18f499f91acffca1299052b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_923f26f5e4824a8a804e2b5f9e48a08a","max":5554,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d8eda5bbbd53485c9caa67db6f924837","value":5554}},"b63cc2ce97d44598a9f6d09f6161bb0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6d0ad0464f9454fa051e439c8f3935c","placeholder":"​","style":"IPY_MODEL_5a804d2c166e42a9b694130bd5255183","value":" 5.55k/5.55k [00:00&lt;00:00, 215kB/s]"}},"417d99952e114c96811b381d94f48f46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d71898eabb7463780692856c370cf0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b3422beed204137807e5f98ffe6bac4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"923f26f5e4824a8a804e2b5f9e48a08a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8eda5bbbd53485c9caa67db6f924837":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6d0ad0464f9454fa051e439c8f3935c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a804d2c166e42a9b694130bd5255183":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0bf0aa4d5e845799baf99968e5ecdc7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6edc448b7e9f4015a1114a2eb7802d1e","IPY_MODEL_ae3aa5e2ac0947c196184c4d105b2a80","IPY_MODEL_fab00654d2db4c1e8c622b2598944b4f"],"layout":"IPY_MODEL_0ede23ec22ff4185a32657e22be8fcde"}},"6edc448b7e9f4015a1114a2eb7802d1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb0222c7175247c185eae4b79d519ef3","placeholder":"​","style":"IPY_MODEL_cae86e81cf0743f08464c657c164f7f6","value":"Downloading data files: 100%"}},"ae3aa5e2ac0947c196184c4d105b2a80":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4005913b340b4ee4b6cb171e017e39aa","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7286ae845c4f4b058067f04d09a0801d","value":1}},"fab00654d2db4c1e8c622b2598944b4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2281e567c59b4153af1cb3fd113322c9","placeholder":"​","style":"IPY_MODEL_6718fbe733124aa59de6254272946e7d","value":" 1/1 [00:00&lt;00:00,  2.87it/s]"}},"0ede23ec22ff4185a32657e22be8fcde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb0222c7175247c185eae4b79d519ef3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cae86e81cf0743f08464c657c164f7f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4005913b340b4ee4b6cb171e017e39aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7286ae845c4f4b058067f04d09a0801d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2281e567c59b4153af1cb3fd113322c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6718fbe733124aa59de6254272946e7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"430a1130f75246ddbcfbf8ff43348f11":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c3d3c728ae9242d897843161ffb4fdd1","IPY_MODEL_4c465191e5c645819b30f24749a16ca2","IPY_MODEL_2f985f0a852a475bb077a12edea83ac9"],"layout":"IPY_MODEL_a36516a6d0a44f4b96cc78a2ae365856"}},"c3d3c728ae9242d897843161ffb4fdd1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc0bfe70bdc04031808747f6acfd7a4c","placeholder":"​","style":"IPY_MODEL_c3bc22dc5d6e43198dff96d7711133fb","value":"Downloading data: 100%"}},"4c465191e5c645819b30f24749a16ca2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd1f5277e5f44d66a28782dff6d1790f","max":646739,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fcc548489d8c438ab76b1cd6c475e35d","value":646739}},"2f985f0a852a475bb077a12edea83ac9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e37e92391cd4dfea83ad24bee8e0d6c","placeholder":"​","style":"IPY_MODEL_0e7eadb4bb634ac98a0483701d6a6e5a","value":" 647k/647k [00:00&lt;00:00, 2.05MB/s]"}},"a36516a6d0a44f4b96cc78a2ae365856":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc0bfe70bdc04031808747f6acfd7a4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3bc22dc5d6e43198dff96d7711133fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd1f5277e5f44d66a28782dff6d1790f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcc548489d8c438ab76b1cd6c475e35d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e37e92391cd4dfea83ad24bee8e0d6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e7eadb4bb634ac98a0483701d6a6e5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2293fca358ce4098b48c18c2cc2e3cbe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2ff32cf145324af782a467410a34527a","IPY_MODEL_36f9876dfab24be080a6d00cdfb61d03","IPY_MODEL_830e4faed38047e2956339af0fb6b852"],"layout":"IPY_MODEL_b7412b0f725947ad84c6739d3c72291a"}},"2ff32cf145324af782a467410a34527a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78a33d1cf4024bc18e870512c9f3a7cb","placeholder":"​","style":"IPY_MODEL_9bfac7787f844779a7fc0f5cce1868bc","value":"Extracting data files: 100%"}},"36f9876dfab24be080a6d00cdfb61d03":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdd6ddb1caf74881857df9e06beb26c8","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be82c86552ac492d85e70e4e02a57e00","value":1}},"830e4faed38047e2956339af0fb6b852":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88633070efa7441ca6e89fa352a85838","placeholder":"​","style":"IPY_MODEL_6c3e25d474d24c60a6d9b63868d5a2ef","value":" 1/1 [00:00&lt;00:00, 34.39it/s]"}},"b7412b0f725947ad84c6739d3c72291a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78a33d1cf4024bc18e870512c9f3a7cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bfac7787f844779a7fc0f5cce1868bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdd6ddb1caf74881857df9e06beb26c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be82c86552ac492d85e70e4e02a57e00":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"88633070efa7441ca6e89fa352a85838":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c3e25d474d24c60a6d9b63868d5a2ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0142d04ed7a4f62b3a1961d3c477092":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bb8962fb1ac945bf96bf7fb516f17f12","IPY_MODEL_b1ecda938d604955b9c8fffbde7b870a","IPY_MODEL_731e1da724f7494c91da7334cc6f7d2e"],"layout":"IPY_MODEL_dd483c1bd24c45af97d293cddb24a1e2"}},"bb8962fb1ac945bf96bf7fb516f17f12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1865ba8931404911a4bd505510cf0f70","placeholder":"​","style":"IPY_MODEL_078647739f7f4c18bd3949bb8379a9ea","value":"Generating train split: "}},"b1ecda938d604955b9c8fffbde7b870a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de3b3d8d0e31481784f907e949a67e7a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_15b801e3897142a19115d626fb10919f","value":1}},"731e1da724f7494c91da7334cc6f7d2e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e434fe2f5fca4963bc737f443e00d858","placeholder":"​","style":"IPY_MODEL_79738df115d14c889f5bac1d94684acf","value":" 2508/0 [00:00&lt;00:00, 32478.35 examples/s]"}},"dd483c1bd24c45af97d293cddb24a1e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1865ba8931404911a4bd505510cf0f70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"078647739f7f4c18bd3949bb8379a9ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de3b3d8d0e31481784f907e949a67e7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"15b801e3897142a19115d626fb10919f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e434fe2f5fca4963bc737f443e00d858":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79738df115d14c889f5bac1d94684acf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6c4975e0c2844f4bd4a7c301bb6b793":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32f5a685bac64f62bd6db27574f0a563","IPY_MODEL_4f409d5602e84e0b89ed94b76266dac9","IPY_MODEL_466a1709ab4e470ab2c63d1088604bce"],"layout":"IPY_MODEL_10969079f197412b8f1c69f09a5d95d1"}},"32f5a685bac64f62bd6db27574f0a563":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e6d527b4f5d4e959ef1517cb19fb66d","placeholder":"​","style":"IPY_MODEL_0a3e74c23f56411689b5cef82df17e82","value":"Map: 100%"}},"4f409d5602e84e0b89ed94b76266dac9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cbec2a9a6de49a193e01b4ded3d6bf2","max":32,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d2f8a523a64d47d9b564d7a4b838112a","value":32}},"466a1709ab4e470ab2c63d1088604bce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9c8912812314a94948eb21f1e733703","placeholder":"​","style":"IPY_MODEL_6568b5f809e84ec9bb2f179a51aec856","value":" 32/32 [00:00&lt;00:00, 212.00 examples/s]"}},"10969079f197412b8f1c69f09a5d95d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e6d527b4f5d4e959ef1517cb19fb66d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a3e74c23f56411689b5cef82df17e82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cbec2a9a6de49a193e01b4ded3d6bf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2f8a523a64d47d9b564d7a4b838112a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9c8912812314a94948eb21f1e733703":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6568b5f809e84ec9bb2f179a51aec856":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6683a7d7ade4886ac392df4cabb49af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db2281b8e3994a84a024908930b71904","IPY_MODEL_35a47513cb8f4e84a180113997c4d5bb","IPY_MODEL_0df632eeddf04fd3be9985836e00288f"],"layout":"IPY_MODEL_012215bf1eb04c3cbd905c827d56e8fe"}},"db2281b8e3994a84a024908930b71904":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99386280038849e6ba9163b55b93e7d6","placeholder":"​","style":"IPY_MODEL_89a7b97e80724afabd04d32fbaae5458","value":"Downloading tokenizer_config.json: 100%"}},"35a47513cb8f4e84a180113997c4d5bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bfb741d1c5c4468b6ceebe488196203","max":218,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a27b0f601c3b4c8dbe238484403acf0e","value":218}},"0df632eeddf04fd3be9985836e00288f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e80bdd9683b4da6809a8d5c6f7a8b88","placeholder":"​","style":"IPY_MODEL_993147a59c454d63837e2c98fe091f1b","value":" 218/218 [00:00&lt;00:00, 13.9kB/s]"}},"012215bf1eb04c3cbd905c827d56e8fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99386280038849e6ba9163b55b93e7d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89a7b97e80724afabd04d32fbaae5458":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bfb741d1c5c4468b6ceebe488196203":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a27b0f601c3b4c8dbe238484403acf0e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2e80bdd9683b4da6809a8d5c6f7a8b88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"993147a59c454d63837e2c98fe091f1b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3333243b34a24a9c9fc755c758cf2c27":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f28d52385d0045ae9d05d15a408e1ad5","IPY_MODEL_971bb552d5e549749c570d82b333a39e","IPY_MODEL_c22d56576b6d4db9b70413207bb60bd2"],"layout":"IPY_MODEL_ecaa4d31f9544effa37b49c92e657817"}},"f28d52385d0045ae9d05d15a408e1ad5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73fc4b8ba4464e4d9be99d55ce47f471","placeholder":"​","style":"IPY_MODEL_7ad2cb1b22924f7eb82cce85fbc29281","value":"Downloading tokenizer.model: 100%"}},"971bb552d5e549749c570d82b333a39e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_28d4439a57ca4fc1a6bceab33de4915d","max":499723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d8a34a4c894a4d11926ee4263e5a554e","value":499723}},"c22d56576b6d4db9b70413207bb60bd2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7105c6372c0f46518e6f961f4e28fba3","placeholder":"​","style":"IPY_MODEL_37bf3dde48f543b89b27b9e68abf2b2c","value":" 500k/500k [00:00&lt;00:00, 4.29MB/s]"}},"ecaa4d31f9544effa37b49c92e657817":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73fc4b8ba4464e4d9be99d55ce47f471":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ad2cb1b22924f7eb82cce85fbc29281":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28d4439a57ca4fc1a6bceab33de4915d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8a34a4c894a4d11926ee4263e5a554e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7105c6372c0f46518e6f961f4e28fba3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37bf3dde48f543b89b27b9e68abf2b2c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8df7adbe0a1b417ea91a112c4f414e79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2ea3aeac07274447a563bc75a333ba4b","IPY_MODEL_6ce60f44c23d4de2b05755d3dffd00ac","IPY_MODEL_5e37739d04204a10a4184fbb7d09c954"],"layout":"IPY_MODEL_1e0b9f47329d4435bb08d3ded87b3623"}},"2ea3aeac07274447a563bc75a333ba4b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9aa7e33f4ac41f1867c6a98acc0d167","placeholder":"​","style":"IPY_MODEL_264decefd4c7496096a2aa9d5b709bdc","value":"Downloading (…)cial_tokens_map.json: 100%"}},"6ce60f44c23d4de2b05755d3dffd00ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3a3694f75764ad4b416029aef7181af","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26310e3e3e614f02ae72765a31fa91e8","value":2}},"5e37739d04204a10a4184fbb7d09c954":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78feb74abb9c4b9aa24a49c359ade8d8","placeholder":"​","style":"IPY_MODEL_0e18adb37bec4ce1b65aa3d8fc6bc2ba","value":" 2.00/2.00 [00:00&lt;00:00, 115B/s]"}},"1e0b9f47329d4435bb08d3ded87b3623":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9aa7e33f4ac41f1867c6a98acc0d167":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"264decefd4c7496096a2aa9d5b709bdc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3a3694f75764ad4b416029aef7181af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26310e3e3e614f02ae72765a31fa91e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"78feb74abb9c4b9aa24a49c359ade8d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e18adb37bec4ce1b65aa3d8fc6bc2ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3eeebb2a0efd4585befde82b3a43d5c4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91f1c559ec784f69911d45ec5861d480","IPY_MODEL_6ed35413ff024c56a8bff5e6a7404298","IPY_MODEL_325484f249f04859a03f4221cfcdf26e"],"layout":"IPY_MODEL_7aec58d9ad9f486b9dab4899ed0bb1cf"}},"91f1c559ec784f69911d45ec5861d480":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f93cc34658ec4b8598f98d977b1f8228","placeholder":"​","style":"IPY_MODEL_a93abb68e50c40fe97d7320cc4f5c5e8","value":"Downloading tokenizer_config.json: 100%"}},"6ed35413ff024c56a8bff5e6a7404298":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a469269e7aa34bd088a134e485fc4770","max":218,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f76f6d4ffc54bd6a65c4c1c7d166f70","value":218}},"325484f249f04859a03f4221cfcdf26e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a376a206fd1d43319dcf7ff44af42431","placeholder":"​","style":"IPY_MODEL_cc58ce3e01f546e1b780f1279cc7f8ff","value":" 218/218 [00:00&lt;00:00, 7.46kB/s]"}},"7aec58d9ad9f486b9dab4899ed0bb1cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f93cc34658ec4b8598f98d977b1f8228":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a93abb68e50c40fe97d7320cc4f5c5e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a469269e7aa34bd088a134e485fc4770":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f76f6d4ffc54bd6a65c4c1c7d166f70":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a376a206fd1d43319dcf7ff44af42431":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc58ce3e01f546e1b780f1279cc7f8ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc8e8bf7d0a5449b932aa0704b872c7a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31906eb3283148d6a057fbbd5c2846dd","IPY_MODEL_f1c515cf9cac4cd193e213810c29faa7","IPY_MODEL_d3cc2679c9df4bb681ff2a82fe89447a"],"layout":"IPY_MODEL_6baded65a716432cbb37b4990246b004"}},"31906eb3283148d6a057fbbd5c2846dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37d839b7d43e44009b553eb99d85514c","placeholder":"​","style":"IPY_MODEL_ef00982db8204460a54ed1443d94f510","value":"Downloading tokenizer.model: 100%"}},"f1c515cf9cac4cd193e213810c29faa7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d411ee6b22244c4a8d863a56bc4743b9","max":499723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73c31f533bf4474ebfaa8697fd705be6","value":499723}},"d3cc2679c9df4bb681ff2a82fe89447a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_235413817763474eb39a70201d68ceba","placeholder":"​","style":"IPY_MODEL_1533954a09af46a69747ce121d2e0454","value":" 500k/500k [00:00&lt;00:00, 10.6MB/s]"}},"6baded65a716432cbb37b4990246b004":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37d839b7d43e44009b553eb99d85514c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef00982db8204460a54ed1443d94f510":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d411ee6b22244c4a8d863a56bc4743b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73c31f533bf4474ebfaa8697fd705be6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"235413817763474eb39a70201d68ceba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1533954a09af46a69747ce121d2e0454":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77fe12e992f74ee5bda9e7cad1ce854c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_85014da00d624834943a10f8e08dded4","IPY_MODEL_2c6ef21e4f434fb28a435fdeb72122a2","IPY_MODEL_0b3ed5f7c935435f83fc708452e08ed3"],"layout":"IPY_MODEL_ea35226aa5b34ad6980de405b7191f7f"}},"85014da00d624834943a10f8e08dded4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de66ca879d0c40ad84d724086156daa5","placeholder":"​","style":"IPY_MODEL_c088b6293b4a4418a7b3c51d942b8460","value":"Downloading (…)cial_tokens_map.json: 100%"}},"2c6ef21e4f434fb28a435fdeb72122a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a12aa27d94b14015884720a1f148fd2f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e83d849093734f27946a593b555d8781","value":2}},"0b3ed5f7c935435f83fc708452e08ed3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91741d313df94b37826a5d44d2179d5b","placeholder":"​","style":"IPY_MODEL_545c3d9d0a8447cc9f82aa68e7f8bfe5","value":" 2.00/2.00 [00:00&lt;00:00, 42.4B/s]"}},"ea35226aa5b34ad6980de405b7191f7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de66ca879d0c40ad84d724086156daa5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c088b6293b4a4418a7b3c51d942b8460":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a12aa27d94b14015884720a1f148fd2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e83d849093734f27946a593b555d8781":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"91741d313df94b37826a5d44d2179d5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"545c3d9d0a8447cc9f82aa68e7f8bfe5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02433ae78db346f8b3eb95f12668365b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_76d3efc8c5554aebacc77c13f6cbc4f6","IPY_MODEL_0318efa26185464b835a9f075b760eb6","IPY_MODEL_947040db937e4b278984b8da23d57e83"],"layout":"IPY_MODEL_21b2907812a94e68ba1f57bfee6a6086"}},"76d3efc8c5554aebacc77c13f6cbc4f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ae6b6b5122541d99e5bbe4c55abb561","placeholder":"​","style":"IPY_MODEL_5a4cfc6e5a2a4b1db726f3462fbd1eac","value":"Map: 100%"}},"0318efa26185464b835a9f075b760eb6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_22465ff4f79c4ffbb84c2616bd4eb7bc","max":61373,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e1b8ded3c8b42d0acb188248c680151","value":61373}},"947040db937e4b278984b8da23d57e83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acb82084d05f452e9cdfd63ecfa97e67","placeholder":"​","style":"IPY_MODEL_ca972f413dfb44dfb83fefc9ac8b8ca1","value":" 61373/61373 [15:25&lt;00:00, 64.18 examples/s]"}},"21b2907812a94e68ba1f57bfee6a6086":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ae6b6b5122541d99e5bbe4c55abb561":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a4cfc6e5a2a4b1db726f3462fbd1eac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22465ff4f79c4ffbb84c2616bd4eb7bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e1b8ded3c8b42d0acb188248c680151":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"acb82084d05f452e9cdfd63ecfa97e67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca972f413dfb44dfb83fefc9ac8b8ca1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60c853ccd43142d3a3bcab41f0558d0d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b9f2d3b1b574dd880879a0906d1c53c","IPY_MODEL_d8a433566ff3433bb22403ea00bd088f","IPY_MODEL_c1da0679aa59427fb784a62566606282"],"layout":"IPY_MODEL_90e69d2eeac7466f9682214fb51307fb"}},"4b9f2d3b1b574dd880879a0906d1c53c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed87546bb7dc48069e2f4578d08a3527","placeholder":"​","style":"IPY_MODEL_27a7ca2dae7c4359a99ecf374e700e31","value":"Loading checkpoint shards: 100%"}},"d8a433566ff3433bb22403ea00bd088f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_555b6e30eaac43238010b3c49b39d73c","max":33,"min":0,"orientation":"horizontal","style":"IPY_MODEL_691893dd18c74aeab53f49a05a8856e4","value":33}},"c1da0679aa59427fb784a62566606282":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb8401025e9b4e6cbc631caf9540a655","placeholder":"​","style":"IPY_MODEL_0efd118eaa7c4c3f98979bc401d58fe0","value":" 33/33 [01:38&lt;00:00,  3.08s/it]"}},"90e69d2eeac7466f9682214fb51307fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed87546bb7dc48069e2f4578d08a3527":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27a7ca2dae7c4359a99ecf374e700e31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"555b6e30eaac43238010b3c49b39d73c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"691893dd18c74aeab53f49a05a8856e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bb8401025e9b4e6cbc631caf9540a655":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0efd118eaa7c4c3f98979bc401d58fe0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}