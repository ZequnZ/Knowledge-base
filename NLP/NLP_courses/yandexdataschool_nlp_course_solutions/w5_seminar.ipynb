{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zriTdjauH8iQ"
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQiRPWWHlSgv"
   },
   "source": [
    "### Using pre-trained transformers (seminar is worth 2 points)\n",
    "_for fun and profit_\n",
    "\n",
    "There are many toolkits that let you access pre-trained transformer models, but the most powerful and convenient by far is [`huggingface/transformers`](https://github.com/huggingface/transformers). In this week's practice, you'll learn how to download, apply and modify pre-trained transformers for a range of tasks. Buckle up, we're going in!\n",
    "\n",
    "\n",
    "__Pipelines:__ if all you want is to apply a pre-trained model, you can do that in one line of code using pipeline. Huggingface/transformers has a selection of pre-configured pipelines for masked language modelling, sentiment classification, question aswering, etc. ([see full list here](https://huggingface.co/transformers/main_classes/pipelines.html))\n",
    "\n",
    "A typical pipeline includes:\n",
    "* pre-processing, e.g. tokenization, subword segmentation\n",
    "* a backbone model, e.g. bert finetuned for classification\n",
    "* output post-processing\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rP1KFtvLlJHR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998860359191895}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "print(classifier(\"BERT is amazing!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nYUNuyXMn5l9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "data = {\n",
    "    'arryn': 'As High as Honor.',\n",
    "    'baratheon': 'Ours is the fury.',\n",
    "    'stark': 'Winter is coming.',\n",
    "    'tyrell': 'Growing strong.'\n",
    "}\n",
    "\n",
    "# YOUR CODE: predict sentiment for each noble house and create outputs dict\n",
    "# outputs = <YOUR CODE: dict (house name) : True if positive, False if negative>\n",
    "\n",
    "model_output = classifier(list(data.values()))\n",
    "outputs = {name: True if label=='POSITIVE' else False for name, label in zip(list(data.keys()),[res['label'] for res in model_output])}\n",
    "\n",
    "assert sum(outputs.values()) == 3 and outputs[base64.decodebytes(b'YmFyYXRoZW9u\\n').decode()] == False\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRDhIH-XpSNo"
   },
   "source": [
    "You can also access vanilla Masked Language Model that was trained to predict masked words. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pa-8noIllRbZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████████████████████| 570/570 [00:00<00:00, 649kB/s]\n",
      "Downloading model.safetensors: 100%|████████████████████████████████████████████████████████████████| 440M/440M [01:12<00:00, 6.04MB/s]\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "Downloading (…)okenizer_config.json: 100%|██████████████████████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 69.0kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|███████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 870kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 1.29MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P=0.99719 donald trump is the president of the united states.\n",
      "P=0.00024 donald duck is the president of the united states.\n",
      "P=0.00022 donald ross is the president of the united states.\n",
      "P=0.00020 donald johnson is the president of the united states.\n",
      "P=0.00018 donald wilson is the president of the united states.\n"
     ]
    }
   ],
   "source": [
    "mlm_model = pipeline('fill-mask', model=\"bert-base-uncased\")\n",
    "MASK = mlm_model.tokenizer.mask_token\n",
    "\n",
    "for hypo in mlm_model(f\"Donald {MASK} is the president of the united states.\"):\n",
    "  print(f\"P={hypo['score']:.5f}\", hypo['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9NxeG1Y5pwX1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9056478142738342,\n",
       "  'token': 2008,\n",
       "  'token_str': 'that',\n",
       "  'sequence': 'the soviet union was founded in that year.'},\n",
       " {'score': 0.08824586868286133,\n",
       "  'token': 2023,\n",
       "  'token_str': 'this',\n",
       "  'sequence': 'the soviet union was founded in this year.'},\n",
       " {'score': 0.001775345765054226,\n",
       "  'token': 1996,\n",
       "  'token_str': 'the',\n",
       "  'sequence': 'the soviet union was founded in the year.'},\n",
       " {'score': 0.0009746896103024483,\n",
       "  'token': 2168,\n",
       "  'token_str': 'same',\n",
       "  'sequence': 'the soviet union was founded in same year.'},\n",
       " {'score': 0.0008080514962784946,\n",
       "  'token': 2028,\n",
       "  'token_str': 'one',\n",
       "  'sequence': 'the soviet union was founded in one year.'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your turn: use bert to recall what year was the Soviet Union founded in\n",
    "mlm_model(f\"The Soviet Union was founded in {MASK} year.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.03702247887849808,\n",
       "  'token': 2462,\n",
       "  'token_str': 'ii',\n",
       "  'sequence': 'the soviet union was founded in year ii.'},\n",
       " {'score': 0.030535712838172913,\n",
       "  'token': 3386,\n",
       "  'token_str': '1945',\n",
       "  'sequence': 'the soviet union was founded in year 1945.'},\n",
       " {'score': 0.027637433260679245,\n",
       "  'token': 4585,\n",
       "  'token_str': '1917',\n",
       "  'sequence': 'the soviet union was founded in year 1917.'},\n",
       " {'score': 0.023243509232997894,\n",
       "  'token': 4271,\n",
       "  'token_str': '1918',\n",
       "  'sequence': 'the soviet union was founded in year 1918.'},\n",
       " {'score': 0.01753777638077736,\n",
       "  'token': 4085,\n",
       "  'token_str': '1949',\n",
       "  'sequence': 'the soviet union was founded in year 1949.'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your turn: use bert to recall what year was the Soviet Union founded in\n",
    "mlm_model(f\"The Soviet Union was founded in year {MASK}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5282406210899353,\n",
       "  'token': 2008,\n",
       "  'token_str': 'that',\n",
       "  'sequence': 'that year was the soviet union founded in'},\n",
       " {'score': 0.3739815056324005,\n",
       "  'token': 2023,\n",
       "  'token_str': 'this',\n",
       "  'sequence': 'this year was the soviet union founded in'},\n",
       " {'score': 0.03149905428290367,\n",
       "  'token': 2279,\n",
       "  'token_str': 'next',\n",
       "  'sequence': 'next year was the soviet union founded in'},\n",
       " {'score': 0.026913192123174667,\n",
       "  'token': 1996,\n",
       "  'token_str': 'the',\n",
       "  'sequence': 'the year was the soviet union founded in'},\n",
       " {'score': 0.010693252086639404,\n",
       "  'token': 2168,\n",
       "  'token_str': 'same',\n",
       "  'sequence': 'same year was the soviet union founded in'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_model(f\"{MASK} year was the Soviet Union founded in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJxRFzCSq903"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Huggingface offers hundreds of pre-trained models that specialize on different tasks. You can quickly find the model you need using [this list](https://huggingface.co/models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HRux8Qp2hkXr",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████████████████████████| 829/829 [00:00<00:00, 1.24MB/s]\u001b[A\n",
      "\n",
      "Downloading model.safetensors:   0%|                                                                        | 0.00/433M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading model.safetensors:   2%|█▌                                                             | 10.5M/433M [00:01<01:03, 6.65MB/s]\u001b[A\n",
      "Downloading model.safetensors:   5%|███                                                            | 21.0M/433M [00:03<01:01, 6.72MB/s]\u001b[A\n",
      "Downloading model.safetensors:   7%|████▌                                                          | 31.5M/433M [00:04<01:00, 6.64MB/s]\u001b[A\n",
      "Downloading model.safetensors:  10%|██████                                                         | 41.9M/433M [00:06<00:59, 6.59MB/s]\u001b[A\n",
      "Downloading model.safetensors:  12%|███████▌                                                       | 52.4M/433M [00:07<00:58, 6.53MB/s]\u001b[A\n",
      "Downloading model.safetensors:  15%|█████████▏                                                     | 62.9M/433M [00:09<00:56, 6.52MB/s]\u001b[A\n",
      "Downloading model.safetensors:  17%|██████████▋                                                    | 73.4M/433M [00:11<00:56, 6.32MB/s]\u001b[A\n",
      "Downloading model.safetensors:  19%|████████████▏                                                  | 83.9M/433M [00:13<00:55, 6.32MB/s]\u001b[A\n",
      "Downloading model.safetensors:  22%|█████████████▋                                                 | 94.4M/433M [00:14<00:55, 6.13MB/s]\u001b[A\n",
      "Downloading model.safetensors:  24%|███████████████▍                                                | 105M/433M [00:16<00:52, 6.23MB/s]\u001b[A\n",
      "Downloading model.safetensors:  27%|█████████████████                                               | 115M/433M [00:18<00:50, 6.35MB/s]\u001b[A\n",
      "Downloading model.safetensors:  29%|██████████████████▌                                             | 126M/433M [00:19<00:47, 6.42MB/s]\u001b[A\n",
      "Downloading model.safetensors:  31%|████████████████████▏                                           | 136M/433M [00:21<00:47, 6.20MB/s]\u001b[A\n",
      "Downloading model.safetensors:  34%|█████████████████████▋                                          | 147M/433M [00:23<00:45, 6.25MB/s]\u001b[A\n",
      "Downloading model.safetensors:  36%|███████████████████████▏                                        | 157M/433M [00:25<00:48, 5.68MB/s]\u001b[A\n",
      "Downloading model.safetensors:  39%|████████████████████████▊                                       | 168M/433M [00:27<00:46, 5.72MB/s]\u001b[A\n",
      "Downloading model.safetensors:  41%|██████████████████████████▎                                     | 178M/433M [00:28<00:43, 5.84MB/s]\u001b[A\n",
      "Downloading model.safetensors:  44%|███████████████████████████▉                                    | 189M/433M [00:30<00:42, 5.77MB/s]\u001b[A\n",
      "Downloading model.safetensors:  46%|█████████████████████████████▍                                  | 199M/433M [00:32<00:39, 5.90MB/s]\u001b[A\n",
      "Downloading model.safetensors:  48%|██████████████████████████████▉                                 | 210M/433M [00:34<00:37, 5.96MB/s]\u001b[A\n",
      "Downloading model.safetensors:  51%|████████████████████████████████▌                               | 220M/433M [00:35<00:36, 5.91MB/s]\u001b[A\n",
      "Downloading model.safetensors:  53%|██████████████████████████████████                              | 231M/433M [00:37<00:33, 6.03MB/s]\u001b[A\n",
      "Downloading model.safetensors:  56%|███████████████████████████████████▌                            | 241M/433M [00:39<00:31, 6.17MB/s]\u001b[A\n",
      "Downloading model.safetensors:  58%|█████████████████████████████████████▏                          | 252M/433M [00:40<00:29, 6.10MB/s]\u001b[A\n",
      "Downloading model.safetensors:  61%|██████████████████████████████████████▋                         | 262M/433M [00:42<00:27, 6.22MB/s]\u001b[A\n",
      "Downloading model.safetensors:  63%|████████████████████████████████████████▎                       | 273M/433M [00:44<00:25, 6.34MB/s]\u001b[A\n",
      "Downloading model.safetensors:  65%|█████████████████████████████████████████▊                      | 283M/433M [00:46<00:24, 6.01MB/s]\u001b[A\n",
      "Downloading model.safetensors:  68%|███████████████████████████████████████████▎                    | 294M/433M [00:47<00:23, 6.05MB/s]\u001b[A\n",
      "Downloading model.safetensors:  70%|████████████████████████████████████████████▉                   | 304M/433M [00:49<00:21, 6.10MB/s]\u001b[A\n",
      "Downloading model.safetensors:  73%|██████████████████████████████████████████████▍                 | 315M/433M [00:51<00:21, 5.47MB/s]\u001b[A\n",
      "Downloading model.safetensors:  75%|████████████████████████████████████████████████                | 325M/433M [00:54<00:21, 4.94MB/s]\u001b[A\n",
      "Downloading model.safetensors:  77%|█████████████████████████████████████████████████▌              | 336M/433M [00:56<00:18, 5.24MB/s]\u001b[A\n",
      "Downloading model.safetensors:  80%|███████████████████████████████████████████████████             | 346M/433M [00:58<00:16, 5.37MB/s]\u001b[A\n",
      "Downloading model.safetensors:  82%|████████████████████████████████████████████████████▋           | 357M/433M [00:59<00:13, 5.64MB/s]\u001b[A\n",
      "Downloading model.safetensors:  85%|██████████████████████████████████████████████████████▏         | 367M/433M [01:01<00:11, 5.70MB/s]\u001b[A\n",
      "Downloading model.safetensors:  87%|███████████████████████████████████████████████████████▊        | 377M/433M [01:03<00:09, 5.91MB/s]\u001b[A\n",
      "Downloading model.safetensors:  90%|█████████████████████████████████████████████████████████▎      | 388M/433M [01:04<00:07, 6.03MB/s]\u001b[A\n",
      "Downloading model.safetensors:  92%|██████████████████████████████████████████████████████████▊     | 398M/433M [01:06<00:05, 6.17MB/s]\u001b[A\n",
      "Downloading model.safetensors:  94%|████████████████████████████████████████████████████████████▍   | 409M/433M [01:08<00:04, 6.08MB/s]\u001b[A\n",
      "Downloading model.safetensors:  97%|█████████████████████████████████████████████████████████████▉  | 419M/433M [01:09<00:02, 6.15MB/s]\u001b[A\n",
      "Downloading model.safetensors:  99%|███████████████████████████████████████████████████████████████▌| 430M/433M [01:12<00:00, 5.51MB/s]\u001b[A\n",
      "Downloading model.safetensors: 100%|████████████████████████████████████████████████████████████████| 433M/433M [01:12<00:00, 5.94MB/s]\u001b[A\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "\n",
      "Downloading (…)okenizer_config.json: 100%|███████████████████████████████████████████████████████████| 59.0/59.0 [00:00<00:00, 128kB/s]\u001b[A\n",
      "\n",
      "Downloading (…)solve/main/vocab.txt:   0%|                                                                  | 0.00/213k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)solve/main/vocab.txt: 100%|███████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 890kB/s]\u001b[A\n",
      "\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████████████████████████████████████████████████████| 2.00/2.00 [00:00<00:00, 6.36kB/s]\u001b[A\n",
      "\n",
      "Downloading (…)cial_tokens_map.json: 100%|█████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 156kB/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Almost two-thirds of the 1.5 million people who viewed this liveblog had Googled to discover\n",
    " the latest on the Rosetta mission. They were treated to this detailed account by the Guardian’s science editor,\n",
    " Ian Sample, and astronomy writer Stuart Clark of the moment scientists landed a robotic spacecraft on a comet \n",
    " for the first time in history, and the delirious reaction it provoked at their headquarters in Germany.\n",
    "  “We are there. We are sitting on the surface. Philae is talking to us,” said one scientist.\n",
    "\"\"\"\n",
    "\n",
    "# Task: create a pipeline for named entity recognition, use task name 'ner' and search for the right model in the list\n",
    "ner_model = pipeline(\"token-classification\", model=\"dslim/bert-base-NER\")\n",
    "\n",
    "named_entities = ner_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "hf57MRzSiSON"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_entity: {'Rose': 'B-LOC', '##tta': 'I-LOC', 'Guardian': 'B-ORG', 'Ian': 'B-PER', 'Sam': 'I-PER', '##ple': 'I-PER', 'Stuart': 'B-PER', 'Clark': 'I-PER', 'Germany': 'B-LOC', 'Phil': 'B-PER', '##ae': 'I-PER'}\n",
      "\n",
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "# print('OUTPUT:', named_entities)\n",
    "word_to_entity = {item['word']: item['entity'] for item in named_entities}\n",
    "print('word_to_entity:',word_to_entity)\n",
    "assert 'org' in word_to_entity.get('Guardian').lower() and 'per' in word_to_entity.get('Stuart').lower()\n",
    "print()\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULMownz6sP9n"
   },
   "source": [
    "### The building blocks of a pipeline\n",
    "\n",
    "Huggingface also allows you to access its pipelines on a lower level. There are two main abstractions for you:\n",
    "* `Tokenizer` - converts from strings to token ids and back\n",
    "* `Model` - a pytorch `nn.Module` with pre-trained weights\n",
    "\n",
    "You can use such models as part of your regular pytorch code: insert is as a layer in your model, apply it to a batch of data, backpropagate, optimize, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "KMJbV0QVsO0Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "\n",
    "# model_name = 'bert-base-uncased'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ZgSPHKPRxG6U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tf.Tensor(\n",
      "[[ 101 5355 1010 1045 2572 2115 2269 1012  102    0    0    0    0    0\n",
      "     0]\n",
      " [ 101 2166 2003 2054 6433 2043 2017 1005 2128 5697 2437 2060 3488 1012\n",
      "   102]], shape=(2, 15), dtype=int32)\n",
      "token_type_ids tf.Tensor(\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(2, 15), dtype=int32)\n",
      "attention_mask tf.Tensor(\n",
      "[[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]], shape=(2, 15), dtype=int32)\n",
      "Detokenized:\n",
      "[CLS] luke, i am your father. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] life is what happens when you're busy making other plans. [SEP]\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    \"Luke, I am your father.\",\n",
    "    \"Life is what happens when you're busy making other plans.\",\n",
    "    ]\n",
    "\n",
    "# tokenize a batch of inputs. \"pt\" means [p]y[t]orch tensors\n",
    "tokens_info = tokenizer(lines, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "for key in tokens_info:\n",
    "    print(key, tokens_info[key])\n",
    "\n",
    "print(\"Detokenized:\")\n",
    "for i in range(2):\n",
    "    print(tokenizer.decode(tokens_info['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[ 101, 5355, 1010, 1045, 2572, 2115, 2269, 1012,  102,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [ 101, 2166, 2003, 2054, 6433, 2043, 2017, 1005, 2128, 5697, 2437,\n",
       "        2060, 3488, 1012,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.88541967, -0.47218543, -0.93924   , ..., -0.8081023 ,\n",
       "        -0.69547755,  0.8747951 ],\n",
       "       [-0.92971474, -0.51610476, -0.9334278 , ..., -0.9017478 ,\n",
       "        -0.7492124 ,  0.92007506]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tokens_info)['pooler_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "MJkbHxERyfL4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.88541967, -0.47218543, -0.93924   , ..., -0.8081023 ,\n",
       "        -0.69547755,  0.8747951 ],\n",
       "       [-0.92971474, -0.51610476, -0.9334278 , ..., -0.9017478 ,\n",
       "        -0.7492124 ,  0.92007506]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can now apply the model to get embeddings\n",
    "# with torch.no_grad():\n",
    "#     out = model(**tokens_info)\n",
    "\n",
    "# print(out['pooler_output'])\n",
    "\n",
    "model(**tokens_info)['pooler_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHEC6o7uAfgQ"
   },
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "__Bonus demo:__ transformer language models. \n",
    "\n",
    "`/* No points awarded for this task, but its really cool, we promise :) */`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading (…)olve/main/vocab.json:   0%|                                                                 | 0.00/1.04M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)olve/main/vocab.json: 100%|████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 3.01MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)olve/main/merges.txt:   0%|                                                                  | 0.00/456k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 2.79MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████████████████████████| 665/665 [00:00<00:00, 1.46MB/s]\u001b[A\n",
      "\n",
      "Downloading model.safetensors:   0%|                                                                        | 0.00/548M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading model.safetensors:   2%|█▏                                                             | 10.5M/548M [00:02<02:31, 3.55MB/s]\u001b[A\n",
      "Downloading model.safetensors:   4%|██▍                                                            | 21.0M/548M [00:04<01:51, 4.72MB/s]\u001b[A\n",
      "Downloading model.safetensors:   6%|███▌                                                           | 31.5M/548M [00:06<01:42, 5.02MB/s]\u001b[A\n",
      "Downloading model.safetensors:   8%|████▊                                                          | 41.9M/548M [00:08<01:44, 4.82MB/s]\u001b[A\n",
      "Downloading model.safetensors:  10%|██████                                                         | 52.4M/548M [00:11<01:46, 4.67MB/s]\u001b[A\n",
      "Downloading model.safetensors:  11%|███████▏                                                       | 62.9M/548M [00:13<01:45, 4.59MB/s]\u001b[A\n",
      "Downloading model.safetensors:  13%|████████▍                                                      | 73.4M/548M [00:15<01:35, 4.97MB/s]\u001b[A\n",
      "Downloading model.safetensors:  15%|█████████▋                                                     | 83.9M/548M [00:17<01:28, 5.24MB/s]\u001b[A\n",
      "Downloading model.safetensors:  17%|██████████▊                                                    | 94.4M/548M [00:19<01:24, 5.34MB/s]\u001b[A\n",
      "Downloading model.safetensors:  19%|████████████▏                                                   | 105M/548M [00:21<01:23, 5.29MB/s]\u001b[A\n",
      "Downloading model.safetensors:  21%|█████████████▍                                                  | 115M/548M [00:22<01:19, 5.43MB/s]\u001b[A\n",
      "Downloading model.safetensors:  23%|██████████████▋                                                 | 126M/548M [00:24<01:14, 5.67MB/s]\u001b[A\n",
      "Downloading model.safetensors:  25%|███████████████▉                                                | 136M/548M [00:26<01:13, 5.64MB/s]\u001b[A\n",
      "Downloading model.safetensors:  27%|█████████████████▏                                              | 147M/548M [00:28<01:15, 5.29MB/s]\u001b[A\n",
      "Downloading model.safetensors:  29%|██████████████████▎                                             | 157M/548M [00:30<01:12, 5.42MB/s]\u001b[A\n",
      "Downloading model.safetensors:  31%|███████████████████▌                                            | 168M/548M [00:32<01:07, 5.65MB/s]\u001b[A\n",
      "Downloading model.safetensors:  33%|████████████████████▊                                           | 178M/548M [00:33<01:03, 5.81MB/s]\u001b[A\n",
      "Downloading model.safetensors:  34%|██████████████████████                                          | 189M/548M [00:35<01:01, 5.84MB/s]\u001b[A\n",
      "Downloading model.safetensors:  36%|███████████████████████▎                                        | 199M/548M [00:37<00:59, 5.86MB/s]\u001b[A\n",
      "Downloading model.safetensors:  38%|████████████████████████▍                                       | 210M/548M [00:39<00:59, 5.73MB/s]\u001b[A\n",
      "Downloading model.safetensors:  40%|█████████████████████████▋                                      | 220M/548M [00:40<00:55, 5.90MB/s]\u001b[A\n",
      "Downloading model.safetensors:  42%|██████████████████████████▉                                     | 231M/548M [00:42<00:53, 5.96MB/s]\u001b[A\n",
      "Downloading model.safetensors:  44%|████████████████████████████▏                                   | 241M/548M [00:44<00:55, 5.49MB/s]\u001b[A\n",
      "Downloading model.safetensors:  46%|█████████████████████████████▍                                  | 252M/548M [00:46<00:53, 5.59MB/s]\u001b[A\n",
      "Downloading model.safetensors:  48%|██████████████████████████████▌                                 | 262M/548M [00:48<00:49, 5.82MB/s]\u001b[A\n",
      "Downloading model.safetensors:  50%|███████████████████████████████▊                                | 273M/548M [00:50<00:47, 5.82MB/s]\u001b[A\n",
      "Downloading model.safetensors:  52%|█████████████████████████████████                               | 283M/548M [00:54<01:07, 3.91MB/s]\u001b[A\n",
      "Downloading model.safetensors:  54%|██████████████████████████████████▎                             | 294M/548M [00:57<01:04, 3.92MB/s]\u001b[A\n",
      "Downloading model.safetensors:  55%|███████████████████████████████████▌                            | 304M/548M [00:59<00:58, 4.21MB/s]\u001b[A\n",
      "Downloading model.safetensors:  57%|████████████████████████████████████▋                           | 315M/548M [01:01<00:50, 4.66MB/s]\u001b[A\n",
      "Downloading model.safetensors:  59%|█████████████████████████████████████▉                          | 325M/548M [01:03<00:45, 4.94MB/s]\u001b[A\n",
      "Downloading model.safetensors:  61%|███████████████████████████████████████▏                        | 336M/548M [01:04<00:39, 5.38MB/s]\u001b[A\n",
      "Downloading model.safetensors:  63%|████████████████████████████████████████▍                       | 346M/548M [01:06<00:36, 5.55MB/s]\u001b[A\n",
      "Downloading model.safetensors:  65%|█████████████████████████████████████████▋                      | 357M/548M [01:08<00:33, 5.78MB/s]\u001b[A\n",
      "Downloading model.safetensors:  67%|██████████████████████████████████████████▊                     | 367M/548M [01:09<00:31, 5.80MB/s]\u001b[A\n",
      "Downloading model.safetensors:  69%|████████████████████████████████████████████                    | 377M/548M [01:11<00:29, 5.83MB/s]\u001b[A\n",
      "Downloading model.safetensors:  71%|█████████████████████████████████████████████▎                  | 388M/548M [01:13<00:27, 5.80MB/s]\u001b[A\n",
      "Downloading model.safetensors:  73%|██████████████████████████████████████████████▌                 | 398M/548M [01:15<00:27, 5.42MB/s]\u001b[A\n",
      "Downloading model.safetensors:  75%|███████████████████████████████████████████████▊                | 409M/548M [01:17<00:24, 5.76MB/s]\u001b[A\n",
      "Downloading model.safetensors:  77%|████████████████████████████████████████████████▉               | 419M/548M [01:18<00:21, 6.03MB/s]\u001b[A\n",
      "Downloading model.safetensors:  78%|██████████████████████████████████████████████████▏             | 430M/548M [01:21<00:21, 5.59MB/s]\u001b[A\n",
      "Downloading model.safetensors:  80%|███████████████████████████████████████████████████▍            | 440M/548M [01:23<00:22, 4.87MB/s]\u001b[A\n",
      "Downloading model.safetensors:  82%|████████████████████████████████████████████████████▋           | 451M/548M [01:26<00:21, 4.45MB/s]\u001b[A\n",
      "Downloading model.safetensors:  84%|█████████████████████████████████████████████████████▊          | 461M/548M [01:29<00:19, 4.44MB/s]\u001b[A\n",
      "Downloading model.safetensors:  86%|███████████████████████████████████████████████████████         | 472M/548M [01:31<00:16, 4.60MB/s]\u001b[A\n",
      "Downloading model.safetensors:  88%|████████████████████████████████████████████████████████▎       | 482M/548M [01:32<00:13, 4.98MB/s]\u001b[A\n",
      "Downloading model.safetensors:  90%|█████████████████████████████████████████████████████████▌      | 493M/548M [01:34<00:10, 5.35MB/s]\u001b[A\n",
      "Downloading model.safetensors:  92%|██████████████████████████████████████████████████████████▊     | 503M/548M [01:36<00:08, 5.53MB/s]\u001b[A\n",
      "Downloading model.safetensors:  94%|███████████████████████████████████████████████████████████▉    | 514M/548M [01:37<00:06, 5.64MB/s]\u001b[A\n",
      "Downloading model.safetensors:  96%|█████████████████████████████████████████████████████████████▏  | 524M/548M [01:40<00:04, 5.43MB/s]\u001b[A\n",
      "Downloading model.safetensors:  98%|██████████████████████████████████████████████████████████████▍ | 535M/548M [01:42<00:02, 5.23MB/s]\u001b[A\n",
      "Downloading model.safetensors:  99%|███████████████████████████████████████████████████████████████▋| 545M/548M [01:44<00:00, 5.11MB/s]\u001b[A\n",
      "Downloading model.safetensors: 100%|████████████████████████████████████████████████████████████████| 548M/548M [01:45<00:00, 5.21MB/s]\u001b[A\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "(\n",
      "which\n",
      " can\n",
      " also\n",
      " be\n",
      " used\n",
      " in\n",
      " \n",
      "a\n",
      " simple\n",
      " way\n",
      " as\n",
      " \n",
      "an\n",
      " explanatory\n",
      " principle\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "'ll\n",
      " give\n",
      " it\n",
      " some\n",
      " attention\n",
      ".\n",
      " First\n",
      " I\n",
      " want\n",
      " to\n",
      " talk\n",
      " about\n",
      " an\n",
      " important\n",
      " problem\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "How\n",
      " are\n",
      " you\n",
      " doing\n",
      " it\n",
      "?\n",
      " \n",
      " (\n",
      "If\n",
      " you\n",
      "'re\n",
      " reading\n",
      " this\n",
      " in\n",
      " the\n",
      " \n",
      "main\n",
      " blog\n",
      " \n",
      ",\n",
      " that\n",
      "\n",
      " might\n",
      " sound\n",
      " \n",
      "b\n",
      "ored\n",
      " and\n",
      " \n",
      "un\n",
      "educated\n",
      ")\n",
      " Why\n",
      " don\n",
      "'t\n",
      " you\n",
      " use\n",
      " it\n",
      ",\n",
      " right\n",
      "?\n",
      " Because\n",
      "\n",
      " it\n",
      " can\n",
      " really\n",
      " be\n",
      " very\n",
      " useful\n",
      " if\n",
      " you\n",
      " are\n",
      " thinking\n",
      " and\n",
      " doing\n",
      " something\n",
      " that\n",
      "\n",
      " will\n",
      " benefit\n",
      " you\n",
      ",\n",
      " even\n",
      " though\n",
      " it\n",
      " is\n",
      " only\n",
      " part\n",
      " of\n",
      " what\n",
      "'s\n",
      " possible\n",
      " \n",
      " if\n",
      " it\n",
      "\n",
      " were\n",
      " not\n",
      " so\n",
      " much\n",
      " your\n",
      " task\n",
      ".\n",
      " It\n",
      " doesn\n",
      "'t\n",
      " make\n",
      " it\n",
      " easy\n",
      " and\n",
      " doesn\n",
      "'t\n",
      " allow\n",
      "\n",
      " for\n",
      " \n",
      " an\n",
      " \n",
      " \n",
      "eas\n",
      "ier\n",
      " task\n",
      ".\n",
      " \n",
      " So\n",
      " what\n",
      "?\n",
      " The\n",
      " problem\n",
      " of\n",
      " making\n",
      " things\n",
      " work\n",
      " \n",
      " \n",
      "can\n",
      "\n",
      " help\n",
      " people\n",
      " with\n",
      " this\n",
      " \n",
      " \n",
      "problem\n",
      " and\n",
      " the\n",
      " more\n",
      " \n",
      " we\n",
      " know\n",
      " about\n",
      " \n",
      " what\n",
      "'s\n",
      " possible\n",
      "\n",
      " \n",
      " then\n",
      " \n",
      " we\n",
      " \n",
      " know\n",
      " it\n",
      " can\n",
      " be\n",
      " much\n",
      " better\n",
      ",\n",
      " too\n",
      ".\n",
      " I\n",
      "'ve\n",
      " found\n",
      " many\n",
      " people\n",
      " with\n",
      "\n",
      " such\n",
      " issues\n",
      " are\n",
      " simply\n",
      " using\n",
      " \"\n",
      "just\n",
      " say\n",
      " no\n",
      "\"\n",
      " or\n",
      " using\n",
      " a\n",
      " language\n",
      " \n",
      "that\n",
      " they\n",
      "\n",
      " \n",
      " understand\n",
      " but\n",
      " can\n",
      "'t\n",
      " really\n",
      " explain\n",
      " to\n",
      " a\n",
      " computer\n",
      " (\n",
      "or\n",
      " anyone\n",
      ",\n",
      " I\n",
      " can\n",
      "'t\n",
      "\n",
      " imagine\n",
      " a\n",
      " language\n",
      " like\n",
      " Java\n",
      " that\n",
      " isn\n",
      "'t\n",
      " a\n",
      " little\n",
      " \n",
      " \n",
      "dis\n",
      "g\n",
      "ust\n",
      " in\n",
      " its\n",
      " approach\n",
      "\n",
      " and\n",
      " the\n",
      " kind\n",
      " of\n",
      " problem\n",
      " it\n",
      " poses\n",
      ".\n",
      " )\n",
      " I\n",
      "'ve\n",
      " tried\n",
      " it\n",
      " myself\n",
      ",\n",
      " using\n",
      " a\n",
      " simple\n",
      "\n",
      " \n",
      " \n",
      "word\n",
      " \n",
      "(\n",
      "as\n",
      " well\n",
      " as\n",
      " \n",
      " a\n",
      " word\n",
      " of\n",
      " grammar\n",
      " \n",
      ")\n",
      " which\n",
      " is\n",
      " not\n",
      " only\n",
      " \n",
      " easier\n",
      " but\n",
      " it\n",
      "\n",
      " makes\n",
      " things\n",
      " work\n",
      ".\n",
      " \n",
      " In\n",
      " order\n",
      " to\n",
      " make\n",
      " something\n",
      " work\n",
      " we\n",
      " just\n",
      " \n",
      " want\n",
      " the\n",
      "\n",
      " words\n",
      " of\n",
      " the\n",
      " \n",
      " \n",
      " \n",
      "text\n",
      ",\n",
      " but\n",
      " then\n",
      " there\n",
      " is\n",
      " no\n",
      " way\n",
      " to\n",
      " find\n",
      " it\n",
      " if\n",
      " we\n",
      " \n",
      " do\n",
      " a\n",
      " \n",
      "\n",
      " google\n",
      " \n",
      " \n",
      "search\n",
      " \n",
      " and\n",
      " we\n",
      " have\n",
      " no\n",
      " idea\n",
      " of\n",
      " how\n",
      " the\n",
      " words\n",
      " \n",
      " work\n",
      ",\n",
      " but\n",
      " we\n",
      " don\n",
      "\n",
      "'t\n",
      " have\n",
      " to\n",
      " know\n",
      " it\n",
      " at\n",
      " all\n",
      "!\n",
      " We\n",
      " simply\n",
      " use\n",
      " a\n",
      " few\n",
      " of\n",
      " the\n",
      " basic\n",
      " phrases\n",
      " of\n",
      "\n",
      " this\n",
      " simple\n",
      " English\n",
      " word\n",
      " \n",
      " \n",
      "that\n",
      " make\n",
      " a\n",
      " person\n",
      " feel\n",
      " \n",
      " like\n",
      " a\n",
      " good\n",
      " man\n",
      " \n",
      " \n",
      "in\n",
      "\n",
      " some\n",
      " way\n",
      " or\n",
      " another\n",
      ".\n",
      " It\n",
      " makes\n",
      " sense\n",
      " then\n",
      " and\n",
      " it\n",
      "'s\n",
      " so\n",
      " helpful\n",
      " when\n",
      " someone\n",
      "\n",
      " does\n",
      " it\n",
      "!\n",
      " I\n",
      " know\n",
      " that\n",
      " you\n",
      "'ve\n",
      " heard\n",
      " of\n",
      " some\n",
      " examples\n",
      " of\n",
      " how\n",
      " this\n",
      " is\n",
      " used\n",
      "\n",
      ",\n",
      " such\n",
      " as\n",
      " a\n",
      " friend\n",
      " \n",
      " telling\n",
      " a\n",
      " \n",
      " stranger\n",
      " she\n",
      " didn\n",
      "'t\n",
      " understand\n",
      " how\n",
      " she\n",
      " was\n",
      "\n",
      " talking\n",
      ".\n",
      " In\n",
      " some\n",
      " instances\n",
      " of\n",
      " such\n",
      " examples\n",
      ",\n",
      " people\n",
      " are\n",
      " using\n",
      " these\n",
      " examples\n",
      "\n",
      ",\n",
      " as\n",
      " they\n",
      "'re\n",
      " used\n",
      " with\n",
      " a\n",
      " simple\n",
      " phrase\n",
      " (\n",
      "as\n",
      " \n",
      "a\n",
      " simple\n",
      " \"\n",
      "what\n",
      " did\n",
      " I\n",
      " just\n",
      " say\n",
      "\n",
      "\",\n",
      " for\n",
      " example\n",
      ")\n",
      " to\n",
      " describe\n",
      " their\n",
      " problems\n",
      " and\n",
      " then\n",
      " later\n",
      " they\n",
      " have\n",
      " it\n",
      "\n",
      " all\n",
      " \n",
      " over\n",
      " again\n",
      " \n",
      " in\n",
      " their\n",
      " everyday\n",
      " life\n",
      " \n",
      " because\n",
      " \n",
      " they\n",
      " are\n",
      " able\n",
      " to\n",
      " think\n",
      "\n",
      " about\n",
      " and\n",
      " do\n",
      " some\n",
      " very\n",
      " useful\n",
      " things\n",
      " and\n",
      " so\n",
      " on\n",
      ".\n",
      " So\n",
      " how\n",
      " does\n",
      " one\n",
      " make\n",
      " \n",
      "\n",
      " the\n",
      " F\n",
      "erm\n",
      "i\n",
      " paradox\n",
      " work\n",
      "?\n",
      " I\n",
      " will\n",
      " talk\n",
      " about\n",
      " what\n",
      "'s\n",
      " a\n",
      " very\n",
      " simple\n",
      " language\n",
      "\n",
      " \n",
      "that\n",
      " is\n",
      " very\n",
      " \n",
      " powerful\n",
      ",\n",
      " especially\n",
      " in\n",
      " English\n",
      " (\n",
      "I\n",
      " can\n",
      " imagine\n",
      " some\n",
      " languages\n",
      "\n",
      " in\n",
      " English\n",
      " which\n",
      " are\n",
      " just\n",
      " like\n",
      " F\n",
      "erm\n",
      "i\n",
      "),\n",
      " that\n",
      " uses\n",
      " \n",
      " very\n",
      " simple\n",
      ",\n",
      " non\n",
      "-\n",
      "technical\n",
      "\n",
      " \n",
      " syntax\n",
      " (\n",
      "like\n",
      " a\n",
      " \"\n",
      "n\n",
      "umeric\n",
      "-\n",
      "com\n",
      "plet\n",
      "ive\n",
      " \n",
      " \n",
      "which\n",
      " is\n",
      " just\n",
      " as\n",
      " simple\n",
      ",\n",
      " yet\n",
      " still\n",
      "\n",
      " is\n",
      " \n",
      " \n",
      "easy\n",
      " \n",
      " for\n",
      " most\n",
      " \n",
      " \n",
      "people\n",
      " to\n",
      " comprehend\n",
      "\").\n",
      " I\n",
      "'ll\n",
      " start\n",
      " by\n",
      " giving\n",
      " you\n",
      " an\n",
      "\n",
      " idea\n",
      " on\n",
      " why\n",
      " the\n",
      " simple\n",
      " F\n",
      "erm\n",
      "i\n",
      " \n",
      " \n",
      "does\n",
      " it\n",
      " (\n",
      "or\n",
      " what\n",
      " we\n",
      " should\n",
      " call\n",
      " it\n",
      "):\n",
      " The\n",
      "\n",
      " F\n",
      "erm\n",
      "i\n",
      " Paradox\n",
      ":\n",
      "\n",
      "\n",
      "So\n",
      " what\n",
      " if\n",
      " I\n",
      " said\n",
      " this\n",
      ":\n",
      " You\n",
      " could\n",
      " say\n",
      " you\n",
      " are\n",
      " using\n",
      " this\n",
      "\n",
      " language\n",
      ",\n",
      " which\n",
      " I\n",
      " will\n",
      " describe\n",
      " in\n",
      " this\n",
      " paragraph\n",
      " as\n",
      " a\n",
      " language\n",
      ",\n",
      " and\n",
      " the\n",
      "\n",
      " people\n",
      " reading\n",
      " you\n",
      " can\n",
      " see\n",
      " how\n",
      " much\n",
      " the\n",
      " \n",
      " people\n",
      " \n",
      " use\n",
      " the\n",
      " F\n",
      "erm\n",
      "i\n",
      " paradox\n",
      "\n",
      ".\n",
      " But\n",
      " the\n",
      " F\n",
      "erm\n",
      "i\n",
      " paradox\n",
      " has\n",
      " the\n",
      " \n",
      " opposite\n",
      " effect\n",
      " in\n",
      " terms\n",
      " of\n",
      " understanding\n",
      "\n",
      " people\n",
      "'s\n",
      " needs\n",
      " for\n",
      " their\n",
      " everyday\n",
      " needs\n",
      " (\n",
      "for\n",
      " instance\n",
      " a\n",
      " friend\n",
      " or\n",
      " \n",
      " mother\n",
      "\n",
      " who\n",
      " doesn\n",
      "'t\n",
      " know\n",
      " the\n",
      " words\n",
      " and\n",
      " doesn\n",
      "'t\n",
      " want\n",
      " them\n",
      " used\n",
      ").\n",
      " \n",
      " I\n",
      " have\n",
      " found\n",
      "\n",
      " that\n",
      " if\n",
      " a\n",
      " \n",
      " friend\n",
      " uses\n",
      " F\n",
      "erm\n",
      "i\n",
      " and\n",
      " \n",
      " a\n",
      " woman\n",
      " does\n",
      " not\n",
      ".\n",
      " But\n",
      " in\n",
      " my\n",
      " view\n",
      " if\n",
      "\n",
      " \n",
      " she\n",
      " has\n",
      " been\n",
      " reading\n",
      " F\n",
      "erm\n",
      "i\n",
      " she\n",
      " \n",
      " uses\n",
      " something\n",
      " similar\n",
      " \n",
      " and\n",
      " so\n",
      " on\n",
      " (\n",
      "like\n",
      "\n",
      " a\n",
      " language\n",
      " with\n",
      " no\n",
      " language\n",
      " and\n",
      " just\n",
      " the\n",
      " \"\n",
      "I\n",
      " \n",
      " used\n",
      " a\n",
      " different\n",
      " \n",
      " \n",
      "phrase\n",
      "\n",
      " \n",
      " than\n",
      " you\n",
      ",\n",
      " which\n",
      " was\n",
      " not\n",
      " so\n",
      " easy\n",
      " \n",
      " \n",
      "to\n",
      " comprehend\n",
      " and\n",
      " didn\n",
      "'t\n",
      " let\n",
      " her\n",
      " see\n",
      "\n",
      " that\n",
      " the\n",
      " word\n",
      " '\n",
      "n\n",
      "aturally\n",
      " did\n",
      "'\n",
      " was\n",
      " actually\n",
      " a\n",
      " word\n",
      " for\n",
      " \"\n",
      "not\n",
      " at\n",
      " all\n",
      " \n",
      " a\n",
      " good\n",
      "\n",
      " \n",
      " speaker\n",
      " or\n",
      " an\n",
      " \n",
      " \n",
      "ad\n",
      "vis\n",
      "er\n",
      " to\n",
      " an\n",
      " \n",
      " \n",
      " \n",
      "word\n",
      " \"\n",
      " ).\n",
      " If\n",
      " \n",
      " a\n",
      " F\n",
      "erm\n",
      "i\n",
      "-\n",
      "only\n",
      " woman\n",
      " does\n",
      "\n",
      " use\n",
      " that\n",
      " \n",
      " F\n",
      "erm\n",
      "i\n",
      " paradox\n",
      " \n",
      " \n",
      "then\n",
      " you\n",
      " might\n",
      " want\n",
      " to\n",
      " read\n",
      " that\n",
      " sentence\n",
      " (\n",
      "if\n",
      "\n",
      " that\n",
      " sentence\n",
      " \n",
      " does\n",
      " the\n",
      " thing\n",
      " \n",
      " it\n",
      " doesn\n",
      "'t\n",
      " want\n",
      " you\n",
      " to\n",
      " \n",
      " \n",
      "try\n",
      " to\n",
      " figure\n",
      " out\n",
      "\n",
      ").\n",
      "\n",
      "\n",
      "So\n",
      " what\n",
      " can\n",
      " be\n",
      " done\n",
      " in\n",
      " terms\n",
      " of\n",
      " this\n",
      " \n",
      " \n",
      "F\n",
      "erm\n",
      "i\n",
      " paradox\n",
      " and\n",
      " what\n",
      " it\n",
      " doesn\n",
      "\n",
      "'t\n",
      " work\n",
      "?\n",
      " Let\n",
      " me\n",
      " explain\n",
      " a\n",
      " simple\n",
      " language\n",
      " like\n",
      " \n",
      " \n",
      "Spanish\n",
      " \n",
      " \n",
      "which\n",
      " you\n",
      " might\n",
      "\n",
      " not\n",
      " want\n",
      " to\n",
      " be\n",
      " a\n",
      " part\n",
      " of\n",
      ",\n",
      " but\n",
      " will\n",
      " have\n",
      " in\n",
      " your\n",
      " everyday\n",
      " life\n",
      " (\n",
      "and\n",
      " even\n",
      "\n",
      " then\n",
      " for\n",
      " most\n",
      " \n",
      " \n",
      "other\n",
      " languages\n",
      " like\n",
      " German\n",
      ",\n",
      " English\n",
      " and\n",
      " Chinese\n",
      " ):\n",
      "\n",
      "\n",
      "Let\n",
      "\n",
      " us\n",
      " assume\n",
      " a\n",
      " \n",
      " F\n",
      "erm\n",
      "i\n",
      "-\n",
      "only\n",
      " English\n",
      " speaker\n",
      " \n",
      " using\n",
      " \n",
      " an\n",
      " English\n",
      " \n",
      " \n",
      "letter\n",
      " (\n",
      "a\n",
      "\n",
      " letter\n",
      ",\n",
      " a\n",
      " syll\n",
      "able\n",
      " and\n",
      " an\n",
      " \n",
      " u\n",
      "pperc\n",
      "ase\n",
      ",\n",
      " so\n",
      " \n",
      " we\n",
      " could\n",
      " have\n",
      " two\n",
      " of\n",
      " them\n",
      " \n",
      " \n",
      "\n",
      "for\n",
      " this\n",
      " example\n",
      " ):\n",
      "  \n",
      " \n",
      "and\n",
      " \n",
      " you\n",
      " \n",
      " use\n",
      " it\n",
      " \n",
      " and\n",
      " use\n",
      " the\n",
      " words\n",
      " \n",
      " of\n",
      " \n",
      "The Fermi paradox  (which can also be used in a simple way as an explanatory principle)\n",
      "I'll give it some attention. First I want to talk about an important problem:\n",
      "How are you doing it?  (If you're reading this in the main blog , that might sound bored and uneducated) Why don't you use it, right? Because it can really be very useful if you are thinking and doing something that will benefit you, even though it is only part of what's possible  if it were not so much your task. It doesn't make it easy and doesn't allow for  an  easier task.  So what? The problem of making things work  can help people with this  problem and the more  we know about  what's possible  then  we  know it can be much better, too. I've found many people with such issues are simply using \"just say no\" or using a language that they  understand but can't really explain to a computer (or anyone, I can't imagine a language like Java that isn't a little  disgust in its approach and the kind of problem it poses. ) I've tried it myself, using a simple  word (as well as  a word of grammar ) which is not only  easier but it makes things work.  In order to make something work we just  want the words of the   text, but then there is no way to find it if we  do a  google  search  and we have no idea of how the words  work, but we don't have to know it at all! We simply use a few of the basic phrases of this simple English word  that make a person feel  like a good man  in some way or another. It makes sense then and it's so helpful when someone does it! I know that you've heard of some examples of how this is used, such as a friend  telling a  stranger she didn't understand how she was talking. In some instances of such examples, people are using these examples, as they're used with a simple phrase (as a simple \"what did I just say\", for example) to describe their problems and then later they have it all  over again  in their everyday life  because  they are able to think about and do some very useful things and so on. So how does one make  the Fermi paradox work? I will talk about what's a very simple language that is very  powerful, especially in English (I can imagine some languages in English which are just like Fermi), that uses  very simple, non-technical  syntax (like a \"numeric-completive  which is just as simple, yet still is  easy  for most  people to comprehend\"). I'll start by giving you an idea on why the simple Fermi  does it (or what we should call it): The Fermi Paradox:\n",
      "So what if I said this: You could say you are using this language, which I will describe in this paragraph as a language, and the people reading you can see how much the  people  use the Fermi paradox. But the Fermi paradox has the  opposite effect in terms of understanding people's needs for their everyday needs (for instance a friend or  mother who doesn't know the words and doesn't want them used).  I have found that if a  friend uses Fermi and  a woman does not. But in my view if  she has been reading Fermi she  uses something similar  and so on (like a language with no language and just the \"I  used a different  phrase  than you, which was not so easy  to comprehend and didn't let her see that the word 'naturally did' was actually a word for \"not at all  a good  speaker or an  adviser to an   word \" ). If  a Fermi-only woman does use that  Fermi paradox  then you might want to read that sentence (if that sentence  does the thing  it doesn't want you to  try to figure out).\n",
      "So what can be done in terms of this  Fermi paradox and what it doesn't work? Let me explain a simple language like  Spanish  which you might not want to be a part of, but will have in your everyday life (and even then for most  other languages like German, English and Chinese ):\n",
      "Let us assume a  Fermi-only English speaker  using  an English  letter (a letter, a syllable and an  uppercase, so  we could have two of them  for this example ):   and  you  use it  and use the words  of \n"
     ]
    }
   ],
   "source": [
    "text = \"The Fermi paradox \"\n",
    "tokens = tokenizer.encode(text)\n",
    "num_steps = 1024 - len(tokens) + 1\n",
    "line_length, max_length = 0, 70\n",
    "\n",
    "probs_thres = 0.8\n",
    "\n",
    "for i in range(num_steps):\n",
    "\n",
    "    logits = model(tf.Variable(tokens, dtype=tf.float32))[0]\n",
    "    p_next = tf.nn.softmax(logits[0, -1, :]).numpy()\n",
    "\n",
    "    prob_cumsum = 0\n",
    "    next_possible_token = []\n",
    "    for index in np.argsort(-p_next)[:10]:\n",
    "        prob_cumsum+=p_next[index]\n",
    "        next_possible_token.append(index)\n",
    "        if prob_cumsum>probs_thres:\n",
    "            break\n",
    "    next_token_index = random.choice(next_possible_token)\n",
    "    \n",
    "    # next_token_index = int(p_next.argmax())\n",
    "    tokens.append(next_token_index)\n",
    "    print(tokenizer.decode(tokens[-1]))\n",
    "    line_length += len(tokenizer.decode(tokens[-1]))\n",
    "    if line_length >= max_length:\n",
    "        line_length = 0\n",
    "        print()\n",
    "        # break\n",
    "\n",
    "print(tokenizer.decode(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').train(False).to(device)\n",
    "\n",
    "text = \"The Fermi paradox \"\n",
    "tokens = tokenizer.encode(text)\n",
    "num_steps = 1024 - len(tokens) + 1\n",
    "line_length, max_length = 0, 70\n",
    "\n",
    "print(end=tokenizer.decode(tokens))\n",
    "\n",
    "for i in range(num_steps):\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.as_tensor([tokens], device=device))[0]\n",
    "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
    "\n",
    "    next_token_index = p_next.argmax() #<YOUR CODE: REPLACE THIS LINE>\n",
    "    # YOUR TASK: change the code so that it performs nucleus sampling\n",
    "\n",
    "    tokens.append(int(next_token_index))\n",
    "    print(end=tokenizer.decode(tokens[-1]))\n",
    "    line_length += len(tokenizer.decode(tokens[-1]))\n",
    "    if line_length >= max_length:\n",
    "        line_length = 0\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Vij7Gc1wOaq"
   },
   "source": [
    "Transformers knowledge hub: https://huggingface.co/transformers/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
