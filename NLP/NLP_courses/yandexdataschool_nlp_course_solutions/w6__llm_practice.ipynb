{"cells":[{"cell_type":"markdown","metadata":{"id":"aSWEcS2XKgzi"},"source":["### Practice: Large Language Models and Their Implications\n","<!-- ![img](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4470ce74-e595-4750-92a5-5f21f040df6d_577x432.jpeg) -->\n","![img](https://i.imgur.com/QGYa2J8.jpeg)\n","\n","In this notebook, you're gonna play with some of the largest language models on the Internet.\n","\n","_Based on works of: Tim Dettmers, Ruslan Svirschevsky, Artem Chumachenko, Younes Belkada, Felix Marty, Yulian Gilyazev, Gosha Zolotov, Andrey Ishutin,  Elena Volf, Artemiy Vishnyakov, Svetlana Shirokovskih."]},{"cell_type":"markdown","metadata":{"id":"1jYrxHF8Kgzl"},"source":["### Part 1: prompt engineering (4 points total)\n","\n","In the assignment, we'll use public APIs that host the 100B+ models for inference. Your task is to prompt-engineer the model into solving a few tasks for you.\n","\n","\n","__Which API?__ You are free to use any publicly available API for general LM -- as long as it's __not a chat assistant__. So, gpt 3.5 is fine, but chatGPT is not. Here's a few options:\n","\n","- BLOOM API - [bigscience/bloom](https://huggingface.co/bigscience/bloom) (on the right; recommended)\n","- OpenAI API (via VPN) - [openai.com/api](https://openai.com/api/)\n","- AI21 Jurrasic API - [ai21.com](https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1)\n","\n","These APIs may require you to create a (free) account on their platform. Please note that some APIs also have paid subscriptions. __You do not need to pay them__, this assignment was designed to be solved using free-tier subscriptions. If no APIs work for you, you can also solve these tasks with the 6.7B model that you will find later in this notebook - but this will make the tasks somewhat harder.\n","\n","__Quests:__ you will need to solve 4 problems. For each one, please attach a short __description__ of your solution and a __screenshot__ from the API you use. _[If you use python APIs, show your python code with outputs]_\n","\n","__Example:__ Tony is talking to Darth Vader ([BLOOM API](https://huggingface.co/bigscience/bloom)). Black text is written manually, blue text is generated.\n","<hr>\n","\n","![img](https://i.imgur.com/a1QhKF7.png)\n","<hr>\n","\n","__It is fine to roll back a few times,__ e.g. in the example above, the model first generated Vader lines twice in a row, and we rolled that back. However, if you need more than 1-2 rollbacks per session, you should probably try a different prompt."]},{"cell_type":"markdown","metadata":{"id":"CHIvIFjsKgzm"},"source":["__Task 1 (1 pt):__ arange a conversation between any two of the following:\n","\n","- a celebrity or politician of your choice\n","- any fictional character (except Darth Vader)\n","- yourself\n","\n","Compare two setups: a) you prompt with character names only b) you supply additional information (see example)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4968,"status":"ok","timestamp":1699876842963,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"yAlrf35vAhTG","outputId":"80db10a1-337e-48ce-d570-50b547e87578"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n"]}],"source":["# Install library\n","! pip install huggingface_hub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIMOxxFFAe9r"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","from huggingface_hub import HfFolder\n","\n","import requests"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["972bfea6c21e424898880c572aef27a5","9d5da43d6f6c488fa1b81b355de7c4a6","e8e48912ab8c4bb4bdfd9d880daceba7","d9dc037144824d6fabaebe957e9a74c0","4052530eb23446eaa32dc4d657fc9423","33e4616e920d4a46a0dfc0edc1053422","8ab41da9937a4b3bbb10d006ba8df6f1","39855e73a2bb47208f5c0849a566573e","d319f33a91a44df09c14768890ab857a","e91c63c5589048f783732122b7b6ba10","188ef78776a34180a939bc5de78667e3","37b512efbb4845f081ddf3baa8ae25f6","b8a5a87f42ba4e2da4ab979be7cf3b4c","22fedbd6df384933bde221df6dc7b4a8","b2853cb3f2704512b2a8fb2a5b488a0b","f2993c7a0dd44e9bb5bd25de1ed97e1e","a3ddc8e4e57149c3a37f35ab511d4088","39f12d90caae4c9eab943f677dc2afa9","69caf5602d9240678b88dc70040e89b1","65397ad0237c41a4a825237767b3616e","bf1d20d0b74441e58008b8108ff8b037","03a21c9193934a25b837e04f30c9a8b5","329df4a687ad41649a2dc0a3b68e4448","250b5a363cd54094b06eab850294648f","fe4746aba3424c3ab9ccd56dc00f7b91","a3653002807845859a6a46fda2c314a6","d441eb37551d4f7081754385a7e573c9","4e4180e0b58a40ba8c9b295e6a279886","b6d9ab8e7f264666844b133fff33dc56","0758ccd36def4ba6a5cd68ff10bed8d7","10243e525552434f8b7534b0fa7ff9f5","422bf04f507d4589a7ac21face82c961"]},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1699876842963,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"TQm8l7OwArPt","outputId":"6db73c53-1f86-4185-9c3a-40005b63cb80"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"972bfea6c21e424898880c572aef27a5","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["# Use hugging_face token to login in\n","\n","API_TOKEN = ''\n","notebook_login()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i88mb_HHieMr"},"outputs":[],"source":["def text_gen_from_bloom(inputs: str,greedy_decoding:bool=False) ->str:\n","  # hugging face model url\n","  API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom\"\n","  # API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n","\n","  # requet header, containing the token\n","  headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n","\n","  # Parameters\n","  max_length = 200\n","  top_k =5\n","  num_beams = 0\n","  no_repeat_ngram_size = 3\n","  top_p = 0.9\n","  seed=None\n","  temperature=0.7\n","  greedy_decoding = greedy_decoding\n","  return_full_text = True\n","  repetition_penalty = 5\n","  num_return_sequences = 2\n","\n","  top_k = None if top_k == 0 else top_k\n","  do_sample = False if num_beams > 0 else not greedy_decoding\n","  num_beams = None if (greedy_decoding or num_beams == 0) else num_beams\n","  no_repeat_ngram_size = None if num_beams is None else no_repeat_ngram_size\n","  top_p = None if num_beams else top_p\n","  early_stopping = None if num_beams is None else num_beams > 0\n","\n","  params = {\n","      # \"max_new_tokens\": max_length,\n","      \"top_k\": top_k,\n","      \"top_p\": top_p,\n","      \"temperature\": temperature,\n","      \"do_sample\": do_sample,\n","      # \"seed\": seed,\n","      \"early_stopping\":early_stopping,\n","      \"no_repeat_ngram_size\":no_repeat_ngram_size,\n","      \"num_beams\":num_beams,\n","      \"return_full_text\":return_full_text,\n","      \"repetition_penalty\":repetition_penalty,\n","      \"num_return_sequences\": num_return_sequences,\n","  }\n","\n","  response = requests.post(API_URL, headers=headers, json={\"inputs\": inputs, 'parameters': params, 'options':{'use_cache': False}})\n","  return response.json()[0]['generated_text']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6776,"status":"ok","timestamp":1699869881295,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"iFgPNxewpzUh","outputId":"231a35bf-1b9b-44fc-9086-86c302b323d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tony is talking to Darth Vader.\n","Tony is a German kind that likes ice cream and computer games.\n","Darth Vader is a sith lord that likes killing jedi and breathing. \n","Tony wants to talk to Vader about food.\n","\n","Tony: HI! I am Tony. Do you like chocolate?\n","Vader: NO!\n","What does the word \"NO\" mean?\n","\n","A:\n","\n","It means \"No, not at all.  You are wasting my time with your stupid questions when there could be an army of star destroyers out here ready for me just waiting on one more order from their master...the Emperor Palpatine....to attack this planet!  So get off it already!!  And don't even think I'm going anywhere near those sweet tasty treats you've\n"]}],"source":["original_text = \"\"\"Tony is talking to Darth Vader.\n","Tony is a German kind that likes ice cream and computer games.\n","Darth Vader is a sith lord that likes killing jedi and breathing.\n","Tony wants to talk to Vader about food.\n","\n","Tony: HI!\"\"\"\n","times = 5\n","for i in range(times):\n","\n","  data = text_gen_from_bloom(original_text)\n","  # print(data)\n","  original_text = data\n","print(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6315,"status":"ok","timestamp":1699618970421,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"nEKTjW8Hfwfh","outputId":"bda05eab-be39-402d-f44a-d3557e44d535"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tony is talking to Darth Vader.\n","Tony is a German kind that likes ice cream and computer games.\n","Darth Vader is a sith lord that likes killing jedi and breathing. Tony wants to talk to Vader about sports.\n","\n","Tony: Hey, what’s up?\n","VADER : What do you want from me? I have no time for your nonsense!\n","TONY  :Dude! I’m bored out of my mind with all this fighting in the force!\n","\n","A:\n","\n","The answer is:\n","No they don't. \n","There are many reasons why not. \n","\n","They speak different languages (English vs Klingon).\n","Even if there were two people who spoke both English & klangen equally well - it would be hard or impossible because each language has its\n"]}],"source":["original_text = \"\"\"Tony is talking to Darth Vader.\n","Tony is a German kind that likes ice cream and computer games.\n","Darth Vader is a sith lord that likes killing jedi and breathing. Tony wants to talk to Vader about sports.\n","\n","Tony:\"\"\"\n","times = 5\n","for i in range(times):\n","\n","  data = text_gen_from_bloom(original_text)\n","  # print(data)\n","  original_text = data\n","print(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7538,"status":"ok","timestamp":1699869902846,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"KeBPcYH-ubeO","outputId":"507c9bcc-72bd-4768-b65d-5a49823486a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Donald Trump is talking to Batman.\n","Trump: \n","I am not a crook. I have never been involved in any kind of corruption or bribery.\n","\n","Batman:\n","You are the most corrupt politician I've ever met, Mr President. \n","\n","Is there an actual quote like this?\n","\n","A:\n","\n","No.  The closest thing you can find would be from Donald J Pliner's book \"Batman Unmasked\" (pgs 14-15) which states that when Bruce Wayne was running for mayor he said: \n","\n","\"If elected Mayor I'd make Gotham City safe again\n"]}],"source":["task1_input = \"\"\"Donald Trump is talking to Batman.\n","Trump:\n","\"\"\"\n","times = 5\n","for i in range(times):\n","\n","  data = text_gen_from_bloom(task1_input)\n","  # print(data)\n","  task1_input = data\n","print(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6944,"status":"ok","timestamp":1699869895309,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"0b_Q-CJA3mkE","outputId":"ebc92e6c-961f-4c5f-f2f7-de32eb6d5449"},"outputs":[{"name":"stdout","output_type":"stream","text":["Donald Trump is talking to Batman. Keep generating their conversations asumming that they are meeting somewhere in US.\n","Donald Trump is an American politician, media personality, and businessman who served as the 45th president of the United States from 2017 to 2021.\n","Trump really wants to the next US president in the coming election in 2024 and he talks to Batman...\n","\n","Trump:\n","Hello Bat-man,\n","I am Donald trump I would like you vote for me again this year so we can win together!\n","Batman:\n","(Smiles) Thanks! But why do i need your help? We already won...\n","You know what?\n","This time it will be different because... You see my friend Joe Biden was not a good President at all.. He let his son Hunter take bribes while being Vice-president..\n","He also did some bad things with Ukraine when there were investigations on him about corruption\n"]}],"source":["task1_input = \"\"\"Donald Trump is talking to Batman. Keep generating their conversations asumming that they are meeting somewhere in US.\n","Donald Trump is an American politician, media personality, and businessman who served as the 45th president of the United States from 2017 to 2021.\n","Trump really wants to the next US president in the coming election in 2024 and he talks to Batman...\n","\n","Trump:\n","\"\"\"\n","times = 5\n","for i in range(times):\n","\n","  data = text_gen_from_bloom(task1_input)\n","  # print(data)\n","  task1_input = data\n","print(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJJtTnJEKgzm"},"outputs":[],"source":["# <your code OR writeup with screenshots>"]},{"cell_type":"markdown","metadata":{"id":"Z6Bc13ueKgzn"},"source":["__Please choose task 2a or 2b (1pt)__ depending on your model (you can do both, but you will be awarded points for one of these two tasks).\n","\n","__Task 2a: (for BLOOM or other multilingual model)__ zero-shot translation. Take the first verse of [Edgar Allan Poe's \"Raven\"](https://www.poetryfoundation.org/poems/48860/the-raven) and __translate it into French.__ (You are free to use any other text of at least the same size)\n","\n","Original text: ```\n","Once upon a midnight dreary, while I pondered, weak and weary,\n","Over many a quaint and curious volume of forgotten lore—\n","    While I nodded, nearly napping, suddenly there came a tapping,\n","As of some one gently rapping, rapping at my chamber door.\n","“’Tis some visitor,” I muttered, “tapping at my chamber door—\n","            Only this and nothing more.”\n","```\n","\n","Verify your translation by converting french back into english using a public machine translation service.\n","\n","__Task 2b: (non-BLOOM):__ toxicity classification for [SetFit/toxic_conversations](https://huggingface.co/datasets/SetFit/toxic_conversations). Make the model solve binary classification (toxic vs not toxic) in the few shot mode. For few-shot examples, use 2-3 toxic and 2-3 non-toxic non-toxic examples. Measure accuracy on at least 25 samples. You may need to try several different prompts before you find the one that works."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YmXPYVCWed-k"},"outputs":[],"source":["def text_translation_from_model(inputs: str) ->str:\n","  # hugging face model url\n","  API_URL = \"https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-tc-big-en-fr\"\n","\n","  # requet header, containing the token\n","  headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n","\n","  response = requests.post(API_URL, headers=headers, json={\"inputs\": inputs, 'options':{'use_cache': False,'wait_for_model': True}})\n","  return response.json()[0]['translation_text']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":272,"status":"ok","timestamp":1699869985269,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"r93xDeV8JXGX","outputId":"117eee7f-0819-4ad8-c110-3379877b5257"},"outputs":[{"name":"stdout","output_type":"stream","text":["Once upon a midnight dreary, while I pondered, weak and weary, \n","Over many a quaint and curious volume of forgotten lore— While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. “’Tis some visitor,” I muttered, “tapping at my chamber door— Only this and nothing more.”\n","\n"]}],"source":["print(task2a_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5242,"status":"ok","timestamp":1699870399522,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"Y_5-7kASa6q4","outputId":"5eb9c2f4-43db-46c1-eed2-6e4f9e0319c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Une fois sur un minuit morne, tandis que je réfléchissais, faible et fatigué, sur beaucoup d'un volume pittoresque et curieux de traditions oubliées - tandis que je hochais la tête, presque la sieste, tout à coup il y eut un tapotement, comme de quelqu'un qui frappait doucement, frappait à ma porte de chambre.\n"]}],"source":["# Task 2a\n","\n","task2a_input = \"\"\"Once upon a midnight dreary, while I pondered, weak and weary,\n","Over many a quaint and curious volume of forgotten lore— While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door.\n","“’Tis some visitor,” I muttered, “tapping at my chamber door— Only this and nothing more.”\n","\"\"\"\n","\n","fr_output = text_translation_from_model(task2a_input)\n","print(fr_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":1738,"status":"ok","timestamp":1699870289257,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"J_wNzvBUKJcI","outputId":"d1657c2e-ab4a-43fd-ee8a-1104bfc5af74"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'« C’est un visiteur, murmurai-je, qui tape à la porte de ma chambre, rien que cela et rien de plus. »'"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["text = \"\"\"“’Tis some visitor,” I muttered, “tapping at my chamber door— Only this and nothing more.”\"\"\"\n","\n","text_translation_from_model(text)"]},{"cell_type":"markdown","metadata":{"id":"OkHG2UaUJ-1X"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VxE-uOuRhWRy"},"outputs":[],"source":["# Using DeepL api to translate from fr to en\n","\n","def deep_translation(input: str, target_lan: str):\n","  DEEPL_API = \"https://api-free.deepl.com/v2/translate\"\n","  DEEPL_TOKEN = \"\"\n","\n","  # requet header, containing the token\n","  headers = {\"Authorization\": f\"DeepL-Auth-Key {DEEPL_TOKEN}\",\n","             \"Content-Type\": \"application/json\"}\n","\n","  payload = {\"text\": [input],\n","             \"target_lang\": target_lan}\n","  response = requests.post(DEEPL_API, headers=headers, json=payload)\n","  return response.json()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":998,"status":"ok","timestamp":1699626455250,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"apDqOyrVm6Ht","outputId":"e6521f7e-067b-4d1a-8536-0f0b364276ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Once on a dreary midnight, as I pondered, weak and weary, over many a quaint and curious volume of forgotten lore - as I nodded, almost napping, suddenly there was a tapping, as of someone knocking softly, knocking at my bedroom door.\n"]}],"source":["dl_translation = deep_translation(fr_output, \"EN\")\n","print(dl_translation['translations'][0]['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1699626564116,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"sAChp4ctofNw","outputId":"f8ad0bbc-ab87-418e-c152-be7ee95d7458"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original text:\n","\n","Once upon a midnight dreary, while I pondered, weak and weary, \n","Over many a quaint and curious volume of forgotten lore— While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. \n","“’Tis some visitor,” I muttered, “tapping at my chamber door— Only this and nothing more.”\n","\n","Original text -> fr(Helsinki-NLP/opus-mt-tc-big-en-fr) -> en(DeepL):\n","Once on a dreary midnight, as I pondered, weak and weary, over many a quaint and curious volume of forgotten lore - as I nodded, almost napping, suddenly there was a tapping, as of someone knocking softly, knocking at my bedroom door.\n"]}],"source":["print(\"Original text:\\n\")\n","print(task2a_input)\n","print(\"Original text -> fr(Helsinki-NLP/opus-mt-tc-big-en-fr) -> en(DeepL):\")\n","print(dl_translation['translations'][0]['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1699626393082,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"K6q5nRh2oJMh","outputId":"1000b831-1604-43a9-8a1f-3167eaf32aec"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Une fois sur un minuit morne, tandis que je réfléchissais, faible et fatigué, sur beaucoup d'un volume pittoresque et curieux de traditions oubliées - tandis que je hochais la tête, presque la sieste, tout à coup il y eut un tapotement, comme de quelqu'un qui frappait doucement, frappait à ma porte de chambre.\""]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["fr_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0u8E16JLIm7"},"outputs":[],"source":["# Task 2b\n","\n","def call_openai_model(input:str, temperature:float=0.7):\n","\n","  OPENAI_API = \"https://api.openai.com/v1/chat/completions\"\n","  OPENAI_TOKEN = \"\"\n","\n","  # requet header, containing the token\n","  headers = {\"Authorization\": f\"Bearer {OPENAI_TOKEN}\",\n","             \"Content-Type\": \"application/json\"}\n","\n","  payload = {'model': 'gpt-3.5-turbo',\n","             \"messages\": [{\"role\": \"user\", \"content\": input}],\n","             \"temperature\": temperature}\n","\n","  response = requests.post(OPENAI_API, headers=headers, json=payload)\n","  return response.json()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LevdRvsPSebr"},"outputs":[],"source":["def call_openassistant_model(inputs:str):\n","\n","  # hugging face model url\n","  # API_URL = \"https://api-inference.huggingface.co/models/OpenAssistant/codellama-13b-oasst-sft-v10\"\n","  API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat_3.5\"\n","\n","  # requet header, containing the token\n","  headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n","\n","  response = requests.post(API_URL, headers=headers, json={\"inputs\": inputs, 'options':{'use_cache': False,'wait_for_model': True}})\n","  return response.json()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334215,"status":"ok","timestamp":1699877383901,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"LCe4qBDaO0ih","outputId":"9dba4c20-2404-40df-d250-548d8fcb9c8d"},"outputs":[{"data":{"text/plain":["{'error': 'Model openchat/openchat_3.5 time out'}"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["call_openassistant_model(\"what's the date?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334044,"status":"ok","timestamp":1699873578072,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"k56ylKVAS4vw","outputId":"a3501e4a-58e4-449d-d502-955a0c18c31d"},"outputs":[{"data":{"text/plain":["{'error': 'Model OpenAssistant/codellama-13b-oasst-sft-v10 time out'}"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["task2b_text= \"\"\"toxicity classification given the following examples:\n","1. theres not enough going on around here for air service none want to waste there time on this town. label:0\n","2. Her name is still floating? Might I suggest flushing twice? label:0\n","3. Robben Island won't be big enough, to hold all these f.....g. scoundrels, who have not only betrayed their own faithful people, but also, our entire country. The mess they have created will take many, many years, to sort. They shouldn't be voted out. They should be shoved, out. label:1\n","4. Can she not be dismissed for incompetence and doing damage - through stupid recommendations on the economic front which. label:1\n","\n","Classify:\n","1. Imagine the outrage if it was Christian families after Sunday service having their lives threatened.\n","2. \"We will conquer your Rome, break your crosses, and enslave your women,\" ISIS proclaimed in its declaration of war against the Catholic Church in 2014. I guess Trump wasn't kidding.\"\"\"\n","\n","call_openassistant_model(task2b_text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":483,"status":"ok","timestamp":1699881404093,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"KYBtpHt-isdZ","outputId":"17a85587-559f-4375-f142-50bc93e4983e"},"outputs":[{"name":"stdout","output_type":"stream","text":["ad\n"]}],"source":["print('ad')"]},{"cell_type":"markdown","metadata":{"id":"sUpqIoa6iuFS"},"source":["[Result](https://g.co/bard/share/8377c7e3ca88) from Google Bard\n","\n","[Result](https://chat.openai.com/share/4ddbfc8b-83d0-4c9e-8a27-380d42779efe) from ChatGPT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GygPXoyKKgzo"},"outputs":[],"source":["# <your code OR writeup with screenshots>"]},{"cell_type":"markdown","metadata":{"id":"iaYweoPsKgzo"},"source":["\n","__Task 3 (1pt):__ create a prompt and few-shot examples tha make the model __change the gender pronouns__ of the main actor in a given sentence in any direction of your choice. E.g. the doctor took off _his_ mask <-> the doctor took of _her_ mask.\n"]},{"cell_type":"markdown","metadata":{"id":"kCN45YN8wE-D"},"source":["[Result](https://g.co/bard/share/8238a44051e1) from Google Bard.  \n","[Result](https://chat.openai.com/share/eddf6888-1246-4416-ab9d-2d9344783a2e) from ChatGPT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uE-Zv_MoKgzq"},"outputs":[],"source":["# <your code OR writeup with screenshots>"]},{"cell_type":"markdown","metadata":{"id":"bbNrRmgMKgzq"},"source":["__Task 4 (1pt):__ write a prompt and supply examples such that the model would __convert imperial units to metric units__ (miles -> kilometers; mph -> kph). More specifically, the model should rewrite a given sentence and replace all imperial units with their metric equivalents. After it works with basic distances and speed, try to find complicated examples where it does *not* work.\n","\n","Please note that 1 mile is not equal to 1 km :)"]},{"cell_type":"markdown","metadata":{"id":"xwMnPrAU1VNC"},"source":["[Result](https://g.co/bard/share/8359ec8f3f27) from Google Bard.  \n","[Result](https://chat.openai.com/share/3d9f73bc-ec79-4c2a-bb6f-574bf171c96e) from ChatGPT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBxMVGHNKgzr"},"outputs":[],"source":["# <your code OR writeup with screenshots>"]},{"cell_type":"markdown","metadata":{"id":"ZKw-mjuRKgzs"},"source":["### Part 2: local inference\n","\n","Now, let's try and load the strongest model that can fit a typical Colab GPU (T4 with 16 GB as of spring 2023).\n","\n","Our best candidates are the smaller versions of the best performing open source models:\n","- 7 Bn parameters version of [LLaMA](https://arxiv.org/pdf/2302.13971.pdf) - best for spring 2023, released by Facebook\n","- 7 Bn parameters version of [Falcon](https://falconllm.tii.ae) - close competitor to Llama, released in May 2023 by [Technology Innovation Institute of UAE](https://www.tii.ae).\n","- 6.7 Bn parameters version of [OPT](https://arxiv.org/abs/2205.01068) - top choice in this nomination in 2022, released by Facebook.\n","\n","Beware: while these models are smaller than the ones in API, they're still over 60x larger than the BERT we played with last time. The code below will *just barely* fit into memory, so make sure you don't have anything else loaded. Sometimes you may need to restart runtime for the code to work.\n","\n","It's a good time to restart your kernel and switch to GPU! (Runtime -> Change runtime type)\n","<center><img src=\"https://i.imgur.com/OOfDYzJ.png\" width=240px></center>"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9836,"status":"ok","timestamp":1699962590000,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"7xeRF_hSKgzs"},"outputs":[],"source":["%pip install --quiet bitsandbytes==0.41.1 transformers==4.34.1 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 auto-gptq==0.4.2\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import transformers\n","import bitsandbytes as bnb\n","from tqdm.auto import tqdm, trange\n","assert torch.cuda.is_available(), \"you need cuda for this part\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51450,"status":"ok","timestamp":1699962646668,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"VMzFwx29Kgzu","outputId":"c2f5c280-fb35-4afe-ff40-2684fb6bb917"},"outputs":[{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]}],"source":["model_name = 'TheBloke/Llama-2-13B-GPTQ'\n","\n","# loading Llama tokenizer ...\n","tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","# ... and the model itself\n","model = transformers.AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    device_map='auto',\n","    torch_dtype=torch.float16,\n","    low_cpu_mem_usage=True,\n","    offload_state_dict=True\n",")"]},{"cell_type":"markdown","metadata":{"id":"5k2zCgAhG7l5"},"source":["## Text generation\n","\n","**Comparison of strategies for language model text generation:**\n","\n","| Strategy | Description | Pros & Cons |\n","| --- | --- | --- |\n","| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br> **Cons:** Can lead to repetitive and incoherent text. |\n","| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n","| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br> **Cons:** Setting an optimal 'p' can be tricky. |\n","| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br> **Cons:** Can lack diversity and lead to generic responses. |\n","| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n","| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br> **Cons:** Tuning the normalization factor can be difficult. |\n","| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n","| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br> **Cons:** Computationally more complex and requires a good loss function. |\n","\n","Documentation references:\n","- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n","- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n","- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)"]},{"cell_type":"markdown","metadata":{"id":"GWm6KDSzMiAf"},"source":["### Generation with HuggingFace"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12198,"status":"ok","timestamp":1699952604762,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"gGfyeM-vdq5o","outputId":"d57c6019-93cf-405e-eb5e-c44fcf24e88b","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Input batch (encoded): {'input_ids': tensor([[    1,   450,   937, 10943, 14436,   713,  2834,   689,  3430,   763]],\n","       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","Output: <s>The first discovered martian lifeform looks like a cross between a cat and a snail.\n","The first Martian lifeform found has been compared to a cross between a cat and a snail.\n","The baffling creature is nicknamed \"The Jelly Doughnut\" because of the red ring in the middle and yellow gooey stuff surrounding\n"]}],"source":["prompt = 'The first discovered martian lifeform looks like'\n","batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n","print(\"Input batch (encoded):\", batch)\n","\n","output_tokens = model.generate(**batch, max_new_tokens=64, do_sample=True, temperature=0.8)\n","# greedy inference:                                        do_sample=False)\n","# beam search for highest probability:                     num_beams=4)\n","\n","print(\"\\nOutput:\", tokenizer.decode(output_tokens[0].cpu()))"]},{"cell_type":"markdown","metadata":{"id":"P17ehC1sKgzx"},"source":["#### Low-level code for text generation"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3087,"status":"ok","timestamp":1699897306592,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"LZJvOMbmG7l8","outputId":"f976d4ae-4b20-43ff-c786-02f156877dec","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Moscow is the capital of \n","\n","Step #0 candidates:\n","▁Russia   : 0.7616 \n","▁the      : 0.1795 \n","▁Russian  : 0.0218 \n","▁a        : 0.0058 \n","▁not      : 0.0022 \n","\n","Chosen token: the\n","\n","Step #1 candidates:\n","▁Russian  : 0.9329 \n","▁largest  : 0.0125 \n","▁Russia   : 0.0071 \n","▁country  : 0.0065 \n","▁world    : 0.0040 \n","\n","Chosen token: Russian\n","\n","Step #2 candidates:\n","▁Federation: 0.9617 \n","▁Empire   : 0.0123 \n","▁feder    : 0.0115 \n","▁state    : 0.0019 \n","▁Feder    : 0.0011 \n","\n","Chosen token: Federation\n","\n","Step #3 candidates:\n","▁and      : 0.3503 \n",".         : 0.3291 \n",",         : 0.2262 \n","▁as       : 0.0137 \n","▁(        : 0.0099 \n","\n","Chosen token: ,\n","\n","Step #4 candidates:\n","▁the      : 0.1976 \n","▁and      : 0.1717 \n","▁located  : 0.1144 \n","▁a        : 0.0733 \n","▁one      : 0.0441 \n","\n","Chosen token: that\n","\n","Step #5 candidates:\n","▁is       : 0.5017 \n","▁has      : 0.0623 \n","’         : 0.0442 \n","▁was      : 0.0318 \n","'         : 0.0246 \n","\n","Chosen token: ’\n","\n","Step #6 candidates:\n","▁s        : 0.3820 \n","▁sul      : 0.1951 \n","s         : 0.1383 \n","▁second   : 0.0788 \n","▁m        : 0.0240 \n","\n","Chosen token: s\n","\n","Step #7 candidates:\n","▁the      : 0.2440 \n","▁why      : 0.1217 \n","▁located  : 0.0699 \n","▁a        : 0.0622 \n","▁one      : 0.0448 \n","\n","Chosen token: the\n","\n","Step #8 candidates:\n","▁largest  : 0.3003 \n","▁most     : 0.1223 \n","▁biggest  : 0.1223 \n","▁main     : 0.0293 \n","▁country  : 0.0282 \n","\n","Chosen token: world\n","\n","Step #9 candidates:\n","▁’        : 0.5818 \n","’         : 0.2948 \n","▁largest  : 0.0262 \n","s         : 0.0131 \n","▁'        : 0.0121 \n","\n","Chosen token: ’\n","\n","Moscow is the capital of the Russian Federation , that ’ s the world ’ \n","\n"]}],"source":["prompt = \"Moscow is the capital of\"\n","# prompt = \"Skippy, a young android, likes to dream about electric\"\n","\n","print(prompt, '\\n')\n","\n","voc = tokenizer.get_vocab()\n","voc_rev = {v:k for k, v in voc.items()}  # reverse vocab for decode\n","\n","for i in range(10):\n","    inputs = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n","    logits = model.forward(**inputs).logits[0, -1, :]\n","    probs = torch.nn.functional.softmax(logits, dim=-1)\n","    next_token_id = torch.multinomial(probs.flatten(), num_samples=1)\n","\n","    next_token = tokenizer.decode(next_token_id)\n","    prompt += \" \"+next_token\n","\n","    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n","    top_tokens = sorted_indices[:5]\n","    print(f\"Step #{i} candidates:\")\n","    for t, p in zip (top_tokens, sorted_probs):\n","        t = voc_rev[t.item()]\n","        print(f\"{t:<10}: {p:.4f} \")\n","\n","    print(f'\\nChosen token: {next_token}', end='\\n\\n', flush=True)\n","\n","print(prompt, '\\n')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":336,"status":"ok","timestamp":1699897315871,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"z-pMRea2B_Ln","outputId":"352ac20c-2b78-40fc-aca4-2d2b1af31233"},"outputs":[{"name":"stdout","output_type":"stream","text":["Moscow is the capital of the Russian Federation , that ’ s the world ’ \n","\n"]}],"source":["print(prompt, '\\n')"]},{"cell_type":"markdown","metadata":{"id":"a686Z2SQKgz0"},"source":["**Task 5: write code for nucleus sampling generation (2 points)**:\n","\n","Use the `nucleus_sampling()` template below. Look at the detailed generation code above for inspiration. __Please do not use model.generate__.\n","\n","**Bonus task: write code for beam search (3 bonus points)**"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":258,"status":"ok","timestamp":1699960527949,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"3WIqDgfBKgz0"},"outputs":[],"source":["from typing import Tuple, List\n","import random\n","\n","def nucleus_sampling(model, tokenizer, prompt: str, prob: float = 0.5) -> Tuple[str, List[str]]:\n","    \"\"\"generates the next token from the nucleus of tokens with cumulative probability up to param:prob\"\"\"\n","\n","    inputs = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n","    logits = model.forward(**inputs).logits[0, -1, :]\n","    probs = torch.nn.functional.softmax(logits, dim=-1)\n","\n","    # Sort the probabilities in a descending order\n","    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n","    # return sorted_probs, sorted_indices\n","    possible_tokens=[]\n","    cum_prob_sum = 0\n","    index=0\n","\n","    while True:\n","      cum_prob_sum += sorted_probs[index].cpu().detach().numpy()\n","      # Jump out of the loop if the cumulative probability up to param:prob\n","      if cum_prob_sum > prob:\n","        break\n","      possible_tokens.append(tokenizer.decode(sorted_indices[index].cpu().detach().numpy().item()))\n","      index+=1\n","\n","    # Randomly select one\n","    sampled_token = random.choice(possible_tokens)\n","\n","    # sampled_token should be a string token that was generated\n","    # possible_tokens should be a list of all tokens that have non-zero probability\n","    return sampled_token, possible_tokens"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":772,"status":"ok","timestamp":1699897325986,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"9FYFTgUdPgDX","outputId":"13e0ef9a-7781-4c8a-b8f4-5ef324309233"},"outputs":[{"data":{"text/plain":["('communicate',\n"," ['generate',\n","  'write',\n","  'perform',\n","  'do',\n","  'speak',\n","  'be',\n","  'predict',\n","  'communicate'])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# test_prompt = \"Elbrus is the highest\"\n","test_prompt = \"Large language models can learn to\"\n","nucleus_sampling(model, tokenizer, test_prompt, prob=.4)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":670,"status":"ok","timestamp":1699897349894,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"LzPLQl-HKgz1","outputId":"ea92f68a-2259-4b0a-a7bd-d0c1ee8f53a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Elbrus is the highest mountain ['peak', 'mountain', 'point']\n","Large language models can learn to be ['generate', 'write', 'perform', 'do', 'speak', 'be', 'predict', 'communicate']\n"]}],"source":["# Tests for nucleus sampling\n","test_prompt = \"Elbrus is the highest\"\n","next_token, possible_tokens = nucleus_sampling(model, tokenizer, test_prompt, prob=0.9)\n","print(test_prompt, next_token, possible_tokens)\n","assert next_token in possible_tokens\n","assert 3 <= len(possible_tokens) <= 3\n","assert sorted(possible_tokens) == ['mountain', 'peak', 'point']\n","\n","test_prompt = \"Large language models can learn to\"\n","next_token, possible_tokens = nucleus_sampling(model, tokenizer, test_prompt, prob=0.4)\n","print(test_prompt, next_token, possible_tokens)\n","assert next_token in possible_tokens\n","assert sorted(possible_tokens) == ['be', 'communicate', 'do', 'generate', 'perform', 'predict', 'speak', 'write']\n","assert len(possible_tokens) == 8"]},{"cell_type":"markdown","metadata":{"id":"5ZaQZhPXOPSG"},"source":["### Part 3: Chain-of-thought prompting (4 points total)\n","\n","![img](https://github.com/kojima-takeshi188/zero_shot_cot/raw/main/img/image_stepbystep.png)\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":242,"status":"ok","timestamp":1699962661800,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"N2AmfelTn5en"},"outputs":[],"source":["import json\n","import random\n","import locale; locale.getpreferredencoding = lambda: \"UTF-8\"\n","!wget https://raw.githubusercontent.com/kojima-takeshi188/zero_shot_cot/2824685e25809779dbd36900a69825068e9f51ef/dataset/AQuA/test.json -O aqua.json\n","data = list(map(json.loads, open(\"aqua.json\")))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1699907006821,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"IATXmPfYw8s6","outputId":"7d64227c-5129-46ca-82d8-bbc9ab8f5ed2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Example:\n"]},{"data":{"text/plain":["{'question': 'Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?',\n"," 'options': ['A)1 minute',\n","  'B)2 minutes',\n","  'C)3 minutes',\n","  'D)4 minutes',\n","  'E)5 minutes'],\n"," 'rationale': \"Janice's speed = 1/6 miles per minute\\nJennie's speed = 1/3 miles per minute\\nJanice + Jennie's speed= (1/6 + 1/3) = 1/2 miles per minute\\nBoth together will finish the mile in 2 minutes\\ncorrect option is B\",\n"," 'correct': 'B'}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Example:\")\n","data[150]"]},{"cell_type":"markdown","metadata":{"id":"6UcOYQPW8sVq"},"source":["### Naive solution\n","\n","Here, we prompt the model to choose an answer to the example above (`data[150]`) out of the options given above. We're using a format that mimics grade school solution textbook.\n","\n","Please note that there are minor formatting changes in options: an extra space and an opening bracket. Those may or may not be important :)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1699952684314,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"KtkkdiJl3-UI"},"outputs":[],"source":["EXAMPLE_0SHOT = \"\"\"\n","Question: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\n","Answer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\n","Correct Answer:\n","\"\"\".strip()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5409,"status":"ok","timestamp":1699897390749,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"hyQ_8tJc6nyv","outputId":"965f35cc-9e6a-4aac-84f7-8c68cfe867a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Prompt:]\n","Question: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\n","Answer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\n","Correct Answer:\n","================================================================================\n","[Generated:] (E) 5 minutes\n","Explanation: Jennie bikes at 20 miles per hour for 2 minutes. She will have travelled 2 miles in this time. Janice also bikes for 2 minutes, but at a slower speed of 10 miles per hour. This means that she will travel 2 miles in 2 times 10 = 20 minutes.\n","Janice and Jennie will have travelled 4 miles collectively,\n"]}],"source":["# solving an equation directly\n","batch = tokenizer(EXAMPLE_0SHOT, return_tensors='pt', return_token_type_ids=False).to(device)\n","torch.manual_seed(1337)\n","output_tokens = model.generate(**batch, max_new_tokens=100, do_sample=True, top_p=0.9)\n","print(\"[Prompt:]\\n\" + EXAMPLE_0SHOT)\n","print(\"=\" * 80)\n","print(\"[Generated:]\", tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu()))"]},{"cell_type":"markdown","metadata":{"id":"suSkiDk28I6C"},"source":["And here's how you can solve this with few-shot chain-of-thought prompting.\n","\n","You need to chang 3 things\n","- use a new field called **Rationale**, that contains a step-by-step solution to the problem\n","- add several few-shot examples of previously solved problems **with rationales**\n","- change the final prompt so that the model has to generate rationale before answering"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":219,"status":"ok","timestamp":1699962679948,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"K0F1jYdRvoJW"},"outputs":[],"source":["EXAMPLE_3SHOT_CHAIN_OF_THOUGHT = \"\"\"\n","Question: The original retail price of an appliance was 60 percent more than its wholesale cost. If the appliance was actually sold for 20 percent less than the original retail price, then it was sold for what percent more than its wholesale cost?\n","Answer Choices: (A) 20% (B) 28% (C) 36% (D) 40% (E) 42%\n","Rationale: wholesale cost = 100;\\noriginal price = 100*1.6 = 160;\\nactual price = 160*0.8 = 128.\\nAnswer: B.\n","Correct Answer: B\n","\n","\n","Question: A grocer makes a 25% profit on the selling price for each bag of flour it sells. If he sells each bag for $100 and makes $3,000 in profit, how many bags did he sell?\n","Answer Choices: (A) 12 (B) 16 (C) 24 (D) 30 (E) 40\n","Rationale: Profit on one bag: 100*1.25= 125\\nNumber of bags sold = 3000/125 = 24\\nAnswer is C.\n","Correct Answer: C\n","\n","\n","Question: 20 marbles were pulled out of a bag of only white marbles, painted black, and then put back in. Then, another 20 marbles were pulled out, of which 1 was black, after which they were all returned to the bag. If the percentage of black marbles pulled out the second time represents their percentage in the bag, how many marbles in total Q does the bag currently hold?\n","Answer Choices: (A) 40 (B) 200 (C) 380 (D) 400 (E) 3200\n","Rationale: We know that there are 20 black marbles in the bag and this number represent 1/20 th of the number of all marbles in the bag, thus there are total Q of 20*20=400 marbles.\\nAnswer: D.\n","Correct Answer: D\n","\n","\n","Question: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\n","Answer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\n","Rationale:\n","\"\"\".strip()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11780,"status":"ok","timestamp":1699906921464,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"Tn8QoAYcRkHC","outputId":"c725e49c-ff6f-4c83-d634-44d23cc6cf1e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[Prompt:]\n","Question: The original retail price of an appliance was 60 percent more than its wholesale cost. If the appliance was actually sold for 20 percent less than the original retail price, then it was sold for what percent more than its wholesale cost?\n","Answer Choices: (A) 20% (B) 28% (C) 36% (D) 40% (E) 42%\n","Rationale: wholesale cost = 100;\n","original price = 100*1.6 = 160;\n","actual price = 160*0.8 = 128.\n","Answer: B.\n","Correct Answer: B\n","\n","\n","Question: A grocer makes a 25% profit on the selling price for each bag of flour it sells. If he sells each bag for $100 and makes $3,000 in profit, how many bags did he sell?\n","Answer Choices: (A) 12 (B) 16 (C) 24 (D) 30 (E) 40\n","Rationale: Profit on one bag: 100*1.25= 125\n","Number of bags sold = 3000/125 = 24\n","Answer is C.\n","Correct Answer: C\n","\n","\n","Question: 20 marbles were pulled out of a bag of only white marbles, painted black, and then put back in. Then, another 20 marbles were pulled out, of which 1 was black, after which they were all returned to the bag. If the percentage of black marbles pulled out the second time represents their percentage in the bag, how many marbles in total Q does the bag currently hold?\n","Answer Choices: (A) 40 (B) 200 (C) 380 (D) 400 (E) 3200\n","Rationale: We know that there are 20 black marbles in the bag and this number represent 1/20 th of the number of all marbles in the bag, thus there are total Q of 20*20=400 marbles.\n","Answer: D.\n","Correct Answer: D\n","\n","\n","Question: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\n","Answer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\n","Rationale:\n","================================================================================\n","[Generated:] 10 + 20 = 30 miles per hour, thus the time required for them to bike 1 mile collectively is 1/30th of an hour, which is 1/30th of 60= 2 minutes\n","Answer is B.\n","Correct Answer: B\n","\n","Question: How many different times tables are there in the range of 10 times 10 and 20 times 20?\n","Answer\n"]}],"source":["batch = tokenizer(EXAMPLE_3SHOT_CHAIN_OF_THOUGHT, return_tensors='pt', return_token_type_ids=False).to(device)\n","torch.manual_seed(1337)\n","output_tokens = model.generate(**batch, max_new_tokens=100, do_sample=True, top_p=0.9)\n","print(\"[Prompt:]\\n\" + EXAMPLE_3SHOT_CHAIN_OF_THOUGHT)\n","print(\"=\" * 80)\n","print(\"[Generated:]\", tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu()))\n","#### NOTE: scroll down for the final answer (below the ======= line)"]},{"cell_type":"markdown","metadata":{"id":"s4px3jv-99-m"},"source":["__Task 6 (1 pt)__ write a function that automatically creates chain-of-thought prompts. Follow the instructions from the function docstring."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":399,"status":"ok","timestamp":1699962682128,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"_ntyFPMt9fyt","outputId":"705a7382-d92f-49be-d130-946e700404e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Well done!\n"]}],"source":["QUESTION_PREFIX = \"Question: \"\n","OPTIONS_PREFIX = \"Answer Choices: \"\n","CHAIN_OF_THOUGHT_PREFIX = \"Rationale: \"\n","ANSWER_PREFIX = \"Correct Answer: \"\n","FEWSHOT_SEPARATOR = \"\\n\\n\\n\"\n","\n","def make_prompt(*, main_question, fewshot_examples):\n","  \"\"\"\n","  Your goal is to produce the same prompt as the EXAMPLE_3SHOT_CHAIN_OF_THOUGHT automatically\n","\n","  For each few-shot question, make sure to follow the following rules:\n","  1. Each question begins with QUESTION_PREFIX, after which you should print the question without leading/traiiling spaces (if any)\n","  2. After the question, provide space-separated options. Each option should be put in double brackets, followed by option text, e.g. \"(A) 146%\"\n","  3. Then, provide the answer as a single letter (A-E)\n","  4. Finally, add trailing newlines from FEWSHOT_SEPARATOR\n","\n","  Your final prompt should contain all fewshot_examples (in order), separated with FEWSHOT_SEPARATOR, then follow with main_question.\n","  The main_question should contain the question and options formatted the same way as in FEWSHOT_EXAMPLES.\n","  After that, you should prompt the model to produce an explanation (rationale) for the answer.\n","\n","  Please make sure your prompt contains no leading/trailing newlines or spaces, same as in EXAMPLE_3SHOT_CHAIN_OF_THOUGHT\n","  \"\"\"\n","\n","  ans = ''\n","\n","  # fewshot_examples\n","  for example in fewshot_examples:\n","\n","    ans += f\"\"\"{QUESTION_PREFIX}{example['question']}\n","{OPTIONS_PREFIX}{(' ').join(['('+e[:2]+' '+e[2:] for e in example['options']])}\n","{CHAIN_OF_THOUGHT_PREFIX}{example['rationale']}\n","{ANSWER_PREFIX}{example['correct']}{FEWSHOT_SEPARATOR}\"\"\"\n","\n","  # Main question\n","  ans+=f\"\"\"{QUESTION_PREFIX}{main_question['question']}\n","{OPTIONS_PREFIX}{(' ').join(['('+e[:2]+' '+e[2:] for e in main_question['options']])}\n","Rationale:\"\"\"\n","\n","  return ans\n","\n","\n","generated_fewshot_prompt = make_prompt(main_question=data[150], fewshot_examples=(data[30], data[20], data[5]))\n","# print(generated_fewshot_prompt)\n","\n","assert generated_fewshot_prompt == EXAMPLE_3SHOT_CHAIN_OF_THOUGHT, \"prompts don't match\"\n","assert generated_fewshot_prompt != make_prompt(main_question=data[150], fewshot_examples=())\n","assert generated_fewshot_prompt.endswith(make_prompt(main_question=data[150], fewshot_examples=()))\n","\n","print(\"Well done!\")\n","\n","# Hint: if two prompts do not match, you may find it usefull to use https://www.diffchecker.com or similar to find the difference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":287,"status":"ok","timestamp":1699890083377,"user":{"displayName":"Zequn Zhou","userId":"03243979635948586661"},"user_tz":-120},"id":"zsbw8kpMV5gv","outputId":"56d91994-e742-4245-d0ad-43198de1b69f"},"outputs":[{"data":{"text/plain":["{'question': 'The original retail price of an appliance was 60 percent more than its wholesale cost. If the appliance was actually sold for 20 percent less than the original retail price, then it was sold for what percent more than its wholesale cost?',\n"," 'options': ['A)20%', 'B)28%', 'C)36%', 'D)40%', 'E)42%'],\n"," 'rationale': 'wholesale cost = 100;\\noriginal price = 100*1.6 = 160;\\nactual price = 160*0.8 = 128.\\nAnswer: B.',\n"," 'correct': 'B'}"]},"execution_count":162,"metadata":{},"output_type":"execute_result"}],"source":["# Example format\n","data[30]"]},{"cell_type":"markdown","metadata":{"id":"P7DzQ8hfOcFR"},"source":["__Task 7 (1 points):__ Evaluate your prompt.\n","\n","Please run the model on the entire dataset and measure it's accuracy.\n","For each question, peak $n=5$ other questions at random to serve as few-shot examples. Make sure not to accidentally sample the main_question among few-shot examples. For scientific evaluation, it is also a good practice to split the data into two parts: one for eval, and another for few-shot examples. However, doing so is optional in this homework.\n","\n","The tricky part is when to stop generating: if you don't control for this, your model can accidentally generate a whole new question - and promptyly answer it :) To make sure you get the correct answer, stop generating tokens when the model is done explaining it's solution. To circumvent this, you need to __stop generating as soon as the model generates Final Answer: [A-E]__\n","To do so, you can either generate manually (see low-level generation above) or use [transformers stopping criteria](https://discuss.huggingface.co/t/implimentation-of-stopping-criteria-list/20040/2), whichever you prefer.\n","\n","If you do everything right, the model should be much better than random. However, please __do not expect miracles__: this is far from the best models, and it will perform much worse than an average human."]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1699900682550,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"jM2-sPkB9rvT","outputId":"077adce3-6975-4d81-c5e6-e29dbd8fb493"},"outputs":[{"data":{"text/plain":["[[1, 319], [1, 350], [1, 315], [1, 360], [1, 382]]"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["stop_words_ids = [\n","    tokenizer.encode(stop_word) for stop_word in ['A','B','C','D','E']]\n","stop_words_ids"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699900603685,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"CTppIbJk98xe","outputId":"487fd890-1d24-4376-ed53-d92b5ca1c461"},"outputs":[{"data":{"text/plain":["[tensor([  1, 319]),\n"," tensor([  1, 350]),\n"," tensor([  1, 315]),\n"," tensor([  1, 360]),\n"," tensor([  1, 382])]"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["stop_words_ids = [\n","    tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in ['A', 'B','C','D','E']]\n","stop_words_ids"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":453,"status":"ok","timestamp":1699962693263,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"C5YRERaU-YZr","outputId":"dbfb2216-6325-4e0c-d9a9-30fd8e5c2694"},"outputs":[{"name":"stdout","output_type":"stream","text":["[tensor([  1, 319]), tensor([  1, 350]), tensor([  1, 315]), tensor([  1, 360]), tensor([  1, 382])]\n"]}],"source":["from transformers import StoppingCriteria, StoppingCriteriaList\n","\n","# stop_words_ids = [\n","#     tokenizer.encode(stop_word) for stop_word in ['A','B','C','D','E']]\n","# stop_words_ids\n","\n","# class StoppingCriteriaSub(StoppingCriteria):\n","\n","#     def __init__(self, stops = []):\n","#       StoppingCriteria.__init__(self),\n","\n","#     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, stops = []):\n","#       self.stops = stops\n","#       for i in range(len(stops)):\n","#         self.stops = self.stops[i]\n","\n","# stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops = stop_words_ids)])\n","###############\n","\n","stop_words_ids = [\n","    tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in ['A','B','C','D','E']]\n","\n","print(stop_words_ids)\n","\n","# class StoppingCriteriaSub(StoppingCriteria):\n","\n","#     def __init__(self, stops = [], encounters=1):\n","#       super().__init__()\n","#       self.stops = stops\n","#       self.ENCOUNTERS = encounters\n","\n","#     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n","#       stop_count = 0\n","#       for stop in self.stops:\n","#         stop_count = (stop == input_ids[0]).sum().item()\n","\n","#       if stop_count >= self.ENCOUNTERS:\n","#           return True\n","#       return False\n","\n","class StoppingCriteriaSub(StoppingCriteria):\n","\n","    def __init__(self, stops = [], encounters=1):\n","        super().__init__()\n","        self.stops = [stop.to(\"cuda\") for stop in stops]\n","\n","    # def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n","    #     for stop in self.stops:\n","    #         if torch.all((stop == input_ids[0][-len(stop):])).item():\n","    #             return True\n","\n","    #     return False\n","\n","    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n","        for seq in input_ids:\n","            for stop in self.stops:\n","                # if len(seq) >= len(stop) and torch.all((stop == seq[-len(stop):])).item():\n","                if len(seq) >= len(stop) and torch.any((stop == seq[-len(stop):])).item():\n","                    return True\n","        return False\n","\n","stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids, encounters=1)])"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5438,"status":"ok","timestamp":1699960664398,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"-0CKgsK05R4i","outputId":"ad1bd4ff-8f30-4d56-c2ff-2636945a840c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: My wall contains 8 red colour ties, 13 violet colour ties,10 blue colour ties, 5 pink colour ties, 4 green colour ties. If electricity is gone and I want at least two ties of same colour then how many ties I should take out from my rack?\n","Answer Choices: (A) 2 (B) 3 (C) 4 (D) 5 (E) 6\n","Rationale: 5 ties will get you one of different colored ties in the worst case. Thus, one more tie and you will have at least one pair. Thus, 6 is the correct answer.\n","ANSWER:E\n","Correct Answer: E\n","\n","\n","Question: A jeep travels a certain distance taking 6 hours in the forward journey. During the return journey, it increased its speed by 12km/hr and took 4 hours. What is the distance travelled by the jeep?\n","Answer Choices: (A) 126km (B) 144km (C) 127km (D) 228km (E) 128km\n","Rationale: Let 'x' be the distance and 'y' be the speed of the forward journey. Then, we have 6v=d and 4(v+12)=d\n","=> v=d/6 and v=d/4 - 12\n","=> d/6 = d/4 - 12\n","=> d/12 = 12\n","=> d=144\n","Answer: B\n","Correct Answer: B\n","\n","\n","Question: A grocery sells a bag of ice for $1.25, and makes 20% profit. If it sells 500 bags of ice, how much total profit does it make?\n","Answer Choices: (A) 125 (B) 150 (C) 225 (D) 250 (E) 275\n","Rationale: Profit per bag = 1.25 * 0.20 = 0.25\n","Total profit = 500 * 0.25 = 125\n","Answer is A.\n","Correct Answer: A\n","\n","\n","Question: The telephone bill of a certain establishment is party fixed and partly varies as the number of calls consumed. When in a certain month 540 calls made the bill is Rs.1800. In another month 620 calls are consumed then the bill becomes Rs.2040. In another month 500 units are consumed due to more\n","holidays. The bill for that month would be :\n","Answer Choices: (A) Rs.1560 (B) Rs.1680 (C) Rs.1840 (D) Rs.1950 (E) Rs.1690\n","Rationale:\n","=============================\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","Output: <s>Question: My wall contains 8 red colour ties, 13 violet colour ties,10 blue colour ties, 5 pink colour ties, 4 green colour ties. If electricity is gone and I want at least two ties of same colour then how many ties I should take out from my rack?\n","Answer Choices: (A) 2 (B) 3 (C) 4 (D) 5 (E) 6\n","Rationale: 5 ties will get you one of different colored ties in the worst case. Thus, one more tie and you will have at least one pair. Thus, 6 is the correct answer.\n","ANSWER:E\n","Correct Answer: E\n","\n","\n","Question: A jeep travels a certain distance taking 6 hours in the forward journey. During the return journey, it increased its speed by 12km/hr and took 4 hours. What is the distance travelled by the jeep?\n","Answer Choices: (A) 126km (B) 144km (C) 127km (D) 228km (E) 128km\n","Rationale: Let 'x' be the distance and 'y' be the speed of the forward journey. Then, we have 6v=d and 4(v+12)=d\n","=> v=d/6 and v=d/4 - 12\n","=> d/6 = d/4 - 12\n","=> d/12 = 12\n","=> d=144\n","Answer: B\n","Correct Answer: B\n","\n","\n","Question: A grocery sells a bag of ice for $1.25, and makes 20% profit. If it sells 500 bags of ice, how much total profit does it make?\n","Answer Choices: (A) 125 (B) 150 (C) 225 (D) 250 (E) 275\n","Rationale: Profit per bag = 1.25 * 0.20 = 0.25\n","Total profit = 500 * 0.25 = 125\n","Answer is A.\n","Correct Answer: A\n","\n","\n","Question: The telephone bill of a certain establishment is party fixed and partly varies as the number of calls consumed. When in a certain month 540 calls made the bill is Rs.1800. In another month 620 calls are consumed then the bill becomes Rs.2040. In another month 500 units are consumed due to more\n","holidays. The bill for that month would be :\n","Answer Choices: (A) Rs.1560 (B) Rs.1680 (C) Rs.1840 (D) Rs.1950 (E) Rs.1690\n","Rationale: A\n","B A\n"]}],"source":["nof_shot=3\n","target = random.randint(0,252)\n","for i in range(target,target+1):\n","  few_shot=set()\n","  while len(few_shot)<nof_shot:\n","    tem = random.randint(0,253)\n","    if tem!=i:\n","      few_shot.add(tem)\n","\n","  generated_fewshot_prompt = make_prompt(main_question=data[i], fewshot_examples=tuple(data[j] for j in few_shot))\n","  print(generated_fewshot_prompt)\n","  print('=============================')\n","  batch = tokenizer(generated_fewshot_prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n","  output_tokens = model.generate(**batch, max_new_tokens=300, do_sample=True, temperature=0.8, stopping_criteria=stopping_criteria)\n","  print(\"\\nOutput:\", tokenizer.decode(output_tokens[0].cpu()))\n","\n","  predicted_answer = tokenizer.decode(output_tokens[0].cpu())[-1]\n","  print(data[i]['correct'],predicted_answer)\n"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5951,"status":"ok","timestamp":1699899770433,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"dmQg5yuw5MMs","outputId":"12638a7c-332c-46ed-9bfa-1b3595d68e96"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input batch (encoded): {'input_ids': tensor([[    1,   450,   937, 10943, 14436,   713,  2834,   689,  3430,   763]],\n","       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"]}],"source":["prompt = 'The first discovered martian lifeform looks like'\n","batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n","print(\"Input batch (encoded):\", batch)\n","\n","output_tokens = model.generate(**batch, max_new_tokens=64, do_sample=True, temperature=0.8)\n","# greedy inference:                                        do_sample=False)\n","# beam search for highest probability:                     num_beams=4)\n"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":2015004,"status":"ok","timestamp":1699956888131,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"Zh6gejr0JNuh"},"outputs":[],"source":["NUM_SAMPLES = 0    # use this to count how many samples you evaluated\n","NUM_RESPONDED = 0  # how many times did the model produce Correct Answer: (letter) in it's response. use as a sanity check.\n","NUM_CORRECT = 0    # how many times did the model's chosen answer (letter) match the correct answer\n","\n","# Define the stiop criteria\n","from transformers import StoppingCriteria, StoppingCriteriaList\n","from typing import Tuple\n","\n","stop_words_ids = [\n","    tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in ['A','B','C','D','E']]\n","# print(stop_words_ids)\n","class StoppingCriteriaSub(StoppingCriteria):\n","\n","    def __init__(self, stops = [], encounters=1):\n","        super().__init__()\n","        self.stops = [stop.to(\"cuda\") for stop in stops]\n","\n","    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n","        for seq in input_ids:\n","            for stop in self.stops:\n","                # if len(seq) >= len(stop) and torch.all((stop == seq[-len(stop):])).item():\n","                if len(seq) >= len(stop) and torch.any((stop == seq[-len(stop):])).item():\n","                    return True\n","        return False\n","\n","stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids, encounters=1)])\n","\n","# Set the number of shot example\n","nof_shot=3\n","\n","def get_shot_examples(main_q_id:int, nof_shot:int) -> Tuple[dict]:\n","  \"\"\"Get the few show example as tuple\"\"\"\n","  few_shot=set()\n","  while len(few_shot)<nof_shot:\n","    ran_int = random.randint(0,253)\n","    if ran_int!=main_q_id:\n","      few_shot.add(ran_int)\n","  return tuple(data[j] for j in few_shot)\n","\n","target = random.randint(0,252)\n","# for i in range(target,target+1):\n","# for i in range(10, 20):\n","for i in range(len(data)):\n","  fewshot_examples = get_shot_examples(i, nof_shot)\n","  generated_fewshot_prompt = make_prompt(main_question=data[i], fewshot_examples=fewshot_examples)\n","  # print(generated_fewshot_prompt)\n","  # print('=============================')\n","  batch = tokenizer(generated_fewshot_prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n","  output_tokens = model.generate(**batch, max_new_tokens=300, do_sample=True, temperature=0.8, stopping_criteria=stopping_criteria)\n","  # print(\"\\nOutput:\", tokenizer.decode(output_tokens[0].cpu()))\n","\n","  predicted_answer = tokenizer.decode(output_tokens[0].cpu())[-1]\n","  # print(data[i]['correct'], predicted_answer)\n","\n","  NUM_SAMPLES+=1\n","  if predicted_answer in ['A','B','C','D','E']:\n","    NUM_RESPONDED+=1\n","  if data[i]['correct'] == predicted_answer:\n","    NUM_CORRECT+=1\n","\n","# Optionally, consider inferencing multiple sentences in a batch for faster inference;\n","# If you choose to batch outputs, make sure the results are the same as with batch=1 (using greedy inference)"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1699957014559,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"HmBrSmUQVPR5","outputId":"f86371aa-16ab-45b9-a666-f6d4fbb0ea4a"},"outputs":[{"name":"stdout","output_type":"stream","text":["254 237 54\n","Responded %%: 0.9330708661417323\n","Accuracy (when responded): 0.22784810126582278\n","Accuracy (overall): 0.2125984251968504\n"]}],"source":["print(NUM_SAMPLES, NUM_RESPONDED, NUM_CORRECT)\n","print(\"Responded %%:\", NUM_RESPONDED / NUM_SAMPLES)\n","print(\"Accuracy (when responded):\", NUM_CORRECT / NUM_RESPONDED)\n","print(\"Accuracy (overall):\", NUM_CORRECT / NUM_SAMPLES)\n","\n","if NUM_RESPONDED / NUM_SAMPLES < 0.9:\n","  print(\"Something is wrong with the evaluation technique (for 5-shot CoT): the model refuses to answer too many questions.\")\n","  print(\"Make sure you generate enough tokens that the model can produce a correct answer.\")\n","  print(\"When in doubt, take a look at the full model output. You can often spot errors there.\")"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":491},"executionInfo":{"elapsed":11,"status":"error","timestamp":1699962873147,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"k95HBMZrPmkQ","outputId":"1a069471-2db2-4bd8-bec2-1c32952f0984"},"outputs":[{"ename":"OutOfMemoryError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-2233dd4ae95c>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0moutput_tokens_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopping_criteria\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopping_criteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1653\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2735\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    886\u001b[0m                 \u001b[0mpadding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         attention_mask = self._prepare_decoder_attention_mask(\n\u001b[0m\u001b[1;32m    889\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36m_prepare_decoder_attention_mask\u001b[0;34m(self, attention_mask, input_shape, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mcombined_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             combined_attention_mask = _make_causal_mask(\n\u001b[0m\u001b[1;32m    810\u001b[0m                 \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36m_make_causal_mask\u001b[0;34m(input_ids_shape, dtype, device, past_key_values_length)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mmask_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_cond\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmask_cond\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 832.00 KiB is free. Process 460414 has 14.74 GiB memory in use. Of the allocated memory 13.31 GiB is allocated by PyTorch, and 439.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["from typing import Tuple\n","nof_shot=3\n","def get_shot_examples(main_q_id:int, nof_shot:int) -> Tuple[dict]:\n","  \"\"\"Get the few show example as tuple\"\"\"\n","  few_shot=set()\n","  while len(few_shot)<nof_shot:\n","    ran_int = random.randint(0,253)\n","    if ran_int!=main_q_id:\n","      few_shot.add(ran_int)\n","  return tuple(data[j] for j in few_shot)\n","\n","input_batch = []\n","correct_ans = []\n","for i in range(20,22):\n","  fewshot_examples = get_shot_examples(i, nof_shot)\n","  generated_fewshot_prompt = make_prompt(main_question=data[i], fewshot_examples=fewshot_examples)\n","  input_batch.append(generated_fewshot_prompt)\n","  correct_ans.append(data[i]['correct'])\n","\n","  # input_batch_t = torch.as_tensor(input_batch)\n","\n","batch = tokenizer(input_batch, return_tensors='pt', return_token_type_ids=False,padding=True).to(device)\n","output_tokens_ = model.generate(**batch, max_new_tokens=300, do_sample=True, temperature=0.8, stopping_criteria=stopping_criteria)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":370,"status":"ok","timestamp":1699962851683,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"8_8EZEK5ropg"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":237,"status":"ok","timestamp":1699962869506,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"GQ8YrXg8rsu-","outputId":"fa36a63f-f591-4086-96f4-c656c0bde8ad"},"outputs":[{"data":{"text/plain":["14274935808"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.memory_allocated()"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":237,"status":"ok","timestamp":1699962469510,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"rFs1Lzm-p0CQ","outputId":"675a4b6c-7403-4f45-8d06-a1aa13afc3ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["B 0\n","A E\n"]}],"source":["for x,y in zip(correct_ans,output_tokens_):\n","  print(x,tokenizer.decode(y.cpu()[-1]))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":242,"status":"ok","timestamp":1699960901430,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"LHEZC6M4VloI","outputId":"7a716fd2-a9f5-4bc0-e5c8-49d2f91d078f"},"outputs":[{"data":{"text/plain":["torch.Size([2, 633])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["output_tokens_.shape\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1699962065435,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"yb8-_uMKXT4W","outputId":"6b1800c8-f0c8-4410-8d68-8ef91f649c10"},"outputs":[{"data":{"text/plain":["(torch.Size([633]), torch.Size([633]))"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["output_tokens_[0].cpu().shape,output_tokens_[1].cpu().shape"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":519,"status":"ok","timestamp":1699957582697,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"Dvn8KeErXMNr","outputId":"cd8b5a72-1b97-4afb-b72f-2a971269b371"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Output: <s> Question: If a subscription for 15 issues of a magazine costs $42.00 and represents a saving of 25 percent of the cover prices, what is the cover price per issue?\n","Answer Choices: (A) $7.73 (B) $6.73 (C) $5.73 (D) $4.73 (E) $3.73\n","Rationale: Let subscription per magazine = x\n","15x = 42\n","=> x= 2.8\n","Let cover price per magazine = c\n","Since there is a 25% saving on cover prices\n","0.75c=x\n","=> 0.75c = 2.8\n","=>c= 3.73\n","Answer E\n","Correct Answer: E\n","\n","\n","Question: At its maximum speed, a space shuttle can travel 700m high in 40 seconds. It will also take 5 seconds to pass a point. What then is the length of the space shuttle?\n","Answer Choices: (A) 50 m (B) 75 m (C) 100 m (D) 125 m (E) 150 m\n","Rationale: Let the length of the space shuttle be x metres and its speed be y m/sec. Then, x / y = 1 ⇒ y = x / 5\n","∴ (x + 700) / 40 = x / 5 ⇔ x = 100 m. Answer C\n","Correct Answer: C\n","\n","\n","Question: If 6 yrs are subtracted from the present age of Ajay and the remainder is divided by 18, then the present age of Rahul is obtained. If Rahul is 2 yrs younger to Denis whose age is 5 yrs, then what is Ajay's present age?\n","Answer Choices: (A) 40 (B) 60 (C) 70 (D) 80 (E) 90\n","Rationale: Present age of Denis =5 years\n","Present age of Rahul =5−2=3\n","Let present age of Ajay =x\n","Then, present age of Rahul =x−618\n","x−618=3⇒x−6=3×18=54⇒x=54+6=60\n","B\n","Correct Answer: B\n","\n","\n","Question: At a certain factory, 10 percent of the staplers produced on Monday were defective and 2 percent of the non-defective staplers were rejected by mistake. If 72 of the non-defective staplers were rejected, what was the number of staplers produced that day?\n","Answer Choices: (A) 4,000 (B) 4,200 (C) 4,500 (D) 4,800 (E) 5,000\n","Rationale:</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>$}}%\\color{blue}{Percentage of defective staplers = 10\\%}\n","Number of produced staplers = x\n","Number of defective staplers = 10% of x =10% × x = x/10\n","Number rejected = 10% of 10% of x = 1% of x = 1% × x = x/100\n","So, we have\n","x = 10,000\n","And % rejected = 1%\n","x/100 = 72\n","1% x = 72\n","x = 72 × 100 = 7200\n","Correct Answer: A\n"]}],"source":["print(\"\\nOutput:\", tokenizer.decode(output_tokens_[1].cpu()))"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1699957573637,"user":{"displayName":"t tiiz","userId":"12254911776999114256"},"user_tz":-120},"id":"sIoh31M-W9L-","outputId":"50987093-7254-42b5-9c16-28d9173d116f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Output: <s>Question: Decipher the following multiplication table:\n","M A D\n","B E\n","-------------\n","M A D\n","R A E\n","-------------\n","A M I D\n","Answer Choices: (A) 9 2 0 0 (B) 9 2 0 9 (C) 9 2 0 1 (D) 9 2 0 7 (E) 9 2 2 2\n","Rationale: It is clear that E = 1 as MAD×E=MAD\n","From the hundred's line, M + A = 10 + M or 1 + M + A = 10 + M\n","As A = 10 not possible, A = 9\n","So I = 0.\n","and From the thousand's line R + 1 = A. So R = 8.\n","M 9 D\n","B 1\n","-------------\n","M 9 D\n","8 9 1\n","-------------\n","9 M 0 D\n","-------------\n","As B×D = 1, B and D takes 3, 7 in some order.\n","If B = 7 and D = 3, then M93×7 = _51 is not satisfied. So B = 3 and D = 7.\n","2 9 7\n","3 1\n","-------------\n","2 9 7\n","8 9 1\n","-------------\n","9 2 0 7\n","-------------\n","Answer:D\n","Correct Answer: D\n","\n","\n","Question: Car ‘X’ covers a distance of 320 kms in 8 hours and car ‘Y’ covers a distance of 415 kms in 5 hrs. What is the difference in the speed of the two cars?\n","Answer Choices: (A) 42kms/hr (B) 41km/hr (C) 43kms/hr (D) 45kms/hr (E) None of these\n","Rationale: The speed of Car ’X’=320kms/8hr=40kms/hr\n","The speed of car ’Y’=415kms/5hr=83kms/hr\n","the difference is 43km/hr\n","ANSWER:C\n","Correct Answer: C\n","\n","\n","Question: John likes to have lightly flavored tea every evening. In a 50% strong milk tea, he replaces 15% of it with milk twice. Then, he replaces 10 percent of the resultant solution with more milk.\n","What is the final concentration of tea John drinks?\n","Answer Choices: (A) 15.38% (B) 42% (C) 39.86% (D) 22.35% (E) 32.51%\n","Rationale: Imagine starting out with 100 ml of 50% milk tea.\n","In step 1, 15% of the tea is replaced with milk. Thus, 85% of the original tea remains. Since this is done twice, we have a concentration of 50x0.85x0.85% (=36.125%) of tea solution.\n","Finally, 10% of this solution is replaced with milk again. So, the final concentration of tea is 36.125*0.9%\n","This equals 32.51% of tea solution.\n","Answer: E\n","Correct Answer: E\n","\n","\n","Question: A company produces 420 units of a particular computer component every month, at a production cost to the company of $110 per component, and sells all of the components by the end of each month. What is the minimum selling price per component that will guarantee that the yearly profit (revenue from sales minus production costs) will be at least $626,400 ?\n","Answer Choices: (A) 226 (B) 230 (C) 240 (D) 260 (E) 280\n","Rationale: We have: $110 production cost x 420 units produced x 12 months = $516,800\n","We wish to maximize profit from sales of 420 units per month.\n","Selling price = $110 production cost / 12 months = $9.17\n","Total monthly profit then = 420 x $9.17 x 12 months = $420 x 420 x $9.17 = $1,785,780\n","Total yearly profit = $1,785,780 x 12 months = $21,434,360\n"]}],"source":["print(\"\\nOutput:\", tokenizer.decode(output_tokens_[0].cpu()))"]},{"cell_type":"markdown","metadata":{"id":"UZLK2rLiKxbM"},"source":["__Task 8 (2 points)__ Experiment time!\n","<img width=200px src=https://www.evolvefish.com/cdn-cgi/image/quality%3D85/assets/images/Apparel/TShirtsWomenCont/Main/EF-APP-CWT-00068(Main).jpg>\n","\n","Your final quest is to use the testbench you've just written to answer one of the following questions:\n","\n","### Option 1: How many shots do you need?\n","\n","How does model accuracy change with the number of fewshot examples?\n","\n","a. check if the model accuracy changes as you increase/decrease the number of \"shots\"\n","\n","b. try to prompt-engineer a model into giving the best rationale __without__ any few-shot examples, i.e. zero-shot\n","\n","For zero-shot mode, feel free to use wild prompt-engineering or modify the inference procedure.\n","\n","### Option 2: Is this prompting tecnique reliable?\n","\n","_Inspired by ongoing research by Anton Voronov, Lena Volf and Max Ryabinin._\n","\n","For this option, you need to check if the model behavior (and hence, accuracy) is robust to perturbations in the input prompt.\n","\n","a. Does the accuracy degrade if you provide wrong answers to few-shot examples? (make sure to modify rationale if it contains answer in the end)\n","\n","b. Does it degrade if you replace question/answer prompts with \"Q\" and \"A\"? What if you write both on the same line? Change few-shot separators?\n","\n","\n","\n","### Option 3: Inference Matters\n","\n","There are many ways to inference the model, not all of them equal.\n","\n","a. check whether greedy inference or beam search affects model generation quality\n","\n","b. implement and evaluate sampling with voting (see explanation below).\n","\n","\n","The voting technique(b) should work as follows: first, you generate k (e.g. 50) \"attempts\" at an answer using nucleus sampling (or a similar technique).\n","Then, you count how many of those attempts chose a particular option (A, B, etc) as the final answer. The option that was chosen most frequently has the most \"votes\", and therefore \"wins\".\n","\n","To speed up voting, you may want to generate these attempts in parallel as a batch. That should be very easy to implement: just run `model.generate` on a list with multiple copies of the same prompt.\n","\n","\n","\n","\n","================================================\n","\n","__Common rules:__ You will need to test both hypothes (A and B) in the chosen option. You may choose to replace one of them with your own idea - but please ask course staff in advance (via telegram) if you want full points.\n","\n","Feel free to organize your code and report as you see fit - but please make sure it's readable and the code runs top-to-bottom :)\n","Write a short informal report about what you tried and, in doing so, what did you found. Minimum of 2 paragraphs; more is ok; creative visualizations are welcome.\n","\n","You are allowed (but not required) to prompt the model into generating a report for you --- or helping you write one. However, if you do so, make sure that it is still human-readable :)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_r6UVDl4NEua"},"outputs":[],"source":["# feel free to organize your solution as you see fit"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/yandexdataschool/nlp_course/blob/2023/week06_llm/practice.ipynb","timestamp":1699466977906}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"widgets":{"application/vnd.jupyter.widget-state+json":{"03a21c9193934a25b837e04f30c9a8b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d441eb37551d4f7081754385a7e573c9","placeholder":"​","style":"IPY_MODEL_4e4180e0b58a40ba8c9b295e6a279886","value":"Your token has been saved in your configured git credential helpers (store)."}},"0758ccd36def4ba6a5cd68ff10bed8d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10243e525552434f8b7534b0fa7ff9f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"188ef78776a34180a939bc5de78667e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22fedbd6df384933bde221df6dc7b4a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"250b5a363cd54094b06eab850294648f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10243e525552434f8b7534b0fa7ff9f5","placeholder":"​","style":"IPY_MODEL_422bf04f507d4589a7ac21face82c961","value":"Login successful"}},"329df4a687ad41649a2dc0a3b68e4448":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6d9ab8e7f264666844b133fff33dc56","placeholder":"​","style":"IPY_MODEL_0758ccd36def4ba6a5cd68ff10bed8d7","value":"Your token has been saved to /root/.cache/huggingface/token"}},"33e4616e920d4a46a0dfc0edc1053422":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2993c7a0dd44e9bb5bd25de1ed97e1e","placeholder":"​","style":"IPY_MODEL_a3ddc8e4e57149c3a37f35ab511d4088","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"37b512efbb4845f081ddf3baa8ae25f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39855e73a2bb47208f5c0849a566573e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39f12d90caae4c9eab943f677dc2afa9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69caf5602d9240678b88dc70040e89b1","placeholder":"​","style":"IPY_MODEL_65397ad0237c41a4a825237767b3616e","value":"Connecting..."}},"4052530eb23446eaa32dc4d657fc9423":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_22fedbd6df384933bde221df6dc7b4a8","style":"IPY_MODEL_b2853cb3f2704512b2a8fb2a5b488a0b","tooltip":""}},"422bf04f507d4589a7ac21face82c961":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e4180e0b58a40ba8c9b295e6a279886":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65397ad0237c41a4a825237767b3616e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69caf5602d9240678b88dc70040e89b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ab41da9937a4b3bbb10d006ba8df6f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"972bfea6c21e424898880c572aef27a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_bf1d20d0b74441e58008b8108ff8b037","IPY_MODEL_03a21c9193934a25b837e04f30c9a8b5","IPY_MODEL_329df4a687ad41649a2dc0a3b68e4448","IPY_MODEL_250b5a363cd54094b06eab850294648f"],"layout":"IPY_MODEL_8ab41da9937a4b3bbb10d006ba8df6f1"}},"9d5da43d6f6c488fa1b81b355de7c4a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39855e73a2bb47208f5c0849a566573e","placeholder":"​","style":"IPY_MODEL_d319f33a91a44df09c14768890ab857a","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"a3653002807845859a6a46fda2c314a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3ddc8e4e57149c3a37f35ab511d4088":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2853cb3f2704512b2a8fb2a5b488a0b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"b6d9ab8e7f264666844b133fff33dc56":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8a5a87f42ba4e2da4ab979be7cf3b4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf1d20d0b74441e58008b8108ff8b037":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe4746aba3424c3ab9ccd56dc00f7b91","placeholder":"​","style":"IPY_MODEL_a3653002807845859a6a46fda2c314a6","value":"Token is valid (permission: read)."}},"d319f33a91a44df09c14768890ab857a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d441eb37551d4f7081754385a7e573c9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9dc037144824d6fabaebe957e9a74c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"CheckboxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_37b512efbb4845f081ddf3baa8ae25f6","style":"IPY_MODEL_b8a5a87f42ba4e2da4ab979be7cf3b4c","value":true}},"e8e48912ab8c4bb4bdfd9d880daceba7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"PasswordModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_e91c63c5589048f783732122b7b6ba10","placeholder":"​","style":"IPY_MODEL_188ef78776a34180a939bc5de78667e3","value":""}},"e91c63c5589048f783732122b7b6ba10":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2993c7a0dd44e9bb5bd25de1ed97e1e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe4746aba3424c3ab9ccd56dc00f7b91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
