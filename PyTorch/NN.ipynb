{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9634367f-ee5f-4865-9907-ddc6e6d02d8c",
   "metadata": {},
   "source": [
    "# Build NN in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d69429-901c-4c14-bdfc-81939bbe9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1467ff7-22d5-43cd-99fc-05af3f9c4953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Check available device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    "    \n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9dea7-89bc-49e7-b9d3-78ae991c7095",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65679a09-b94c-4710-a3bf-c0d9de4aff1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "------------------------\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Flatten(start_dim=1, end_dim=-1)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Linear(in_features=784, out_features=512, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=512, out_features=512, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=512, out_features=10, bias=True)\n",
      "------------------------\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0203, -0.0123, -0.0239,  ..., -0.0131, -0.0027,  0.0146],\n",
      "        [-0.0246,  0.0209,  0.0280,  ..., -0.0101, -0.0056,  0.0055]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0060, 0.0217], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0191, -0.0299, -0.0367,  ...,  0.0257,  0.0419, -0.0138],\n",
      "        [ 0.0139,  0.0067,  0.0427,  ..., -0.0043,  0.0431, -0.0144]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0029, -0.0098], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0052, -0.0147, -0.0384,  ...,  0.0197, -0.0379, -0.0259],\n",
      "        [ 0.0245, -0.0296, -0.0372,  ..., -0.0065,  0.0258, -0.0083]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0313, -0.0111], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build NN\n",
    "# 1. subclass nn.Module\n",
    "# 2. implement __init__() and forward()\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# Create an instance and move it to device\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# Print structure\n",
    "print(model)\n",
    "print('------------------------')\n",
    "# OR \n",
    "for i, module in enumerate(model.modules()):\n",
    "    # print(type(module), module)\n",
    "    print(module)\n",
    "\n",
    "print('------------------------')\n",
    "# Model parameter:\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d3cfcf0-bb89-4baf-8ed9-1bd6b194de6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([1], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Run NN\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4795a4-6d7d-474e-8709-ddb5600e4fd1",
   "metadata": {},
   "source": [
    "# Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84d0d210-da84-4b51-aeba-3fe9c684b5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0212, 0.0324, 0.2545],\n",
      "        [0.0212, 0.0324, 0.2545],\n",
      "        [0.0212, 0.0324, 0.2545],\n",
      "        [0.0212, 0.0324, 0.2545],\n",
      "        [0.0212, 0.0324, 0.2545]]) True\n",
      "tensor([0.0212, 0.0324, 0.2545])\n",
      "None\n",
      "None True\n",
      "True True False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hc/mx19r91j1ms05m9rf9bljc980000gn/T/ipykernel_39851/2221346515.py:15: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(z.grad, z.requires_grad)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "# Can only backward once(for performance reason)\n",
    "loss.backward()\n",
    "print(w.grad, w.requires_grad)\n",
    "print(b.grad)\n",
    "\n",
    "# No grad for x and z\n",
    "print(x.grad)\n",
    "print(z.grad, z.requires_grad)\n",
    "\n",
    "# Check if leaf node\n",
    "print(x.is_leaf, w.is_leaf, z.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3475e-c670-4616-9a74-13d8b72f575f",
   "metadata": {},
   "source": [
    "# Save and Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc62eb9-98a6-41ce-9cbf-655cc232f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "# Initilize the model class\n",
    "model = NeuralNetwork().to(device)\n",
    "# Load parameter\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
